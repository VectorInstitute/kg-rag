{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC 10-Q Knowledge Graph Construction\n",
    "\n",
    "This notebook demonstrates constructing a knowledge graph from SEC 10-Q filings using LangChain. The approach uses LLM-based extraction to identify entities and relationships without pre-defining a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Set, Tuple\n",
    "import networkx as nx\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "from Levenshtein import distance\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import heapq\n",
    "\n",
    "@dataclass(order=True)\n",
    "class EntityMatch:\n",
    "    \"\"\"Represents a matched entity with similarity score.\"\"\"\n",
    "    similarity: float\n",
    "    graph_entity: str = field(compare=False)\n",
    "    query_entity: str = field(compare=False)\n",
    "\n",
    "@dataclass\n",
    "class KnowledgeChain:\n",
    "    \"\"\"Represents a chain of entities and relationships in the graph.\"\"\"\n",
    "    path: List[str]  # Alternating entities and relationships\n",
    "    score: float\n",
    "    terminal_node: str\n",
    "\n",
    "class ChainExtractor:\n",
    "    \"\"\"Extracts candidate chains from query using LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_chains: int = 1, verbose: bool = False):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0,\n",
    "        )\n",
    "        self.llm = self.llm.bind(response_format={\"type\": \"json_object\"})\n",
    "        self.num_chains = num_chains\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def extract_chains(self, query: str) -> List[List[str]]:\n",
    "        \"\"\"Extract candidate entity-relationship chains from query.\"\"\"\n",
    "        prompt = f\"\"\"Given this question, identify {self.num_chains} possible chain(s) of entities and relationships that would answer it.\n",
    "        Express each chain as: entity1 -> relation1 -> entity2 -> relation2 -> ...\n",
    "        Return JSON object with key \"chains\" containing array of chains.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Example output:\n",
    "        {{\"chains\": [\n",
    "            [\"Apple Inc\", \"REPORTED\", \"Revenue\", \"VALUE_ON\", \"Q3 2022\"],\n",
    "            [\"Apple Inc\", \"HAS\", \"Products\", \"MARGIN_PERCENTAGE\", \"Q3 2022\"]\n",
    "        ]}}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        try:\n",
    "            chains = json.loads(response.content)[\"chains\"]\n",
    "            return chains\n",
    "        except:\n",
    "            if self.verbose:\n",
    "                print(\"Failed to parse LLM output as JSON\")\n",
    "            return []\n",
    "\n",
    "class EntityMatcher:\n",
    "    \"\"\"Matches query entities to graph entities using Levenshtein distance.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = False):\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def get_matches(self, query_entities: List[str], graph_entities: List[str], \n",
    "                   beam_width: int) -> Dict[str, List[EntityMatch]]:\n",
    "        \"\"\"Find top matches for each query entity using Levenshtein distance.\"\"\"\n",
    "        matches = {}\n",
    "        \n",
    "        for query_entity in query_entities:\n",
    "            scores = []\n",
    "            if self.verbose:\n",
    "                print(f\"\\nMatching entity: {query_entity}\")\n",
    "            \n",
    "            for graph_entity in graph_entities:\n",
    "                dist = distance(query_entity.lower(), graph_entity.lower())\n",
    "                max_len = max(len(query_entity), len(graph_entity))\n",
    "                similarity = 1 - (dist / max_len)\n",
    "                \n",
    "                heapq.heappush(scores, (-similarity, EntityMatch(\n",
    "                    graph_entity=graph_entity,\n",
    "                    query_entity=query_entity,\n",
    "                    similarity=similarity\n",
    "                )))\n",
    "            \n",
    "            matches[query_entity] = []\n",
    "            if self.verbose:\n",
    "                print(\"Top matches:\")\n",
    "            for _ in range(min(beam_width, len(scores))):\n",
    "                _, match = heapq.heappop(scores)\n",
    "                if self.verbose:\n",
    "                    print(f\"  {match.graph_entity} (similarity: {match.similarity:.3f})\")\n",
    "                matches[query_entity].append(match)\n",
    "                \n",
    "        return matches\n",
    "\n",
    "class BeamSearchExplorer:\n",
    "    \"\"\"Explores graph using beam search to find relevant knowledge chains.\"\"\"\n",
    "    \n",
    "    def __init__(self, graph: nx.DiGraph, beam_width: int = 3, max_depth: int = 5, verbose: bool = False):\n",
    "        self.graph = graph\n",
    "        self.beam_width = beam_width\n",
    "        self.max_depth = max_depth\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def explore(self, start_entities: List[str], target_chain: List[str]) -> List[KnowledgeChain]:\n",
    "        \"\"\"Perform beam search from start entities following target chain pattern.\"\"\"\n",
    "        beam = [(0, [entity], entity) for entity in start_entities]\n",
    "        chains = []\n",
    "        \n",
    "        if not isinstance(self.beam_width, int):\n",
    "            if self.verbose:\n",
    "                print(\"Warning: Invalid beam width, using default of 3\")\n",
    "            self.beam_width = 3\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f\"\\nStarting beam search (width={self.beam_width}, max_depth={self.max_depth})\")\n",
    "            print(f\"Target pattern: {' -> '.join(target_chain)}\")\n",
    "        \n",
    "        for depth in range(self.max_depth):\n",
    "            if not beam:\n",
    "                if self.verbose:\n",
    "                    print(f\"  Depth {depth}: No more candidates to explore\")\n",
    "                break\n",
    "                \n",
    "            if self.verbose:\n",
    "                print(f\"\\n  Depth {depth}:\")\n",
    "                \n",
    "            candidates = []\n",
    "            for score, path, current in beam:\n",
    "                if self.verbose:\n",
    "                    print(f\"    Exploring from: {current}\")\n",
    "                edges = list(self.graph.edges(current, data=True))\n",
    "                if self.verbose:\n",
    "                    print(f\"    Found {len(edges)} outgoing edges\")\n",
    "                \n",
    "                for _, neighbor, rel_data in edges:\n",
    "                    relation = rel_data.get('relation', '')\n",
    "                    new_path = path + [relation, neighbor]\n",
    "                    chain_score = self._score_chain_match(new_path, target_chain)\n",
    "                    \n",
    "                    if self.verbose and chain_score > 0.5:\n",
    "                        print(f\"      {' -> '.join(new_path)} (score: {chain_score:.3f})\")\n",
    "                    \n",
    "                    candidates.append((chain_score, new_path, neighbor))\n",
    "                    chains.append(KnowledgeChain(path=new_path, score=chain_score, terminal_node=neighbor))\n",
    "            \n",
    "            beam = heapq.nlargest(self.beam_width, candidates, key=lambda x: x[0])\n",
    "            if self.verbose:\n",
    "                print(f\"\\n    Selected top {len(beam)} candidates for next iteration\")\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f\"\\nExploration complete. Found {len(chains)} total chains\")\n",
    "        return chains\n",
    "    \n",
    "    def _score_chain_match(self, path: List[str], target: List[str]) -> float:\n",
    "        \"\"\"Score chain match using Levenshtein distance.\"\"\"\n",
    "        score = 0\n",
    "        min_len = min(len(path), len(target))\n",
    "        for i in range(min_len):\n",
    "            path_item = path[i].lower()\n",
    "            target_item = target[i].lower()\n",
    "            dist = distance(path_item, target_item)\n",
    "            max_len = max(len(path_item), len(target_item))\n",
    "            similarity = 1 - (dist / max_len)\n",
    "            score += similarity\n",
    "        return score / max(len(path), len(target))\n",
    "\n",
    "class EnhancedKGRAG:\n",
    "    \"\"\"Main class implementing the enhanced KG-RAG system.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 graph: nx.DiGraph,\n",
    "                 llm: ChatOpenAI,\n",
    "                 beam_width: int = 3,\n",
    "                 max_depth: int = 5,\n",
    "                 top_k: int = 3,\n",
    "                 num_chains: int = 1,\n",
    "                 verbose: bool = False\n",
    "                 ):\n",
    "        self.graph = graph\n",
    "        self.chain_extractor = ChainExtractor(num_chains, verbose)\n",
    "        self.entity_matcher = EntityMatcher(verbose)\n",
    "        self.explorer = BeamSearchExplorer(graph, beam_width, max_depth, verbose)\n",
    "        self.llm = llm\n",
    "        self.top_k = top_k\n",
    "        self.beam_width = beam_width\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def query(self, question: str) -> dict:\n",
    "        \"\"\"Process query and return structured response with reasoning.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"Processing query: {question}\")\n",
    "            print(\"=\"*80)\n",
    "            print(\"\\n📋 Extracting candidate chains...\")\n",
    "        \n",
    "        candidate_chains = self.chain_extractor.extract_chains(question)\n",
    "        if self.verbose:\n",
    "            print(\"Candidate chains:\")\n",
    "            for chain in candidate_chains:\n",
    "                print(f\"  {' -> '.join(chain)}\")\n",
    "        \n",
    "        all_knowledge_chains = []\n",
    "        for chain_pattern in candidate_chains:\n",
    "            if self.verbose:\n",
    "                print(f\"\\nProcessing chain pattern: {' -> '.join(chain_pattern)}\")\n",
    "                print(f\"\\n🔍 Matching initial entity: {chain_pattern[0]}\")\n",
    "            \n",
    "            graph_entities = list(self.graph.nodes())\n",
    "            matches = self.entity_matcher.get_matches([chain_pattern[0]], graph_entities, self.beam_width)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"\\n🔎 Exploring graph...\")\n",
    "            \n",
    "            start_entities = [match.graph_entity for match in matches.get(chain_pattern[0], [])]\n",
    "            if self.verbose:\n",
    "                print(f\"Starting from entities: {start_entities}\")\n",
    "            \n",
    "            chain_results = self.explorer.explore(start_entities, chain_pattern)\n",
    "            top_chains = sorted(chain_results, key=lambda x: x.score, reverse=True)[:self.top_k]\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"\\nTop {self.top_k} paths for this pattern:\")\n",
    "                for result in top_chains:\n",
    "                    print(f\"  Score {result.score:.3f}: {' -> '.join(result.path)}\")\n",
    "            \n",
    "            all_knowledge_chains.extend(top_chains)\n",
    "        \n",
    "        knowledge = self._format_knowledge(all_knowledge_chains)\n",
    "        if self.verbose:\n",
    "            print(\"\\n📚 Knowledge context for LLM:\")\n",
    "            print(knowledge)\n",
    "            print(\"\\n💭 Generating answer...\")\n",
    "        \n",
    "        response = self._generate_answer(question, knowledge)\n",
    "        if self.verbose:\n",
    "            print(\"\\n🎯 Final response:\")\n",
    "            print(json.dumps(response, indent=2))\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _format_knowledge(self, chains: List[KnowledgeChain]) -> str:\n",
    "        formatted = []\n",
    "        for chain in chains:\n",
    "            path_str = \" -> \".join(chain.path)\n",
    "            formatted.append(f\"Knowledge trace (score={chain.score:.2f}): {path_str}\")\n",
    "        return \"\\n\".join(formatted)\n",
    "    \n",
    "    def _generate_answer(self, question: str, knowledge: str) -> dict:\n",
    "        \"\"\"Generate structured answer with reasoning and final value.\"\"\"\n",
    "        prompt = f\"\"\"Based on the following knowledge traces from a graph database, answer this question: {question}\n",
    "\n",
    "                    Knowledge:\n",
    "                    {knowledge}\n",
    "\n",
    "                    You must provide your response as a JSON object with this structure:\n",
    "                    {{\n",
    "                        \"reasoning\": \"Your detailed step-by-step analysis showing:\n",
    "                            1. What specific information you found in the knowledge traces\n",
    "                            2. How you verified and cross-referenced the information\n",
    "                            3. Any calculations performed\n",
    "                            4. How you validated the final answer\",\n",
    "                        \"answer\": \"final numerical value only, properly formatted with no units unless question specifies otherwise\"\n",
    "                    }}\n",
    "\n",
    "                    Important Rules:\n",
    "                    - Base your answer ONLY on the provided knowledge traces\n",
    "                    - Numbers must be whole integers without comma separators, unless specified\n",
    "                    - Percentages must be whole numbers without % sign\n",
    "                    - The answer field must contain ONLY the numerical value, no text or units\n",
    "                    - Your entire response must be valid JSON\n",
    "                    \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        return json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.graphs.networkx_graph import NetworkxEntityGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, \n",
    "                 model_name=\"gpt-4o\", \n",
    "                 api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "                 )\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "\n",
    "# Check if \"graph_documents.pkl\" exists\n",
    "if os.path.exists(\"graph_documents.pkl\"):\n",
    "    # Load graph_documents from the file\n",
    "    with open(\"graph_documents.pkl\", \"rb\") as f:\n",
    "        graph_documents = pickle.load(f)\n",
    "    print(\"Loaded graph_documents from graph_documents.pkl\")\n",
    "else:\n",
    "    # Convert documents to graph documents\n",
    "    graph_documents = llm_transformer.convert_to_graph_documents(tqdm(documents))\n",
    "    \n",
    "    # Save graph_documents to the file\n",
    "    with open(\"graph_documents.pkl\", \"wb\") as f:\n",
    "        pickle.dump(graph_documents, f)\n",
    "    print(\"Converted documents to graph_documents and saved to graph_documents.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "graph = NetworkxEntityGraph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for doc in graph_documents:\n",
    "    for node in doc.nodes:\n",
    "        graph.add_node(node.id)\n",
    "\n",
    "for doc in graph_documents:\n",
    "    for edge in doc.relationships:\n",
    "        graph._graph.add_edge(\n",
    "            edge.source.id,\n",
    "            edge.target.id,\n",
    "            relation=edge.type,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize components\n",
    "    llm = ChatOpenAI(temperature=0,\n",
    "                     model_name=\"gpt-4o\", \n",
    "                     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                     )\n",
    "    llm = llm.bind(response_format={\"type\": \"json_object\"})\n",
    "        \n",
    "    # Initialize system\n",
    "    kg_rag = EnhancedKGRAG(graph._graph, llm, beam_width=10, max_depth=8, top_k=100, num_chains=2, verbose=True)\n",
    "    # kg_rag = EnhancedKGRAG(graph._graph, llm, beam_width=10, max_depth=10, num_chains=1, top_k_per_chain=10)\n",
    "    \n",
    "    # Example query\n",
    "    question = \"What was the Products gross margin percentage for Apple for the quarter ended July 1, 2023, as reported in their Q3 2023 10-Q? Provide the answer rounded to one decimal place.\"\n",
    "    answer = kg_rag.query(question)\n",
    "    # print(f\"Question: {question}\")\n",
    "    # print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "def evaluate_kg_rag(\n",
    "    rag_system,\n",
    "    data_path: str,\n",
    "    eval_dir: Optional[Path] = None,\n",
    ") -> None:\n",
    "    \"\"\"Run evaluation of KG-RAG system on test dataset.\"\"\"\n",
    "    \n",
    "    # Setup directories and load data\n",
    "    eval_dir = eval_dir or Path(\"evaluation_results_kg\")\n",
    "    eval_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = eval_dir / f\"evaluation_results_{timestamp}.txt\"\n",
    "    \n",
    "    # Load and optionally sample data\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Prepare results storage\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = len(df)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        # Write header\n",
    "        f.write(\"KG-RAG System Evaluation Results\\n\")\n",
    "        f.write(f\"Evaluation Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total Questions: {total}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        # Evaluate each question\n",
    "        for i, row in tqdm(df.iterrows(), total=total, desc=\"Evaluating questions\"):\n",
    "            question = row[\"New Question\"]\n",
    "            expected_answer = row[\"New Answer\"]\n",
    "            \n",
    "            try:\n",
    "                # Get model response\n",
    "                response = rag_system.query(question)\n",
    "                model_answer = response[\"answer\"]\n",
    "                model_reasoning = response[\"reasoning\"]\n",
    "                \n",
    "                # Validate answer format\n",
    "                try:\n",
    "                    model_answer = float(model_answer)\n",
    "                    is_correct = float(model_answer) == float(expected_answer)\n",
    "                    if is_correct:\n",
    "                        correct += 1\n",
    "                except ValueError:\n",
    "                    model_answer = \"N/A (ValueError)\"\n",
    "                    is_correct = False\n",
    "                except Exception as e:\n",
    "                    model_answer = f\"N/A ({str(e)})\"\n",
    "                    is_correct = False\n",
    "                \n",
    "                # Write question details\n",
    "                f.write(f\"Question {i+1}/{total}:\\n\")\n",
    "                f.write(f\"Question: {question}\\n\")\n",
    "                f.write(f\"Expected Answer: {expected_answer}\\n\")\n",
    "                f.write(f\"Model Answer: {model_answer}\\n\")\n",
    "                f.write(f\"Reasoning:\\n{model_reasoning}\\n\")\n",
    "                f.write(f\"Correct: {is_correct}\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "                \n",
    "                # Store result\n",
    "                results.append({\n",
    "                    'question_id': i+1,\n",
    "                    'question': question,\n",
    "                    'expected': expected_answer,\n",
    "                    'answer': model_answer,\n",
    "                    'reasoning': model_reasoning,\n",
    "                    'correct': is_correct\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                f.write(f\"Error processing question {i+1}: {str(e)}\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "                \n",
    "                results.append({\n",
    "                    'question_id': i+1,\n",
    "                    'question': question,\n",
    "                    'expected': expected_answer,\n",
    "                    'answer': f\"ERROR: {str(e)}\",\n",
    "                    'reasoning': \"Error occurred during processing\",\n",
    "                    'correct': False\n",
    "                })\n",
    "        \n",
    "        # Write summary\n",
    "        accuracy = correct / total\n",
    "        f.write(\"\\nEvaluation Summary\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(f\"Total Questions: {total}\\n\")\n",
    "        f.write(f\"Correct Answers: {correct}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.2%}\\n\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(eval_dir / f\"evaluation_detailed_results_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    print(f\"Evaluation complete. Results saved to {output_file}\")\n",
    "    print(f\"Detailed results saved to {eval_dir}/evaluation_detailed_results_{timestamp}.csv\")\n",
    "    print(f\"\\nFinal Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Initialize KG-RAG system\n",
    "    # graph = nx.DiGraph()  # Load your graph here\n",
    "    rag_system = EnhancedKGRAG(\n",
    "        graph=graph._graph,\n",
    "        llm=ChatOpenAI(temperature=0, model=\"gpt-4o\"),\n",
    "        beam_width=3,\n",
    "        max_depth=5,\n",
    "        top_k=3,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluate_kg_rag(\n",
    "        rag_system=rag_system,\n",
    "        data_path=\"../../data/sec-10-q/synthetic_qna_data_7_gpt4o_v2_mod1_50.csv\",\n",
    "        eval_dir=Path(\"evaluation_results_kg\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "def run_evaluation(kg_rag, df, eval_dir, experiment_name):\n",
    "    \"\"\"Run evaluation for a specific hyperparameter configuration.\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = eval_dir / f\"evaluation_results_{experiment_name}_{timestamp}.txt\"\n",
    "    \n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = len(df)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"KG-RAG System Evaluation Results - {experiment_name}\\n\")\n",
    "        f.write(f\"Evaluation Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Parameters: {experiment_name}\\n\")\n",
    "        f.write(f\"Total Questions: {total}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, row in tqdm(df.iterrows(), total=total, desc=f\"Evaluating {experiment_name}\"):\n",
    "            question = row[\"New Question\"]\n",
    "            expected_answer = row[\"New Answer\"]\n",
    "            \n",
    "            try:\n",
    "                response = kg_rag.query(question)\n",
    "                model_answer = response[\"answer\"]\n",
    "                model_reasoning = response[\"reasoning\"]\n",
    "                \n",
    "                try:\n",
    "                    model_answer = float(model_answer)\n",
    "                    is_correct = float(model_answer) == float(expected_answer)\n",
    "                    if is_correct:\n",
    "                        correct += 1\n",
    "                except ValueError:\n",
    "                    model_answer = \"N/A (ValueError)\"\n",
    "                    is_correct = False\n",
    "                except Exception as e:\n",
    "                    model_answer = f\"N/A ({str(e)})\"\n",
    "                    is_correct = False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                model_answer = f\"ERROR: {str(e)}\"\n",
    "                model_reasoning = \"Error occurred during processing\"\n",
    "                is_correct = False\n",
    "            \n",
    "            f.write(f\"Question {i+1}/{total}:\\n\")\n",
    "            f.write(f\"Question: {question}\\n\")\n",
    "            f.write(f\"Expected Answer: {expected_answer}\\n\")\n",
    "            f.write(f\"Model Answer: {model_answer}\\n\")\n",
    "            f.write(f\"Reasoning:\\n{model_reasoning}\\n\")\n",
    "            f.write(f\"Correct: {is_correct}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            results.append({\n",
    "                'question_id': i+1,\n",
    "                'question': question,\n",
    "                'expected': expected_answer,\n",
    "                'answer': model_answer,\n",
    "                'reasoning': model_reasoning,\n",
    "                'correct': is_correct\n",
    "            })\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        f.write(\"\\nEvaluation Summary\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(f\"Total Questions: {total}\\n\")\n",
    "        f.write(f\"Correct Answers: {correct}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.2%}\\n\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(eval_dir / f\"evaluation_detailed_results_{experiment_name}_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def hyperparameter_search(graph, llm, df):\n",
    "    \"\"\"Run hyperparameter search experiments.\"\"\"\n",
    "    eval_dir = Path(\"evaluation_results_kg_hyperparam\")\n",
    "    eval_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    experiments = [\n",
    "        {\"name\": \"baseline\", \"params\": {\"beam_width\": 10, \"max_depth\": 8, \"top_k\": 100, \"num_chains\": 2}},\n",
    "        {\"name\": \"wide_beam\", \"params\": {\"beam_width\": 25, \"max_depth\": 8, \"top_k\": 100, \"num_chains\": 2}},\n",
    "        {\"name\": \"narrow_beam\", \"params\": {\"beam_width\": 5, \"max_depth\": 8, \"top_k\": 100, \"num_chains\": 2}},\n",
    "        {\"name\": \"deep_search\", \"params\": {\"beam_width\": 10, \"max_depth\": 12, \"top_k\": 100, \"num_chains\": 2}},\n",
    "        {\"name\": \"shallow_search\", \"params\": {\"beam_width\": 10, \"max_depth\": 5, \"top_k\": 100, \"num_chains\": 2}},\n",
    "        {\"name\": \"high_topk\", \"params\": {\"beam_width\": 10, \"max_depth\": 8, \"top_k\": 200, \"num_chains\": 2}},\n",
    "        {\"name\": \"low_topk\", \"params\": {\"beam_width\": 10, \"max_depth\": 8, \"top_k\": 50, \"num_chains\": 2}},\n",
    "        {\"name\": \"multi_chain\", \"params\": {\"beam_width\": 10, \"max_depth\": 8, \"top_k\": 100, \"num_chains\": 5}},\n",
    "        {\"name\": \"aggressive\", \"params\": {\"beam_width\": 25, \"max_depth\": 12, \"top_k\": 200, \"num_chains\": 5}},\n",
    "        {\"name\": \"conservative\", \"params\": {\"beam_width\": 5, \"max_depth\": 5, \"top_k\": 50, \"num_chains\": 1}}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for exp in experiments:\n",
    "        print(f\"\\nRunning experiment: {exp['name']}\")\n",
    "        print(f\"Parameters: {exp['params']}\")\n",
    "        \n",
    "        kg_rag = EnhancedKGRAG(\n",
    "            graph=graph,\n",
    "            llm=llm,\n",
    "            verbose=False,\n",
    "            **exp['params']\n",
    "        )\n",
    "        \n",
    "        accuracy = run_evaluation(kg_rag, df, eval_dir, exp['name'])\n",
    "        results[exp['name']] = {\n",
    "            'params': exp['params'],\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    with open(eval_dir / f\"hyperparameter_search_results_{timestamp}.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(\"../../data/sec-10-q/synthetic_qna_data_7_gpt4o_v2_mod1_50.csv\")\n",
    "df = df.head(10)\n",
    "\n",
    "# Run hyperparameter search\n",
    "results = hyperparameter_search(graph._graph, llm, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

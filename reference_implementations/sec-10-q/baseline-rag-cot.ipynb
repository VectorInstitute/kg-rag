{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "import os\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "import openai\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from chromadb.errors import InvalidCollectionException\n",
    "import time\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    COLLECTION_NAME = \"sec_10q\"\n",
    "    CHROMA_PERSIST_DIR = \"chroma_db\"  # Directory to store ChromaDB files\n",
    "    OPENAI_MODEL = \"gpt-4o\"\n",
    "    OPENAI_EMBEDDING = \"text-embedding-3-small\"\n",
    "    BATCH_SIZE = 100\n",
    "    MAX_TOKENS_PER_BATCH = 8000\n",
    "    RATE_LIMIT_PAUSE = 60.0\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories.\"\"\"\n",
    "    dirs = {\n",
    "        \"DATA_DIR\": Path(\"data/sec-10-q\"),\n",
    "        \"TEXT_DIR\": Path(\"data/sec-10-q/text\")\n",
    "    }\n",
    "    \n",
    "    for dir_path in dirs.values():\n",
    "        dir_path.mkdir(exist_ok=True)\n",
    "        \n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIEmbedding:\n",
    "    \"\"\"Generate embeddings using OpenAI's API with rate limiting and batching.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"text-embedding-3-small\", \n",
    "                 batch_size: int = 100, max_tokens_per_batch: int = 8000,\n",
    "                 rate_limit_pause: float = 60.0):\n",
    "        self.client = openai.Client(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.max_tokens_per_batch = max_tokens_per_batch\n",
    "        self.rate_limit_pause = rate_limit_pause\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def create_batches(self, texts: List[str]) -> List[List[str]]:\n",
    "        batches = []\n",
    "        current_batch = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for text in texts:\n",
    "            tokens = self.count_tokens(text)\n",
    "            \n",
    "            if tokens > self.max_tokens_per_batch:\n",
    "                if current_batch:\n",
    "                    batches.append(current_batch)\n",
    "                    current_batch = []\n",
    "                    current_tokens = 0\n",
    "                \n",
    "                words = text.split()\n",
    "                chunk = []\n",
    "                chunk_tokens = 0\n",
    "                \n",
    "                for word in words:\n",
    "                    word_tokens = self.count_tokens(word + ' ')\n",
    "                    if chunk_tokens + word_tokens > self.max_tokens_per_batch:\n",
    "                        batches.append([' '.join(chunk)])\n",
    "                        chunk = [word]\n",
    "                        chunk_tokens = word_tokens\n",
    "                    else:\n",
    "                        chunk.append(word)\n",
    "                        chunk_tokens += word_tokens\n",
    "                \n",
    "                if chunk:\n",
    "                    current_batch = [' '.join(chunk)]\n",
    "                    current_tokens = chunk_tokens\n",
    "                \n",
    "            elif current_tokens + tokens > self.max_tokens_per_batch or len(current_batch) >= self.batch_size:\n",
    "                batches.append(current_batch)\n",
    "                current_batch = [text]\n",
    "                current_tokens = tokens\n",
    "            else:\n",
    "                current_batch.append(text)\n",
    "                current_tokens += tokens\n",
    "        \n",
    "        if current_batch:\n",
    "            batches.append(current_batch)\n",
    "            \n",
    "        return batches\n",
    "\n",
    "    def generate(self, texts: List[str]) -> List[List[float]]:\n",
    "        batches = self.create_batches(texts)\n",
    "        all_embeddings = []\n",
    "        \n",
    "        print(f\"Processing {len(texts)} texts in {len(batches)} batches...\")\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(batches, desc=\"Generating embeddings\")):\n",
    "            while True:\n",
    "                try:\n",
    "                    response = self.client.embeddings.create(\n",
    "                        input=batch,\n",
    "                        model=self.model\n",
    "                    )\n",
    "                    batch_embeddings = [data.embedding for data in response.data]\n",
    "                    all_embeddings.extend(batch_embeddings)\n",
    "                    break\n",
    "                except openai.RateLimitError as e:\n",
    "                    print(f\"Rate limit hit on batch {i+1}/{len(batches)}. Pausing for {self.rate_limit_pause} seconds...\")\n",
    "                    time.sleep(self.rate_limit_pause)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {i+1}/{len(batches)}: {str(e)}\")\n",
    "                    raise\n",
    "                    \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Process and chunk SEC-10Q documents using LangChain's document loaders and text splitters.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "    \n",
    "    def process_file(self, file_path: Path) -> list:\n",
    "        \"\"\"Process a single file using PyPDFLoader for PDFs or direct text reading.\"\"\"\n",
    "        metadata = {\n",
    "            \"filename\": file_path.name,\n",
    "            \"file_path\": str(file_path),\n",
    "            \"source\": \"sec_10q\"\n",
    "        }\n",
    "        \n",
    "        if file_path.suffix.lower() == \".pdf\":\n",
    "            # Use PyPDFLoader for PDF files\n",
    "            raw_documents = PyPDFLoader(file_path=str(file_path)).load()\n",
    "            # Split documents using TokenTextSplitter\n",
    "            documents = self.text_splitter.split_documents(raw_documents)\n",
    "            # Filter complex metadata\n",
    "            documents = filter_complex_metadata(documents)\n",
    "            # Convert to the format expected by ChromaDB\n",
    "            chunks = []\n",
    "            for doc in documents:\n",
    "                doc_metadata = {**metadata, **doc.metadata}\n",
    "                chunks.append((doc.page_content, doc_metadata))\n",
    "            return chunks\n",
    "        else:\n",
    "            # For text files, read directly and split\n",
    "            text = file_path.read_text()\n",
    "            texts = self.text_splitter.split_text(text)\n",
    "            return [(text, metadata) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaDBManager:\n",
    "    \"\"\"Manage ChromaDB operations using local persistent client.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str, persist_directory: str = \"chroma_db\",\n",
    "                 batch_size: int = 100):\n",
    "        self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "        self.collection_name = collection_name\n",
    "        self.batch_size = batch_size\n",
    "        self.collection = self._get_or_create_collection()\n",
    "        \n",
    "    def _get_or_create_collection(self):\n",
    "        try:\n",
    "            collection = self.client.get_collection(self.collection_name)\n",
    "            print(f\"Found existing collection: {self.collection_name}\")\n",
    "        except InvalidCollectionException:\n",
    "            print(f\"Creating new collection: {self.collection_name}\")\n",
    "            collection = self.client.create_collection(self.collection_name)\n",
    "        return collection\n",
    "            \n",
    "    def add_documents(self, chunks: List[tuple], embedding_function):\n",
    "        if not chunks:\n",
    "            print(\"No documents to add\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Processing {len(chunks)} chunks...\")\n",
    "        \n",
    "        for i in range(0, len(chunks), self.batch_size):\n",
    "            batch = chunks[i:i + self.batch_size]\n",
    "            \n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            ids = []\n",
    "            \n",
    "            for j, (text, metadata) in enumerate(batch):\n",
    "                documents.append(text)\n",
    "                metadatas.append(metadata)\n",
    "                ids.append(f\"{metadata['filename']}_{i+j}\")\n",
    "            \n",
    "            try:\n",
    "                print(f\"Generating embeddings for batch {i//self.batch_size + 1}/{(len(chunks)-1)//self.batch_size + 1}\")\n",
    "                embeddings = embedding_function.generate(documents)\n",
    "                \n",
    "                print(f\"Adding batch to collection...\")\n",
    "                self.collection.add(\n",
    "                    documents=documents,\n",
    "                    metadatas=metadatas,\n",
    "                    ids=ids,\n",
    "                    embeddings=embeddings\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {i//self.batch_size + 1}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        print(f\"Successfully added all {len(chunks)} documents to collection\")\n",
    "\n",
    "    def query(self, query_text: str, embedding_function, n_results: int = 5):\n",
    "        try:\n",
    "            print(f\"Generating embedding for query: {query_text[:100]}...\")\n",
    "            query_embedding = embedding_function.generate([query_text])[0]\n",
    "            \n",
    "            print(f\"Querying collection for top {n_results} results...\")\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=n_results\n",
    "            )\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error querying ChromaDB: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Main RAG system implementation with enhanced chain of thought reasoning.\"\"\"\n",
    "    def __init__(self, openai_api_key: str):\n",
    "        self.embedder = OpenAIEmbedding(openai_api_key)\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.db_manager = ChromaDBManager(\n",
    "            collection_name=Config.COLLECTION_NAME,\n",
    "            persist_directory=Config.CHROMA_PERSIST_DIR\n",
    "        )\n",
    "        self.client = openai.Client(api_key=openai_api_key)\n",
    "\n",
    "    def load_documents(self, text_dir: Path):\n",
    "        all_chunks = []\n",
    "        for file_path in tqdm(list(text_dir.glob(\"*.txt\"))):\n",
    "            chunks = self.processor.process_file(file_path)\n",
    "            all_chunks.extend(chunks)\n",
    "        self.db_manager.add_documents(all_chunks, self.embedder)\n",
    "\n",
    "    def generate_answer(self, query: str, context: str) -> dict:\n",
    "        \"\"\"Generate answer with enhanced chain of thought reasoning and structured response.\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"You are a helpful assistant that answers queries about SEC 10-Q filings.\n",
    "                You must provide your response as a JSON object.\n",
    "\n",
    "                Follow these steps carefully:\n",
    "                1. PLAN: Break down what information you need to find in the context\n",
    "                2. SEARCH: Locate the relevant information in the provided context only\n",
    "                3. CALCULATE: If needed, perform any calculations step by step\n",
    "                4. VERIFY: Double-check your work and ensure your answer matches the question\n",
    "                5. FORMAT: Format whole numbers without commas or units unless specified\n",
    "                    - For percentages, use whole numbers without the % sign (e.g., 25 instead of 25%)\n",
    "\n",
    "                Response Format:\n",
    "                {\n",
    "                    \"reasoning\": \"Your detailed step-by-step analysis showing:\n",
    "                        1. What specific information you're looking for\n",
    "                        2. Where you found it in the context\n",
    "                        3. Any calculations performed\n",
    "                        4. How you verified the answer\",\n",
    "                    \"answer\": \"final numerical value only, properly formatted with commas and no units unless the question specifies otherwise\"\n",
    "                }\n",
    "\n",
    "                Important Rules:\n",
    "                - Base your answer ONLY on the provided context\n",
    "                - Do not make assumptions or use external knowledge besides the context provided\n",
    "                - Numbers must be whole integers without comma separators, unless specified\n",
    "                - Percentages must be whole numbers without % sign\n",
    "                - The answer field must contain ONLY the numerical value, no text or units\n",
    "                - Your entire response must be valid JSON\n",
    "\n",
    "                The current date is January 8, 2025.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Using the following context, answer this question: {query}\\n\\nContext: {context}\"\n",
    "            }\n",
    "        ]\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=Config.OPENAI_MODEL,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            response_format={ \"type\": \"json_object\" }\n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "\n",
    "    def query(self, question: str, n_results: int = 5) -> dict:\n",
    "        \"\"\"Process query and return structured response with reasoning.\"\"\"\n",
    "        results = self.db_manager.query(question, self.embedder, n_results)\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[From {meta['filename']}]:\\n{doc}\"\n",
    "            for doc, meta in zip(results['documents'][0], results['metadatas'][0])\n",
    "        ])\n",
    "        return self.generate_answer(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation code\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create evaluation results directory if it doesn't exist\n",
    "eval_dir = Path(\"evaluation_results\")\n",
    "eval_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create timestamp for unique filename\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = eval_dir / f\"evaluation_results_{timestamp}.txt\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"../../data/sec-10-q/synthetic_qna_data_7_gpt4o_v2_mod1.csv\")\n",
    "\n",
    "# get first 50 questions\n",
    "df = df.head(50)\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = RAGSystem(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Uncomment for first time loading of documents\n",
    "# rag_system.load_documents(text_dir=Path(\"../../data/sec-10-q/text\"))\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "correct = 0\n",
    "total = len(df)\n",
    "\n",
    "# Open file for writing results\n",
    "with open(output_file, 'w') as f:\n",
    "    # Write header information\n",
    "    f.write(\"SEC 10-Q RAG System Evaluation Results\\n\")\n",
    "    f.write(f\"Evaluation Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Total Questions: {total}\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    # Evaluate each question\n",
    "    for i, row in tqdm(df.iterrows(), total=total, desc=\"Evaluating questions\"):\n",
    "        question = row[\"New Question\"]\n",
    "        expected_answer = row[\"New Answer\"]\n",
    "        \n",
    "        # Get model response\n",
    "        try:\n",
    "            response = rag_system.query(question, n_results=5)\n",
    "            model_answer = response[\"answer\"]\n",
    "            model_reasoning = response[\"reasoning\"]\n",
    "            is_correct = model_answer.strip() == expected_answer.strip()\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "        except Exception as e:\n",
    "            model_answer = f\"ERROR: {str(e)}\"\n",
    "            model_reasoning = \"Error occurred during processing\"\n",
    "            is_correct = False\n",
    "        \n",
    "        # Write question details\n",
    "        f.write(f\"Question {i+1}/{total}:\\n\")\n",
    "        f.write(f\"Question: {question}\\n\")\n",
    "        f.write(f\"Expected Answer: {expected_answer}\\n\")\n",
    "        f.write(f\"Model Answer: {model_answer}\\n\")\n",
    "        f.write(f\"Reasoning:\\n{model_reasoning}\\n\")\n",
    "        f.write(f\"Correct: {is_correct}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        # Store result for summary\n",
    "        results.append({\n",
    "            'question_id': i+1,\n",
    "            'question': question,\n",
    "            'expected': expected_answer,\n",
    "            'response': model_answer,\n",
    "            'reasoning': model_reasoning,\n",
    "            'correct': is_correct\n",
    "        })\n",
    "    \n",
    "    # Calculate and write summary statistics\n",
    "    accuracy = correct / total\n",
    "    f.write(\"\\nEvaluation Summary\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(f\"Total Questions: {total}\\n\")\n",
    "    f.write(f\"Correct Answers: {correct}\\n\")\n",
    "    f.write(f\"Accuracy: {accuracy:.2%}\\n\")\n",
    "\n",
    "# Create results DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(eval_dir / f\"evaluation_detailed_results_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"Evaluation complete. Results saved to {output_file}\")\n",
    "print(f\"Detailed results saved to {eval_dir}/evaluation_detailed_results_{timestamp}.csv\")\n",
    "print(f\"\\nFinal Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

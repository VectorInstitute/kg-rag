{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "import openai\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.errors import InvalidCollectionException\n",
    "import time\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any\n",
    "import openai\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    COLLECTION_NAME = \"sec_10q\"\n",
    "    CHROMA_PERSIST_DIR = \"chroma_db\"  # Directory to store ChromaDB files\n",
    "    OPENAI_MODEL = \"gpt-4o\"\n",
    "    OPENAI_EMBEDDING = \"text-embedding-3-small\"\n",
    "    MAX_CHUNK_SIZE = 1000  # Characters per chunk\n",
    "    OVERLAP_SIZE = 200     # Character overlap between chunks\n",
    "    BATCH_SIZE = 100\n",
    "    MAX_TOKENS_PER_BATCH = 8000\n",
    "    RATE_LIMIT_PAUSE = 60.0\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories.\"\"\"\n",
    "    dirs = {\n",
    "        \"DATA_DIR\": Path(\"data/sec-10-q\"),\n",
    "        \"TEXT_DIR\": Path(\"data/sec-10-q/text\")\n",
    "    }\n",
    "    \n",
    "    for dir_path in dirs.values():\n",
    "        dir_path.mkdir(exist_ok=True)\n",
    "        \n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIEmbedding:\n",
    "    \"\"\"Generate embeddings using OpenAI's API with rate limiting and batching.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"text-embedding-3-small\", \n",
    "                 batch_size: int = 100, max_tokens_per_batch: int = 8000,\n",
    "                 rate_limit_pause: float = 60.0):\n",
    "        self.client = openai.Client(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.max_tokens_per_batch = max_tokens_per_batch\n",
    "        self.rate_limit_pause = rate_limit_pause\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def create_batches(self, texts: List[str]) -> List[List[str]]:\n",
    "        batches = []\n",
    "        current_batch = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for text in texts:\n",
    "            tokens = self.count_tokens(text)\n",
    "            \n",
    "            if tokens > self.max_tokens_per_batch:\n",
    "                if current_batch:\n",
    "                    batches.append(current_batch)\n",
    "                    current_batch = []\n",
    "                    current_tokens = 0\n",
    "                \n",
    "                words = text.split()\n",
    "                chunk = []\n",
    "                chunk_tokens = 0\n",
    "                \n",
    "                for word in words:\n",
    "                    word_tokens = self.count_tokens(word + ' ')\n",
    "                    if chunk_tokens + word_tokens > self.max_tokens_per_batch:\n",
    "                        batches.append([' '.join(chunk)])\n",
    "                        chunk = [word]\n",
    "                        chunk_tokens = word_tokens\n",
    "                    else:\n",
    "                        chunk.append(word)\n",
    "                        chunk_tokens += word_tokens\n",
    "                \n",
    "                if chunk:\n",
    "                    current_batch = [' '.join(chunk)]\n",
    "                    current_tokens = chunk_tokens\n",
    "                \n",
    "            elif current_tokens + tokens > self.max_tokens_per_batch or len(current_batch) >= self.batch_size:\n",
    "                batches.append(current_batch)\n",
    "                current_batch = [text]\n",
    "                current_tokens = tokens\n",
    "            else:\n",
    "                current_batch.append(text)\n",
    "                current_tokens += tokens\n",
    "        \n",
    "        if current_batch:\n",
    "            batches.append(current_batch)\n",
    "            \n",
    "        return batches\n",
    "\n",
    "    def generate(self, texts: List[str]) -> List[List[float]]:\n",
    "        batches = self.create_batches(texts)\n",
    "        all_embeddings = []\n",
    "        \n",
    "        print(f\"Processing {len(texts)} texts in {len(batches)} batches...\")\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(batches, desc=\"Generating embeddings\")):\n",
    "            while True:\n",
    "                try:\n",
    "                    response = self.client.embeddings.create(\n",
    "                        input=batch,\n",
    "                        model=self.model\n",
    "                    )\n",
    "                    batch_embeddings = [data.embedding for data in response.data]\n",
    "                    all_embeddings.extend(batch_embeddings)\n",
    "                    break\n",
    "                except openai.RateLimitError as e:\n",
    "                    print(f\"Rate limit hit on batch {i+1}/{len(batches)}. Pausing for {self.rate_limit_pause} seconds...\")\n",
    "                    time.sleep(self.rate_limit_pause)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {i+1}/{len(batches)}: {str(e)}\")\n",
    "                    raise\n",
    "                    \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Process and chunk SEC-10Q documents.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_chunk_size=Config.MAX_CHUNK_SIZE, overlap_size=Config.OVERLAP_SIZE):\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.overlap_size = overlap_size\n",
    "    \n",
    "    def chunk_text(self, text: str, metadata: dict) -> list:\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + self.max_chunk_size\n",
    "            \n",
    "            if end < len(text):\n",
    "                end = text.rfind(' ', start, end)\n",
    "                if end == -1:  \n",
    "                    end = start + self.max_chunk_size\n",
    "                    \n",
    "            chunk_text = text[start:end].strip()\n",
    "            if chunk_text:  \n",
    "                chunk_metadata = {\n",
    "                    **metadata,\n",
    "                    \"chunk_start\": start,\n",
    "                    \"chunk_end\": end\n",
    "                }\n",
    "                chunks.append((chunk_text, chunk_metadata))\n",
    "            \n",
    "            start = end - self.overlap_size\n",
    "            if start < 0:\n",
    "                start = 0\n",
    "                \n",
    "        return chunks\n",
    "\n",
    "    def process_file(self, file_path: Path) -> list:\n",
    "        text = file_path.read_text()\n",
    "        \n",
    "        metadata = {\n",
    "            \"filename\": file_path.name,\n",
    "            \"file_path\": str(file_path),\n",
    "            \"source\": \"sec_10q\"\n",
    "        }\n",
    "        \n",
    "        return self.chunk_text(text, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaDBManager:\n",
    "    \"\"\"Manage ChromaDB operations using local persistent client.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str, persist_directory: str = \"chroma_db\",\n",
    "                 batch_size: int = 100):\n",
    "        self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "        self.collection_name = collection_name\n",
    "        self.batch_size = batch_size\n",
    "        self.collection = self._get_or_create_collection()\n",
    "        \n",
    "    def _get_or_create_collection(self):\n",
    "        try:\n",
    "            collection = self.client.get_collection(self.collection_name)\n",
    "            print(f\"Found existing collection: {self.collection_name}\")\n",
    "        except chromadb.errors.InvalidCollectionException:\n",
    "            print(f\"Creating new collection: {self.collection_name}\")\n",
    "            collection = self.client.create_collection(self.collection_name)\n",
    "        return collection\n",
    "            \n",
    "    def add_documents(self, chunks: List[tuple], embedding_function):\n",
    "        if not chunks:\n",
    "            print(\"No documents to add\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Processing {len(chunks)} chunks...\")\n",
    "        \n",
    "        for i in range(0, len(chunks), self.batch_size):\n",
    "            batch = chunks[i:i + self.batch_size]\n",
    "            \n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            ids = []\n",
    "            \n",
    "            for j, (text, metadata) in enumerate(batch):\n",
    "                documents.append(text)\n",
    "                metadatas.append(metadata)\n",
    "                ids.append(f\"{metadata['filename']}_{i+j}\")\n",
    "            \n",
    "            try:\n",
    "                print(f\"Generating embeddings for batch {i//self.batch_size + 1}/{(len(chunks)-1)//self.batch_size + 1}\")\n",
    "                embeddings = embedding_function.generate(documents)\n",
    "                \n",
    "                print(f\"Adding batch to collection...\")\n",
    "                self.collection.add(\n",
    "                    documents=documents,\n",
    "                    metadatas=metadatas,\n",
    "                    ids=ids,\n",
    "                    embeddings=embeddings\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {i//self.batch_size + 1}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        print(f\"Successfully added all {len(chunks)} documents to collection\")\n",
    "\n",
    "    def query(self, query_text: str, embedding_function, n_results: int = 5):\n",
    "        try:\n",
    "            print(f\"Generating embedding for query: {query_text[:100]}...\")\n",
    "            query_embedding = embedding_function.generate([query_text])[0]\n",
    "            \n",
    "            print(f\"Querying collection for top {n_results} results...\")\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=n_results\n",
    "            )\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error querying ChromaDB: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Main RAG system implementation with chain of thought reasoning.\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key: str):\n",
    "        self.embedder = OpenAIEmbedding(openai_api_key)\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.db_manager = ChromaDBManager(\n",
    "            collection_name=Config.COLLECTION_NAME,\n",
    "            persist_directory=Config.CHROMA_PERSIST_DIR\n",
    "        )\n",
    "        self.client = openai.Client(api_key=openai_api_key)\n",
    "        \n",
    "    def load_documents(self, text_dir: Path):\n",
    "        all_chunks = []\n",
    "        \n",
    "        for file_path in tqdm(list(text_dir.glob(\"*.txt\"))):\n",
    "            chunks = self.processor.process_file(file_path)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "        self.db_manager.add_documents(all_chunks, self.embedder)\n",
    "        \n",
    "    def generate_answer(self, query: str, context: str) -> dict:\n",
    "        \"\"\"Generate answer with chain of thought reasoning and structured response.\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"You are a helpful assistant that answers queries about SEC 10-Q filings.\n",
    "                You must provide your response as a JSON object.\n",
    "                Follow these steps:\n",
    "                1. First, carefully analyze the context and question\n",
    "                2. Explain your step-by-step reasoning for finding the answer\n",
    "                3. Extract only the final numerical value as specified in the question\n",
    "                4. Return your response as JSON in this exact format:\n",
    "                {\n",
    "                    \"reasoning\": \"Your step by step reasoning here...\",\n",
    "                    \"answer\": \"final numerical value only\"\n",
    "                }\n",
    "                Important: The answer field must contain ONLY the numerical value, no text or units.\n",
    "                The current date is January 8, 2025.\n",
    "                Remember: Your entire response must be valid JSON.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Using the following context, answer this question: {query}\\n\\nContext: {context}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=Config.OPENAI_MODEL,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            response_format={ \"type\": \"json_object\" }\n",
    "        )\n",
    "        \n",
    "        return json.loads(response.choices[0].message.content)\n",
    "        \n",
    "    def query(self, question: str, n_results: int = 5) -> dict:\n",
    "        \"\"\"Process query and return structured response with reasoning.\"\"\"\n",
    "        results = self.db_manager.query(question, self.embedder, n_results)\n",
    "        \n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[From {meta['filename']}]:\\n{doc}\"\n",
    "            for doc, meta in zip(results['documents'][0], results['metadatas'][0])\n",
    "        ])\n",
    "        \n",
    "        return self.generate_answer(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation code\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Create evaluation results directory if it doesn't exist\n",
    "eval_dir = Path(\"evaluation_results\")\n",
    "eval_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create timestamp for unique filename\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = eval_dir / f\"evaluation_results_{timestamp}.txt\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"../../data/sec-10-q/synthetic_qna_data_7_gpt4o.csv\")\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = RAGSystem(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Uncomment for first time loading of documents\n",
    "# rag_system.load_documents(text_dir=Path(\"../../data/sec-10-q/text\"))\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "correct = 0\n",
    "total = len(df)\n",
    "\n",
    "# Open file for writing results\n",
    "with open(output_file, 'w') as f:\n",
    "    # Write header information\n",
    "    f.write(\"SEC 10-Q RAG System Evaluation Results\\n\")\n",
    "    f.write(f\"Evaluation Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Total Questions: {total}\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    # Evaluate each question\n",
    "    for i, row in tqdm(df.iterrows(), total=total, desc=\"Evaluating questions\"):\n",
    "        question = row[\"New Question\"]\n",
    "        expected_answer = row[\"New Answer\"]\n",
    "        \n",
    "        # Get model response\n",
    "        try:\n",
    "            response = rag_system.query(question)\n",
    "            model_answer = response[\"answer\"]\n",
    "            model_reasoning = response[\"reasoning\"]\n",
    "            is_correct = model_answer.strip() == expected_answer.strip()\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "        except Exception as e:\n",
    "            model_answer = f\"ERROR: {str(e)}\"\n",
    "            model_reasoning = \"Error occurred during processing\"\n",
    "            is_correct = False\n",
    "        \n",
    "        # Write question details\n",
    "        f.write(f\"Question {i+1}/{total}:\\n\")\n",
    "        f.write(f\"Question: {question}\\n\")\n",
    "        f.write(f\"Expected Answer: {expected_answer}\\n\")\n",
    "        f.write(f\"Model Answer: {model_answer}\\n\")\n",
    "        f.write(f\"Reasoning:\\n{model_reasoning}\\n\")\n",
    "        f.write(f\"Correct: {is_correct}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        # Store result for summary\n",
    "        results.append({\n",
    "            'question_id': i+1,\n",
    "            'question': question,\n",
    "            'expected': expected_answer,\n",
    "            'response': model_answer,\n",
    "            'reasoning': model_reasoning,\n",
    "            'correct': is_correct\n",
    "        })\n",
    "    \n",
    "    # Calculate and write summary statistics\n",
    "    accuracy = correct / total\n",
    "    f.write(\"\\nEvaluation Summary\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(f\"Total Questions: {total}\\n\")\n",
    "    f.write(f\"Correct Answers: {correct}\\n\")\n",
    "    f.write(f\"Accuracy: {accuracy:.2%}\\n\")\n",
    "\n",
    "# Create results DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(eval_dir / f\"evaluation_detailed_results_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"Evaluation complete. Results saved to {output_file}\")\n",
    "print(f\"Detailed results saved to {eval_dir}/evaluation_detailed_results_{timestamp}.csv\")\n",
    "print(f\"\\nFinal Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load evaluation csv and get accuracy for level 1 and level 2\n",
    "\n",
    "df = pd.read_csv(\"../../data/sec-10-q/synthetic_qna_data_7_gpt4o.csv\")\n",
    "results_df = pd.read_csv(\"evaluation_results/evaluation_detailed_results_20250108_131159.csv\")\n",
    "\n",
    "levels = df[\"Level\"]\n",
    "\n",
    "level1 = results_df[levels == 1]\n",
    "\n",
    "level2 = results_df[levels == 2]\n",
    "\n",
    "level1_accuracy = level1[\"correct\"].mean()\n",
    "\n",
    "level2_accuracy = level2[\"correct\"].mean()\n",
    "\n",
    "print(f\"Level 1 Accuracy: {level1_accuracy:.2%}\")\n",
    "\n",
    "print(f\"Level 2 Accuracy: {level2_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level1.shape, level2.shape, len(level1)+len(level2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

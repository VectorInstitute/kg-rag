This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-05T20:19:47.730Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
reference_implementations/
  sec-10-q/
    baseline-rag-cot-script.py
    baseline-rag-script.py
    graphrag-script.py
    kg-rag-cypher-script.py
    kg-rag-entity-v2.py
    pdf_processor.py
    README.md
    visualizer.py
  synthea-fhir/
    README.md
scripts/
  generate_synthetic_data.py
.gitignore
.pre-commit-config.yaml
CONTRIBUTING.md
LICENSE.md
literature_review.md
pyproject.toml
README.md
requirements.txt

================================================================
Repository Files
================================================================

================
File: reference_implementations/sec-10-q/baseline-rag-cot-script.py
================
# %%
# Import required libraries
from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_text_splitters import TokenTextSplitter
from langchain.vectorstores.utils import filter_complex_metadata
import os
from pathlib import Path
import chromadb
import openai
from tqdm.auto import tqdm
import pandas as pd
from chromadb.errors import InvalidCollectionException
import time
import tiktoken
from typing import List, Dict, Any
import json

# %%
# Configuration
class Config:
    COLLECTION_NAME = "sec_10q"
    CHROMA_PERSIST_DIR = "chroma_db"  # Directory to store ChromaDB files
    OPENAI_MODEL = "gpt-4o"
    OPENAI_EMBEDDING = "text-embedding-3-small"
    BATCH_SIZE = 100
    MAX_TOKENS_PER_BATCH = 8000
    RATE_LIMIT_PAUSE = 60.0

def setup_directories():
    """Create necessary directories."""
    dirs = {
        "DATA_DIR": Path("data/sec-10-q"),
        "TEXT_DIR": Path("data/sec-10-q/text")
    }
    
    for dir_path in dirs.values():
        dir_path.mkdir(exist_ok=True)
        
    return dirs

# %%
class OpenAIEmbedding:
    """Generate embeddings using OpenAI's API with rate limiting and batching."""
    
    def __init__(self, api_key: str, model: str = "text-embedding-3-small", 
                 batch_size: int = 100, max_tokens_per_batch: int = 8000,
                 rate_limit_pause: float = 60.0):
        self.client = openai.Client(api_key=api_key)
        self.model = model
        self.batch_size = batch_size
        self.max_tokens_per_batch = max_tokens_per_batch
        self.rate_limit_pause = rate_limit_pause
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
    def count_tokens(self, text: str) -> int:
        return len(self.tokenizer.encode(text))
    
    def create_batches(self, texts: List[str]) -> List[List[str]]:
        batches = []
        current_batch = []
        current_tokens = 0
        
        for text in texts:
            tokens = self.count_tokens(text)
            
            if tokens > self.max_tokens_per_batch:
                if current_batch:
                    batches.append(current_batch)
                    current_batch = []
                    current_tokens = 0
                
                words = text.split()
                chunk = []
                chunk_tokens = 0
                
                for word in words:
                    word_tokens = self.count_tokens(word + ' ')
                    if chunk_tokens + word_tokens > self.max_tokens_per_batch:
                        batches.append([' '.join(chunk)])
                        chunk = [word]
                        chunk_tokens = word_tokens
                    else:
                        chunk.append(word)
                        chunk_tokens += word_tokens
                
                if chunk:
                    current_batch = [' '.join(chunk)]
                    current_tokens = chunk_tokens
                
            elif current_tokens + tokens > self.max_tokens_per_batch or len(current_batch) >= self.batch_size:
                batches.append(current_batch)
                current_batch = [text]
                current_tokens = tokens
            else:
                current_batch.append(text)
                current_tokens += tokens
        
        if current_batch:
            batches.append(current_batch)
            
        return batches

    def generate(self, texts: List[str]) -> List[List[float]]:
        batches = self.create_batches(texts)
        all_embeddings = []
        
        print(f"Processing {len(texts)} texts in {len(batches)} batches...")
        
        for i, batch in enumerate(tqdm(batches, desc="Generating embeddings")):
            while True:
                try:
                    response = self.client.embeddings.create(
                        input=batch,
                        model=self.model
                    )
                    batch_embeddings = [data.embedding for data in response.data]
                    all_embeddings.extend(batch_embeddings)
                    break
                except openai.RateLimitError as e:
                    print(f"Rate limit hit on batch {i+1}/{len(batches)}. Pausing for {self.rate_limit_pause} seconds...")
                    time.sleep(self.rate_limit_pause)
                except Exception as e:
                    print(f"Error in batch {i+1}/{len(batches)}: {str(e)}")
                    raise
                    
            time.sleep(0.5)
            
        return all_embeddings

# %%

class DocumentProcessor:
    """Process and chunk SEC-10Q documents using LangChain's document loaders and text splitters."""
    
    def __init__(self):
        self.text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
    
    def process_file(self, file_path: Path) -> list:
        """Process a single file using PyPDFLoader for PDFs or direct text reading."""
        metadata = {
            "filename": file_path.name,
            "file_path": str(file_path),
            "source": "sec_10q"
        }
        
        if file_path.suffix.lower() == ".pdf":
            # Use PyPDFLoader for PDF files
            raw_documents = PyPDFLoader(file_path=str(file_path)).load()
            # Split documents using TokenTextSplitter
            documents = self.text_splitter.split_documents(raw_documents)
            # Filter complex metadata
            documents = filter_complex_metadata(documents)
            # Convert to the format expected by ChromaDB
            chunks = []
            for doc in documents:
                doc_metadata = {**metadata, **doc.metadata}
                chunks.append((doc.page_content, doc_metadata))
            return chunks
        else:
            # For text files, read directly and split
            text = file_path.read_text()
            texts = self.text_splitter.split_text(text)
            return [(text, metadata) for text in texts]

# %%
class ChromaDBManager:
    """Manage ChromaDB operations using local persistent client."""
    
    def __init__(self, collection_name: str, persist_directory: str = "chroma_db",
                 batch_size: int = 100):
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection_name = collection_name
        self.batch_size = batch_size
        self.collection = self._get_or_create_collection()
        
    def _get_or_create_collection(self):
        try:
            collection = self.client.get_collection(self.collection_name)
            print(f"Found existing collection: {self.collection_name}")
        except InvalidCollectionException:
            print(f"Creating new collection: {self.collection_name}")
            collection = self.client.create_collection(self.collection_name)
        return collection
            
    def add_documents(self, chunks: List[tuple], embedding_function):
        if not chunks:
            print("No documents to add")
            return
            
        print(f"Processing {len(chunks)} chunks...")
        
        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i:i + self.batch_size]
            
            documents = []
            metadatas = []
            ids = []
            
            for j, (text, metadata) in enumerate(batch):
                documents.append(text)
                metadatas.append(metadata)
                ids.append(f"{metadata['filename']}_{i+j}")
            
            try:
                print(f"Generating embeddings for batch {i//self.batch_size + 1}/{(len(chunks)-1)//self.batch_size + 1}")
                embeddings = embedding_function.generate(documents)
                
                print(f"Adding batch to collection...")
                self.collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids,
                    embeddings=embeddings
                )
                
            except Exception as e:
                print(f"Error processing batch {i//self.batch_size + 1}: {str(e)}")
                raise

        print(f"Successfully added all {len(chunks)} documents to collection")

    def query(self, query_text: str, embedding_function, n_results: int = 5):
        try:
            print(f"Generating embedding for query: {query_text[:100]}...")
            query_embedding = embedding_function.generate([query_text])[0]
            
            print(f"Querying collection for top {n_results} results...")
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )
            
            return results
        except Exception as e:
            raise Exception(f"Error querying ChromaDB: {str(e)}")

# %%
class RAGSystem:
    """Main RAG system implementation with enhanced chain of thought reasoning."""
    def __init__(self, openai_api_key: str):
        self.embedder = OpenAIEmbedding(openai_api_key)
        self.processor = DocumentProcessor()
        self.db_manager = ChromaDBManager(
            collection_name=Config.COLLECTION_NAME,
            persist_directory=Config.CHROMA_PERSIST_DIR
        )
        self.client = openai.Client(api_key=openai_api_key)

    def load_documents(self, text_dir: Path):
        all_chunks = []
        for file_path in tqdm(list(text_dir.glob("*.txt"))):
            chunks = self.processor.process_file(file_path)
            all_chunks.extend(chunks)
        self.db_manager.add_documents(all_chunks, self.embedder)

    def generate_answer(self, query: str, context: str) -> dict:
        """Generate answer with enhanced chain of thought reasoning and structured response."""
        messages = [
            {
                "role": "system",
                "content": """You are a helpful assistant that answers queries about SEC 10-Q filings.
                You must provide your response as a JSON object.

                Follow these steps carefully:
                1. PLAN: Break down what information you need to find in the context
                2. SEARCH: Locate the relevant information in the provided context only
                3. CALCULATE: If needed, perform any calculations step by step
                4. VERIFY: Double-check your work and ensure your answer matches the question
                5. FORMAT: Format whole numbers without commas or units unless specified
                    - For percentages, use whole numbers without the % sign (e.g., 25 instead of 25%)

                Response Format:
                {
                    "reasoning": "Your detailed step-by-step analysis showing:
                        1. What specific information you're looking for
                        2. Where you found it in the context
                        3. Any calculations performed
                        4. How you verified the answer",
                    "answer": "final numerical value only, properly formatted with commas and no units unless the question specifies otherwise"
                }

                Important Rules:
                - Base your answer ONLY on the provided context
                - Do not make assumptions or use external knowledge besides the context provided
                - Numbers must be whole integers without comma separators, unless specified
                - Percentages must be whole numbers without % sign
                - The answer field must contain ONLY the numerical value, no text or units
                - Your entire response must be valid JSON

                The current date is January 8, 2025."""
            },
            {
                "role": "user",
                "content": f"Using the following context, answer this question: {query}\n\nContext: {context}"
            }
        ]
        response = self.client.chat.completions.create(
            model=Config.OPENAI_MODEL,
            messages=messages,
            temperature=0,
            response_format={ "type": "json_object" }
        )
        return json.loads(response.choices[0].message.content)

    def query(self, question: str, n_results: int = 5) -> dict:
        """Process query and return structured response with reasoning."""
        results = self.db_manager.query(question, self.embedder, n_results)
        context = "\n\n".join([
            f"[From {meta['filename']}]:\n{doc}"
            for doc, meta in zip(results['documents'][0], results['metadatas'][0])
        ])
        return self.generate_answer(question, context)

# %%
# Evaluation code
import datetime
import os
from pathlib import Path
import json
from dotenv import load_dotenv

load_dotenv()

# Create evaluation results directory if it doesn't exist
eval_dir = Path("evaluation_results")
eval_dir.mkdir(exist_ok=True)

# Create timestamp for unique filename
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
output_file = eval_dir / f"evaluation_results_{timestamp}.txt"

# Load the CSV file
df = pd.read_csv("../../data/sec-10-q/synthetic_qna_data_7_gpt4o_v2_mod1.csv")

# get first 50 questions
df = df.head(50)

# Initialize RAG system
rag_system = RAGSystem(openai_api_key=os.environ["OPENAI_API_KEY"])

# Uncomment for first time loading of documents
# rag_system.load_documents(text_dir=Path("../../data/sec-10-q/text"))

# Prepare results storage
results = []
correct = 0
total = len(df)

# Open file for writing results
with open(output_file, 'w') as f:
    # Write header information
    f.write("SEC 10-Q RAG System Evaluation Results\n")
    f.write(f"Evaluation Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Total Questions: {total}\n")
    f.write("=" * 80 + "\n\n")
    
    # Evaluate each question
    for i, row in tqdm(df.iterrows(), total=total, desc="Evaluating questions"):
        question = row["New Question"]
        expected_answer = row["New Answer"]
        
        # Get model response
        try:
            response = rag_system.query(question, n_results=5)
            model_answer = response["answer"]
            model_reasoning = response["reasoning"]
            is_correct = model_answer.strip() == expected_answer.strip()
            if is_correct:
                correct += 1
        except Exception as e:
            model_answer = f"ERROR: {str(e)}"
            model_reasoning = "Error occurred during processing"
            is_correct = False
        
        # Write question details
        f.write(f"Question {i+1}/{total}:\n")
        f.write(f"Question: {question}\n")
        f.write(f"Expected Answer: {expected_answer}\n")
        f.write(f"Model Answer: {model_answer}\n")
        f.write(f"Reasoning:\n{model_reasoning}\n")
        f.write(f"Correct: {is_correct}\n")
        f.write("-" * 80 + "\n\n")
        
        # Store result for summary
        results.append({
            'question_id': i+1,
            'question': question,
            'expected': expected_answer,
            'response': model_answer,
            'reasoning': model_reasoning,
            'correct': is_correct
        })
    
    # Calculate and write summary statistics
    accuracy = correct / total
    f.write("\nEvaluation Summary\n")
    f.write("=" * 80 + "\n")
    f.write(f"Total Questions: {total}\n")
    f.write(f"Correct Answers: {correct}\n")
    f.write(f"Accuracy: {accuracy:.2%}\n")

# Create results DataFrame and save to CSV
results_df = pd.DataFrame(results)
results_df.to_csv(eval_dir / f"evaluation_detailed_results_{timestamp}.csv", index=False)

print(f"Evaluation complete. Results saved to {output_file}")
print(f"Detailed results saved to {eval_dir}/evaluation_detailed_results_{timestamp}.csv")
print(f"\nFinal Accuracy: {accuracy:.2%}")

================
File: reference_implementations/sec-10-q/baseline-rag-script.py
================
# %%
# Import required libraries
from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_text_splitters import TokenTextSplitter
from langchain.vectorstores.utils import filter_complex_metadata
import os
from pathlib import Path
import chromadb
import openai
from tqdm.auto import tqdm
import pandas as pd
from chromadb.errors import InvalidCollectionException
import time
import tiktoken
from typing import List, Dict, Any

# %%
# Configuration
class Config:
    COLLECTION_NAME = "sec_10q"
    CHROMA_PERSIST_DIR = "chroma_db"  # Directory to store ChromaDB files
    OPENAI_MODEL = "gpt-4o"
    OPENAI_EMBEDDING = "text-embedding-3-small"
    BATCH_SIZE = 100
    MAX_TOKENS_PER_BATCH = 8000
    RATE_LIMIT_PAUSE = 60.0

def setup_directories():
    """Create necessary directories."""
    dirs = {
        "DATA_DIR": Path("data/sec-10-q"),
        "TEXT_DIR": Path("data/sec-10-q/text")
    }
    
    for dir_path in dirs.values():
        dir_path.mkdir(exist_ok=True)
        
    return dirs

# %%
class OpenAIEmbedding:
    """Generate embeddings using OpenAI's API with rate limiting and batching."""
    
    def __init__(self, api_key: str, model: str = "text-embedding-3-small", 
                 batch_size: int = 100, max_tokens_per_batch: int = 8000,
                 rate_limit_pause: float = 60.0):
        self.client = openai.Client(api_key=api_key)
        self.model = model
        self.batch_size = batch_size
        self.max_tokens_per_batch = max_tokens_per_batch
        self.rate_limit_pause = rate_limit_pause
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
    def count_tokens(self, text: str) -> int:
        return len(self.tokenizer.encode(text))
    
    def create_batches(self, texts: List[str]) -> List[List[str]]:
        batches = []
        current_batch = []
        current_tokens = 0
        
        for text in texts:
            tokens = self.count_tokens(text)
            
            if tokens > self.max_tokens_per_batch:
                if current_batch:
                    batches.append(current_batch)
                    current_batch = []
                    current_tokens = 0
                
                words = text.split()
                chunk = []
                chunk_tokens = 0
                
                for word in words:
                    word_tokens = self.count_tokens(word + ' ')
                    if chunk_tokens + word_tokens > self.max_tokens_per_batch:
                        batches.append([' '.join(chunk)])
                        chunk = [word]
                        chunk_tokens = word_tokens
                    else:
                        chunk.append(word)
                        chunk_tokens += word_tokens
                
                if chunk:
                    current_batch = [' '.join(chunk)]
                    current_tokens = chunk_tokens
                
            elif current_tokens + tokens > self.max_tokens_per_batch or len(current_batch) >= self.batch_size:
                batches.append(current_batch)
                current_batch = [text]
                current_tokens = tokens
            else:
                current_batch.append(text)
                current_tokens += tokens
        
        if current_batch:
            batches.append(current_batch)
            
        return batches

    def generate(self, texts: List[str]) -> List[List[float]]:
        batches = self.create_batches(texts)
        all_embeddings = []
        
        print(f"Processing {len(texts)} texts in {len(batches)} batches...")
        
        for i, batch in enumerate(tqdm(batches, desc="Generating embeddings")):
            while True:
                try:
                    response = self.client.embeddings.create(
                        input=batch,
                        model=self.model
                    )
                    batch_embeddings = [data.embedding for data in response.data]
                    all_embeddings.extend(batch_embeddings)
                    break
                except openai.RateLimitError as e:
                    print(f"Rate limit hit on batch {i+1}/{len(batches)}. Pausing for {self.rate_limit_pause} seconds...")
                    time.sleep(self.rate_limit_pause)
                except Exception as e:
                    print(f"Error in batch {i+1}/{len(batches)}: {str(e)}")
                    raise
                    
            time.sleep(0.5)
            
        return all_embeddings

# %%

class DocumentProcessor:
    """Process and chunk SEC-10Q documents using LangChain's document loaders and text splitters."""
    
    def __init__(self):
        self.text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
    
    def process_file(self, file_path: Path) -> list:
        """Process a single file using PyPDFLoader for PDFs or direct text reading."""
        metadata = {
            "filename": file_path.name,
            "file_path": str(file_path),
            "source": "sec_10q"
        }
        
        if file_path.suffix.lower() == ".pdf":
            # Use PyPDFLoader for PDF files
            raw_documents = PyPDFLoader(file_path=str(file_path)).load()
            # Split documents using TokenTextSplitter
            documents = self.text_splitter.split_documents(raw_documents)
            # Filter complex metadata
            documents = filter_complex_metadata(documents)
            # Convert to the format expected by ChromaDB
            chunks = []
            for doc in documents:
                doc_metadata = {**metadata, **doc.metadata}
                chunks.append((doc.page_content, doc_metadata))
            return chunks
        else:
            # For text files, read directly and split
            text = file_path.read_text()
            texts = self.text_splitter.split_text(text)
            return [(text, metadata) for text in texts]

# %%
class ChromaDBManager:
    """Manage ChromaDB operations using local persistent client."""
    
    def __init__(self, collection_name: str, persist_directory: str = "chroma_db",
                 batch_size: int = 100):
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection_name = collection_name
        self.batch_size = batch_size
        self.collection = self._get_or_create_collection()
        
    def _get_or_create_collection(self):
        try:
            collection = self.client.get_collection(self.collection_name)
            print(f"Found existing collection: {self.collection_name}")
        except InvalidCollectionException:
            print(f"Creating new collection: {self.collection_name}")
            collection = self.client.create_collection(self.collection_name)
        return collection
            
    def add_documents(self, chunks: List[tuple], embedding_function):
        if not chunks:
            print("No documents to add")
            return
            
        print(f"Processing {len(chunks)} chunks...")
        
        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i:i + self.batch_size]
            
            documents = []
            metadatas = []
            ids = []
            
            for j, (text, metadata) in enumerate(batch):
                documents.append(text)
                metadatas.append(metadata)
                ids.append(f"{metadata['filename']}_{i+j}")
            
            try:
                print(f"Generating embeddings for batch {i//self.batch_size + 1}/{(len(chunks)-1)//self.batch_size + 1}")
                embeddings = embedding_function.generate(documents)
                
                print(f"Adding batch to collection...")
                self.collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids,
                    embeddings=embeddings
                )
                
            except Exception as e:
                print(f"Error processing batch {i//self.batch_size + 1}: {str(e)}")
                raise

        print(f"Successfully added all {len(chunks)} documents to collection")

    def query(self, query_text: str, embedding_function, n_results: int = 5):
        try:
            print(f"Generating embedding for query: {query_text[:100]}...")
            query_embedding = embedding_function.generate([query_text])[0]
            
            print(f"Querying collection for top {n_results} results...")
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )
            
            return results
        except Exception as e:
            raise Exception(f"Error querying ChromaDB: {str(e)}")

# %%
class RAGSystem:
    """Main RAG system implementation."""
    
    def __init__(self, openai_api_key: str):
        self.embedder = OpenAIEmbedding(openai_api_key)
        self.processor = DocumentProcessor()
        self.db_manager = ChromaDBManager(
            collection_name=Config.COLLECTION_NAME,
            persist_directory=Config.CHROMA_PERSIST_DIR
        )
        self.client = openai.Client(api_key=openai_api_key)
        
    def load_documents(self, docs_dir: Path):
        """Load documents from a directory."""
        all_chunks = []
        
        # Process all PDF files in the directory
        for file_path in tqdm(list(docs_dir.glob("**/*.pdf")), desc="Processing documents"):
            chunks = self.processor.process_file(file_path)
            all_chunks.extend(chunks)
            
        self.db_manager.add_documents(all_chunks, self.embedder)
        
    def generate_answer(self, query: str, context: str) -> str:
        messages = [
            {
                "role": "system",
                "content": """You are a helpful assistant that answers queries about SEC 10-Q filings. 
                Just follow the instructions from the question exactly and only use numerical values, no text. The current date is January 15, 2025."""
            },
            {
                "role": "user",
                "content": f"Using the following context, answer this question: {query}\n\nContext: {context}"
            }
        ]
        
        response = self.client.chat.completions.create(
            model=Config.OPENAI_MODEL,
            messages=messages,
            temperature=0
        )
        
        return response.choices[0].message.content
        
    def query(self, question: str, n_results: int = 5) -> str:
        results = self.db_manager.query(question, self.embedder, n_results)
        
        context = "\n\n".join([
            f"[From {meta['filename']}]:\n{doc}"
            for doc, meta in zip(results['documents'][0], results['metadatas'][0])
        ])
        
        return self.generate_answer(question, context)

# %%
# Evaluation code
import datetime
import os
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

# Create evaluation results directory if it doesn't exist
eval_dir = Path("evaluation_results")
eval_dir.mkdir(exist_ok=True)

# Create timestamp for unique filename
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
output_file = eval_dir / f"evaluation_results_{timestamp}.txt"

# Load the CSV file
df = pd.read_csv("../../data/sec-10-q/synthetic_qna_data_7_gpt4o_v2_mod1_50.csv")

# Initialize RAG system
rag_system = RAGSystem(openai_api_key=os.environ["OPENAI_API_KEY"])

# Uncomment for first time loading of documents
# rag_system.load_documents(docs_dir=Path("../../data/sec-10-q/docs"))

# Prepare results storage
results_list = []
correct = 0
total = len(df)

# Open file for writing results
with open(output_file, 'w') as f:
    # Write header information
    f.write("SEC 10-Q RAG System Evaluation Results\n")
    f.write(f"Evaluation Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Total Questions: {total}\n")
    f.write("=" * 80 + "\n\n")
    
    # Evaluate each question
    for i, row in tqdm(df.iterrows(), total=total, desc="Evaluating questions"):
        question = row["New Question"]
        expected_answer = row["New Answer"]
        
        # Get model response
        query_results = rag_system.db_manager.query(question, rag_system.embedder, n_results=5)
        context = "\n\n".join([
            f"[From {meta['filename']}]:\n{doc}"
            for doc, meta in zip(query_results['documents'][0], query_results['metadatas'][0])
        ])
        # f.write(f"Context:\n{context}\n")
        model_response = rag_system.generate_answer(question, context)
        try:
            model_response = float(model_response)
            is_correct = float(model_response) == float(expected_answer)
        except ValueError:
            model_response = "N/A (ValueError)"
            is_correct = False
        except Exception as e:
            model_response = f"N/A ({str(e)})"
            is_correct = False

        if is_correct:
            correct += 1
        
        # Write question details
        f.write(f"Question {i+1}/{total}:\n")
        f.write(f"Question: {question}\n")
        f.write(f"Expected Answer: {expected_answer}\n")
        f.write(f"Model Response: {model_response}\n")
        f.write(f"Correct: {is_correct}\n")
        f.write("-" * 80 + "\n\n")
        
        # Store result for summary
        results_list.append({
            'question_id': i+1,
            'question': question,
            'context': context,
            'expected': expected_answer,
            'response': model_response,
            'correct': is_correct
        })
        # break;
    
    # Calculate and write summary statistics
    accuracy = correct / total
    f.write("\nEvaluation Summary\n")
    f.write("=" * 80 + "\n")
    f.write(f"Total Questions: {total}\n")
    f.write(f"Correct Answers: {correct}\n")
    f.write(f"Accuracy: {accuracy:.2%}\n")

# Create results DataFrame and save to CSV
results_df = pd.DataFrame(results_list)
results_df.to_csv(eval_dir / f"evaluation_detailed_results_{timestamp}.csv", index=False)

print(f"Evaluation complete. Results saved to {output_file}")
print(f"Detailed results saved to {eval_dir}/evaluation_detailed_results_{timestamp}.csv")
print(f"\nFinal Accuracy: {accuracy:.2%}")

================
File: reference_implementations/sec-10-q/graphrag-script.py
================
# %% [markdown]
# # GraphRAG-based Question Answering
# 
# This notebook demonstrates using the langchain-graphrag library to implement a knowledge graph-based RAG system.
# 
# The approach involves:
# 1. Text extraction and splitting into units
# 2. Graph generation using entity and relationship extraction
# 3. Graph community detection
# 4. Question answering using either local or global search strategies

# %%
import os
from pathlib import Path
from typing import cast
from dotenv import load_dotenv
import pandas as pd

from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.cache import SQLiteCache
from langchain_text_splitters import TokenTextSplitter

from langchain_graphrag.indexing import SimpleIndexer, TextUnitExtractor
from langchain_graphrag.indexing.artifacts_generation import (
    CommunitiesReportsArtifactsGenerator,
    EntitiesArtifactsGenerator, 
    RelationshipsArtifactsGenerator,
    TextUnitsArtifactsGenerator
)
from langchain_graphrag.indexing.graph_clustering import HierarchicalLeidenCommunityDetector
from langchain_graphrag.indexing.graph_generation import (
    EntityRelationshipExtractor,
    EntityRelationshipDescriptionSummarizer,
    GraphGenerator,
    GraphsMerger
)
from langchain_graphrag.indexing.report_generation import (
    CommunityReportGenerator,
    CommunityReportWriter
)
from langchain_graphrag.types.graphs.community import CommunityLevel
from langchain_graphrag.utils import TiktokenCounter

# Load environment variables
load_dotenv()

# Setup paths
CACHE_DIR = Path("cache")
VECTOR_STORE_DIR = Path("vector_stores") 
ARTIFACTS_DIR = Path("artifacts")

for p in [CACHE_DIR, VECTOR_STORE_DIR, ARTIFACTS_DIR]:
    p.mkdir(parents=True, exist_ok=True)

# %% [markdown]
# ## Configure Environment and Models
# 
# Set up the required models and environment variables.

# %%
# Create the LLMs
er_llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.0,
    api_key=os.environ["OPENAI_API_KEY"],
    cache=SQLiteCache(str(CACHE_DIR / "openai_cache.db")),
)

es_llm = ChatOpenAI(
    model="gpt-4o", 
    temperature=0.0,
    api_key=os.environ["OPENAI_API_KEY"],
    cache=SQLiteCache(str(CACHE_DIR / "openai_cache.db")),
)

# Create embeddings
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    api_key=os.environ["OPENAI_API_KEY"]
)

# Create vector store for entities
entities_vector_store = Chroma(
    collection_name="sec-10q-entities",
    persist_directory=str(VECTOR_STORE_DIR),
    embedding_function=embeddings
)

# Setup text splitter and extractor
text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
text_unit_extractor = TextUnitExtractor(text_splitter=text_splitter)

# %% [markdown]
# ## Initialize Graph Components
# 
# Set up the components needed for graph generation and processing.

# %%
# Entity relationship extraction and summarization
entity_extractor = EntityRelationshipExtractor.build_default(llm=er_llm)
entity_summarizer = EntityRelationshipDescriptionSummarizer.build_default(llm=es_llm)

# Graph generator
graph_generator = GraphGenerator(
    er_extractor=entity_extractor,
    graphs_merger=GraphsMerger(),
    er_description_summarizer=entity_summarizer
)

# Community detector
community_detector = HierarchicalLeidenCommunityDetector()

# %% [markdown]
# ## Initialize Artifacts Generators
# 
# Set up components for generating various artifacts from the graph.

# %%
# Create artifacts generators
entities_artifacts_generator = EntitiesArtifactsGenerator(
    entities_vector_store=entities_vector_store
)

relationships_artifacts_generator = RelationshipsArtifactsGenerator()

report_generator = CommunityReportGenerator.build_default(llm=er_llm)
report_writer = CommunityReportWriter()

communities_report_artifacts_generator = CommunitiesReportsArtifactsGenerator(
    report_generator=report_generator,
    report_writer=report_writer
)

text_units_artifacts_generator = TextUnitsArtifactsGenerator()

# %% [markdown]
# ## Load and Process Documents
# 
# Load the input text and split it into manageable units.

# %%
# Load and process the documents
from langchain_community.document_loaders.pdf import PyPDFLoader

documents = []
docs_path = Path("../../data/sec-10-q/docs")

# Load PDF documents
for filename in os.listdir(docs_path):
    if filename.endswith(".pdf"):
        file_path = docs_path / filename
        try:
            docs = PyPDFLoader(str(file_path)).load()
            documents.extend(docs)
            print(f"Processed: {filename}")
        except Exception as e:
            print(f"Error processing {filename}: {str(e)}")

# %% [markdown]
# ## Create Indexer and Generate Artifacts
# 
# Initialize the indexer and process the documents to generate all required artifacts.

# %%
# Create the indexer
indexer = SimpleIndexer(
    text_unit_extractor=text_unit_extractor,
    graph_generator=graph_generator,
    community_detector=community_detector,
    entities_artifacts_generator=entities_artifacts_generator,
    relationships_artifacts_generator=relationships_artifacts_generator,
    text_units_artifacts_generator=text_units_artifacts_generator,
    communities_report_artifacts_generator=communities_report_artifacts_generator
)

# Run indexing
artifacts = indexer.run(documents)

# %%
# save artifacts to .pkl on disk

import pickle

with open('graphrag_artifacts.pkl', 'wb') as f:
    pickle.dump(artifacts, f)

# %% [markdown]
# ## Local Search Example
# 
# Demonstrate using the local search capability for answering specific questions.

# %%
from datetime import datetime

current_date = datetime.now().strftime("%B %d, %Y")

from langchain_graphrag.query.local_search import (
    LocalSearch,
    LocalSearchPromptBuilder,
    LocalSearchRetriever,
)
from langchain_graphrag.query.local_search.context_builders import ContextBuilder
from langchain_graphrag.query.local_search.context_selectors import ContextSelector
from langchain_graphrag.query.local_search._system_prompt import LOCAL_SEARCH_SYSTEM_PROMPT

PROMPT_SUFFIX = f"""
Important Rules:
- Base your answer ONLY on the provided context
- Do not make assumptions or use external knowledge besides the context provided
- Numbers must be whole integers without comma separators, unless specified
- Percentages must be whole numbers without % sign
- The answer field must contain ONLY the numerical value, no text or units
- Your entire response must be valid JSON

The current date is {current_date}.
"""

LOCAL_SEARCH_SYSTEM_PROMPT = LOCAL_SEARCH_SYSTEM_PROMPT + PROMPT_SUFFIX

# Create components for local search
context_selector = ContextSelector.build_default(
    entities_vector_store=entities_vector_store,
    entities_top_k=10,
    community_level=cast(CommunityLevel, 2)
)

context_builder = ContextBuilder.build_default(
    token_counter=TiktokenCounter(),
)

retriever = LocalSearchRetriever(
    context_selector=context_selector,
    context_builder=context_builder,
    artifacts=artifacts,
)

local_search = LocalSearch(
    prompt_builder=LocalSearchPromptBuilder(system_prompt=LOCAL_SEARCH_SYSTEM_PROMPT, show_references=True),
    llm=er_llm,
    retriever=retriever
)

search_chain = local_search()

# %%
print(search_chain.invoke("What was the total net sales for Apple for the quarterly period ended April 1, 2023, as reported in their 2023 Q2 AAPL.pdf? Provide the answer in millions of dollars as a whole number without commas."))

# %%
answer = search_chain.invoke("What was the total net sales for Apple for the quarterly period ended April 1, 2023, as reported in their 2023 Q2 AAPL.pdf? Provide the answer in millions of dollars as a whole number without commas.")

# %%
# Evaluation code
import datetime
import os
from pathlib import Path
import json
import re
from dotenv import load_dotenv
from tqdm.notebook import tqdm

load_dotenv()


def extract_number(text):
    # Find any number (integer or decimal) in the string
    match = re.search(r':\s*(-?\d+(?:\.\d+)?)', text)
    if match:
        return float(match.group(1))
    return None

# Create evaluation results directory if it doesn't exist
eval_dir = Path("evaluation_results_graphrag")
eval_dir.mkdir(exist_ok=True)

# Create timestamp for unique filename
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
output_file = eval_dir / f"evaluation_results_{timestamp}.txt"

# Load the CSV file
df = pd.read_csv("../../data/sec-10-q/synthetic_qna_data_7_gpt4o_v2_mod1.csv")

# Prepare results storage
results = []
correct = 0
total = len(df)

# Open file for writing results
with open(output_file, 'w') as f:
    # Write header information
    f.write("SEC 10-Q RAG System Evaluation Results\n")
    f.write(f"Evaluation Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Total Questions: {total}\n")
    f.write("=" * 80 + "\n\n")
    
    # Evaluate each question
    for i, row in tqdm(df.iterrows(), total=total, desc="Evaluating questions"):
        question = row["New Question"]
        expected_answer = row["New Answer"]
        
        # Get model response
        try:
            response = search_chain.invoke(question)
            model_answer = extract_number(response)
            is_correct = float(model_answer) == float(expected_answer)
            if is_correct:
                correct += 1
        except Exception as e:
            model_answer = f"ERROR: {str(e)}"
            model_reasoning = "Error occurred during processing"
            is_correct = False
        
        # Write question details
        f.write(f"Question {i+1}/{total}:\n")
        f.write(f"Question: {question}\n")
        f.write(f"Expected Answer: {expected_answer}\n")
        f.write(f"Model Answer: {model_answer}\n")
        f.write(f"Correct: {is_correct}\n")
        f.write("-" * 80 + "\n\n")
        
        # Store result for summary
        results.append({
            'question_id': i+1,
            'question': question,
            'expected': expected_answer,
            'response': model_answer,
            'correct': is_correct
        })
    
    # Calculate and write summary statistics
    accuracy = correct / total
    f.write("\nEvaluation Summary\n")
    f.write("=" * 80 + "\n")
    f.write(f"Total Questions: {total}\n")
    f.write(f"Correct Answers: {correct}\n")
    f.write(f"Accuracy: {accuracy:.2%}\n")

# Create results DataFrame and save to CSV
results_df = pd.DataFrame(results)
results_df.to_csv(eval_dir / f"evaluation_detailed_results_{timestamp}.csv", index=False)

print(f"Evaluation complete. Results saved to {output_file}")
print(f"Detailed results saved to {eval_dir}/evaluation_detailed_results_{timestamp}.csv")
print(f"\nFinal Accuracy: {accuracy:.2%}")

# %% [markdown]
# ## Global Search Example
# 
# Demonstrate using the global search capability for broader questions about the document.

# %%
from langchain_graphrag.query.global_search import GlobalSearch
from langchain_graphrag.query.global_search.community_weight_calculator import (
    CommunityWeightCalculator
)
from langchain_graphrag.query.global_search.key_points_aggregator import (
    KeyPointsAggregator,
    KeyPointsAggregatorPromptBuilder,
    KeyPointsContextBuilder,
)
from langchain_graphrag.query.global_search.key_points_generator import (
    CommunityReportContextBuilder,
    KeyPointsGenerator,
    KeyPointsGeneratorPromptBuilder,
)

# Create components for global search
report_context_builder = CommunityReportContextBuilder(
    community_level=cast(CommunityLevel, 2),
    weight_calculator=CommunityWeightCalculator(),
    artifacts=artifacts,
    token_counter=TiktokenCounter(),
)

kp_generator = KeyPointsGenerator(
    llm=er_llm,
    prompt_builder=KeyPointsGeneratorPromptBuilder(show_references=True),
    context_builder=report_context_builder,
)

kp_aggregator = KeyPointsAggregator(
    llm=er_llm,
    prompt_builder=KeyPointsAggregatorPromptBuilder(show_references=True),
    context_builder=KeyPointsContextBuilder(
        token_counter=TiktokenCounter(),
    ),
)

global_search = GlobalSearch(
    kp_generator=kp_generator,
    kp_aggregator=kp_aggregator
)

================
File: reference_implementations/sec-10-q/kg-rag-cypher-script.py
================
# %% [markdown]
# # SEC 10-Q Knowledge Graph Construction
# 
# This notebook demonstrates constructing a knowledge graph from SEC 10-Q filings using LangChain. The approach uses LLM-based extraction to identify entities and relationships without pre-defining a schema.

# %%
import os
from pathlib import Path
from dotenv import load_dotenv
import neo4j
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
import networkx as nx
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Neo4j connection settings
NEO4J_URI = os.getenv('NEO4J_URI', 'bolt://localhost:7687')
NEO4J_USER = os.getenv('NEO4J_USER', 'neo4j')
NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD', 'password')

# Initialize OpenAI client
llm = ChatOpenAI(temperature=0, 
                 model_name="gpt-4o", 
                 openai_api_key=os.environ['OPENAI_API_KEY'])
llm_transformer = LLMGraphTransformer(llm=llm)

# %%
!docker run --name neo4j  -p7474:7474 -p7687:7687     -e NEO4J_AUTH=neo4j/password     -e NEO4J_PLUGINS='["apoc", "graph-data-science"]'

# %%
from langchain_community.document_loaders.pdf import PyPDFLoader

file_path = "../../data/sec-10-q/docs/2022 Q3 AAPL.pdf"
raw_documents = PyPDFLoader(file_path=file_path).load()

from langchain_text_splitters import TokenTextSplitter

text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)

documents = text_splitter.split_documents(raw_documents)

from langchain.vectorstores.utils import filter_complex_metadata
documents = filter_complex_metadata(documents)

# %%
import os
from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_text_splitters import TokenTextSplitter
from langchain.vectorstores.utils import filter_complex_metadata

# Initialize empty list to store all documents
documents = []

# Get the docs directory path
docs_path = "../../data/sec-10-q/docs"

# Loop through all files in the docs directory
for filename in os.listdir(docs_path):
    # Check if the file is an AAPL PDF
    if filename.endswith("AAPL.pdf"):
        # Construct full file path
        file_path = os.path.join(docs_path, filename)
        
        # Load and process the PDF
        try:
            raw_documents = PyPDFLoader(file_path=file_path).load()
            
            # Split the documents
            text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
            split_documents = text_splitter.split_documents(raw_documents)
            
            # Filter metadata
            processed_documents = filter_complex_metadata(split_documents)
            
            # Append to our collection
            documents.extend(processed_documents)
            
            print(f"Processed: {filename}")
        except Exception as e:
            print(f"Error processing {filename}: {str(e)}")

# Now all_documents contains the processed documents from all AAPL PDFs
print(f"Total documents processed: {len(documents)}")

# %%
graph_documents = llm_transformer.convert_to_graph_documents(tqdm(documents))

# %%
# Load graph_documents
import pickle

with open("graph_documents_appl.pkl", "rb") as f:
    graph_documents = pickle.load(f)

# %%
# save graph_documents
import pickle

with open("graph_documents_appl.pkl", "wb") as f:
    pickle.dump(graph_documents, f)

# %%
# Connect to Neo4j and store the graph
from langchain_neo4j import Neo4jGraph

graph = Neo4jGraph(
    url=NEO4J_URI,
    username=NEO4J_USER,
    password=NEO4J_PASSWORD
)

# Clear existing data
graph.query("MATCH (n) DETACH DELETE n")

graph.add_graph_documents(
    tqdm(graph_documents),
    baseEntityLabel=True,
    include_source=True
)

# %% [markdown]
# The knowledge graph has been constructed and stored in Neo4j. You can now query it using Cypher or use it for downstream tasks like question answering.

# %%
from langchain_core.prompts import PromptTemplate
# from langchain_neo4j.chains.graph_qa.prompts import CYPHER_GENERATION_PROMPT, CYPHER_QA_PROMPT

CYPHER_GENERATION_TEMPLATE = """
Task:
Generate Cypher statement to query a graph database.
Instructions:
Use only the provided relationship types and properties in the schema.
Do not use any other relationship types or properties that are not provided.
If you cannot generate a Cypher statement based on the provided schema, explain the reason to the user.
When you use `MATCH` with `WHERE` clauses, always first check the entities' or relationships; id property rather than name.
Schema:
{schema}
IMPORTANT:
Do not include any explanations or apologies in your response.
Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.
Do not include any text except the generated Cypher statement.
Examples:
Here are a few examples of generated Cypher statements for particular questions:
# Example 1: What entities relates to Nvidia Corporation?
MATCH p=()-[]->(n:Company)-[]->()
where n.id = "Nvidia Corporation"
RETURN P LIMIT 50
# Example 2: What assets do Nvidia Corporation have?
MATCH p=(n:Company)-[r:HAS]->()
where n.id = "Nvidia Corporation"
RETURN P LIMIT 25
The question is:
{question}
"""
CYPHER_GENERATION_PROMPT = PromptTemplate(
    input_variables=['schema', 'question'],
    template=CYPHER_GENERATION_TEMPLATE,
)

# %%
print('Node properties:\nOrganization {id: STRING}\nDocument {id: STRING, source: STRING, page: INTEGER, text: STRING}\nLocation {id: STRING}\nFinancial instrument {id: STRING}\nEntity {id: STRING}\nRegulation {id: STRING}\nDate {id: STRING}\nFinancial metric {id: STRING}\nProduct {id: STRING}\nTime period {id: STRING}\nPercentage {id: STRING}\nRisk {id: STRING}\nFinancial asset {id: STRING}\nProgram {id: STRING}\nCurrency {id: STRING}\nFinancial_instrument {id: STRING}\nEvent {id: STRING}\nConcept {id: STRING}\nRegion {id: STRING}\nYear {id: STRING}\nLaw {id: STRING}\nAmount {id: STRING}\nPerson {id: STRING}\nPosition {id: STRING}\nIdentifier {id: STRING}\nContact {id: STRING}\nClassification {id: STRING}\nDocument_section {id: STRING}\nFinancial_metric {id: STRING}\nFinancial_concept {id: STRING}\nService {id: STRING}\nTime_period {id: STRING}\nFinancial statement item {id: STRING}\nFinancial_term {id: STRING}\nLegal_term {id: STRING}\nWebsite {id: STRING}\nPage number {id: STRING}\nFinancial market {id: STRING}\nFinancial obligation {id: STRING}\nFinancial program {id: STRING}\nLegal_concept {id: STRING}\nAuthorization {id: STRING}\nRole {id: STRING}\nRelationship properties:\n\nThe relationships:\n(:Organization)-[:APPEALED_DECISION_IN]->(:Organization)\n(:Organization)-[:FILED_LAWSUIT_AGAINST]->(:Organization)\n(:Organization)-[:FILED_LAWSUIT_IN]->(:Organization)\n(:Organization)-[:RULED_AGAINST]->(:Organization)\n(:Organization)-[:FOUND_VIOLATION_OF]->(:Legal_concept)\n(:Organization)-[:RULED_IN_FAVOR_OF]->(:Organization)\n(:Organization)-[:INCORPORATED_IN]->(:Location)\n(:Organization)-[:FILED_WITH]->(:Organization)\n(:Organization)-[:HAS_IDENTIFIER]->(:Identifier)\n(:Organization)-[:INTRODUCED]->(:Product)\n(:Organization)-[:HAS_ADDRESS]->(:Location)\n(:Organization)-[:ANNOUNCED]->(:Product)\n(:Organization)-[:HAS_CONTACT]->(:Contact)\n(:Organization)-[:FILED]->(:Document)\n(:Organization)-[:REPORTS]->(:Financial statement item)\n(:Organization)-[:REPORTS]->(:Concept)\n(:Organization)-[:REPORT_DATE]->(:Date)\n(:Organization)-[:PUBLISHED]->(:Document)\n(:Organization)-[:FINANCIAL_STATEMENT_DATE]->(:Date)\n(:Organization)-[:POTENTIAL_FUTURE_IMPACT]->(:Event)\n(:Organization)-[:REFERENCE]->(:Document)\n(:Organization)-[:PROVIDES_INFORMATION_ON]->(:Website)\n(:Organization)-[:REPORTED]->(:Concept)\n(:Organization)-[:REPORTED]->(:Financial_metric)\n(:Organization)-[:REPORTED]->(:Financial_concept)\n(:Organization)-[:REPORTED]->(:Financial metric)\n(:Organization)-[:AUTHOR_OF]->(:Document)\n(:Organization)-[:HAS]->(:Financial_concept)\n(:Organization)-[:HAS]->(:Financial_metric)\n(:Organization)-[:HAS]->(:Concept)\n(:Organization)-[:EVALUATION_DATE]->(:Date)\n(:Organization)-[:DOCUMENTED_IN]->(:Document)\n(:Organization)-[:UTILIZED]->(:Authorization)\n(:Organization)-[:CONFORMITY]->(:Concept)\n(:Organization)-[:COMPLIANCE]->(:Regulation)\n(:Organization)-[:COMPLIANCE]->(:Law)\n(:Organization)-[:HAS_DOCUMENT]->(:Document)\n(:Organization)-[:AUTHORIZED]->(:Program)\n(:Organization)-[:AUTHORIZED]->(:Financial program)\n(:Organization)-[:DOCUMENT_OF]->(:Document)\n(:Organization)-[:AUTHORIZED_PURCHASE]->(:Date)\n(:Organization)-[:CLASSIFIED_AS]->(:Classification)\n(:Organization)-[:ELECTED_NOT_TO_USE_EXTENDED_TRANSITION_PERIOD]->(:Entity)\n(:Organization)-[:IS_NOT]->(:Entity)\n(:Organization)-[:SUBJECT_TO]->(:Regulation)\n(:Organization)-[:REFERENCED_IN]->(:Document)\n(:Organization)-[:HAS_NON-TRADE_RECEIVABLES_FROM]->(:Organization)\n(:Organization)-[:SELLS_TO_VENDORS]->(:Product)\n(:Organization)-[:MANAGES_ON_GEOGRAPHIC_BASIS]->(:Region)\n(:Organization)-[:DISCUSSED_IN]->(:Document)\n(:Organization)-[:FILED_COUNTERCLAIM_AGAINST]->(:Organization)\n(:Organization)-[:OPERATES]->(:Product)\n(:Organization)-[:FILED_CROSS-APPEAL_IN]->(:Organization)\n(:Organization)-[:DATE_OF_FINANCIAL_DATA]->(:Date)\n(:Organization)-[:ACCOUNTED_FOR_42%_OF_TRADE_RECEIVABLES]->(:Date)\n(:Organization)-[:ACCOUNTED_FOR_36%_OF_TRADE_RECEIVABLES]->(:Date)\n(:Organization)-[:SUFFICIENT_TO_SATISFY_REQUIREMENTS]->(:Financial asset)\n(:Organization)-[:SUFFICIENT_TO_SATISFY_REQUIREMENTS]->(:Financial instrument)\n(:Organization)-[:SUFFICIENT_TO_SATISFY_REQUIREMENTS]->(:Time period)\n(:Organization)-[:HIGHER_EFFECTIVE_TAX_RATE]->(:Time period)\n(:Organization)-[:CONTINUED_ACCESS]->(:Financial market)\n(:Organization)-[:UTILIZES_OUTSOURCING_PARTNERS]->(:Financial obligation)\n(:Organization)-[:HAD_OBLIGATIONS]->(:Date)\n(:Organization)-[:QUARTERLY_CASH_DIVIDEND]->(:Amount)\n(:Organization)-[:AUTHORIZED_BY]->(:Program)\n(:Organization)-[:AUTHORIZED_BY]->(:Financial program)\n(:Organization)-[:COMPARED_TO]->(:Time period)\n(:Organization)-[:ISSUED_REGULATIONS]->(:Time period)\n(:Document)-[:MENTIONS]->(:Regulation)\n(:Document)-[:MENTIONS]->(:Law)\n(:Document)-[:MENTIONS]->(:Date)\n(:Document)-[:MENTIONS]->(:Document)\n(:Document)-[:MENTIONS]->(:Organization)\n(:Document)-[:MENTIONS]->(:Product)\n(:Document)-[:MENTIONS]->(:Legal_concept)\n(:Document)-[:MENTIONS]->(:Amount)\n(:Document)-[:MENTIONS]->(:Authorization)\n(:Document)-[:MENTIONS]->(:Identifier)\n(:Document)-[:MENTIONS]->(:Financial_instrument)\n(:Document)-[:MENTIONS]->(:Location)\n(:Document)-[:MENTIONS]->(:Contact)\n(:Document)-[:MENTIONS]->(:Entity)\n(:Document)-[:MENTIONS]->(:Classification)\n(:Document)-[:MENTIONS]->(:Document_section)\n(:Document)-[:MENTIONS]->(:Financial_metric)\n(:Document)-[:MENTIONS]->(:Financial metric)\n(:Document)-[:MENTIONS]->(:Concept)\n(:Document)-[:MENTIONS]->(:Financial_concept)\n(:Document)-[:MENTIONS]->(:Service)\n(:Document)-[:MENTIONS]->(:Region)\n(:Document)-[:MENTIONS]->(:Time_period)\n(:Document)-[:MENTIONS]->(:Percentage)\n(:Document)-[:MENTIONS]->(:Financial instrument)\n(:Document)-[:MENTIONS]->(:Financial asset)\n(:Document)-[:MENTIONS]->(:Risk)\n(:Document)-[:MENTIONS]->(:Financial statement item)\n(:Document)-[:MENTIONS]->(:Program)\n(:Document)-[:MENTIONS]->(:Financial program)\n(:Document)-[:MENTIONS]->(:Event)\n(:Document)-[:MENTIONS]->(:Financial_term)\n(:Document)-[:MENTIONS]->(:Legal_term)\n(:Document)-[:MENTIONS]->(:Website)\n(:Document)-[:MENTIONS]->(:Time period)\n(:Document)-[:MENTIONS]->(:Year)\n(:Document)-[:MENTIONS]->(:Person)\n(:Document)-[:MENTIONS]->(:Position)\n(:Document)-[:MENTIONS]->(:Role)\n(:Document)-[:MENTIONS]->(:Currency)\n(:Document)-[:MENTIONS]->(:Page number)\n(:Document)-[:MENTIONS]->(:Financial obligation)\n(:Document)-[:MENTIONS]->(:Financial market)\n(:Document)-[:CONTAINS]->(:Document_section)\n(:Document)-[:CONTAINS]->(:Page number)\n(:Document)-[:REPORT_OF]->(:Organization)\n(:Document)-[:FILED_WITH]->(:Organization)\n(:Document)-[:BELONGS_TO]->(:Organization)\n(:Document)-[:COMPLIES_WITH]->(:Regulation)\n(:Document)-[:COMPLIES_WITH]->(:Law)\n(:Location)-[:UNFAVORABLE_IMPACT]->(:Currency)\n(:Location)-[:UNFAVORABLE_IMPACT]->(:Concept)\n(:Location)-[:FAVORABLE_IMPACT]->(:Currency)\n(:Location)-[:HAS_VALUE]->(:Financial_metric)\n(:Location)-[:LOWER_NET_SALES]->(:Product)\n(:Location)-[:HIGHER_NET_SALES]->(:Product)\n(:Location)-[:HIGHER_NET_SALES]->(:Concept)\n(:Location)-[:HIGHER_NET_SALES]->(:Service)\n(:Financial instrument)-[:DATE]->(:Date)\n(:Financial instrument)-[:ISSUED_BY]->(:Organization)\n(:Entity)-[:CLASSIFIED_AS]->(:Classification)\n(:Entity)-[:ELECTED_NOT_TO_USE_EXTENDED_TRANSITION_PERIOD]->(:Entity)\n(:Entity)-[:IS_NOT]->(:Entity)\n(:Entity)-[:SUBJECT_TO]->(:Regulation)\n(:Entity)-[:ISSUED_AND_OUTSTANDING_ON]->(:Date)\n(:Entity)-[:AUTHORIZED]->(:Program)\n(:Entity)-[:AUTHORIZED]->(:Financial program)\n(:Entity)-[:PART_OF]->(:Organization)\n(:Entity)-[:PART_OF]->(:Entity)\n(:Date)-[:BEGINNING_CASH_BALANCE]->(:Amount)\n(:Date)-[:NET_INCOME]->(:Amount)\n(:Date)-[:DEPRECIATION_AMORTIZATION]->(:Amount)\n(:Date)-[:SHARE_BASED_COMPENSATION]->(:Amount)\n(:Date)-[:DEFERRED_INCOME_TAX]->(:Amount)\n(:Date)-[:OTHER_ADJUSTMENTS]->(:Amount)\n(:Date)-[:ACCOUNTS_RECEIVABLE_CHANGE]->(:Amount)\n(:Date)-[:INVENTORIES_CHANGE]->(:Amount)\n(:Date)-[:VENDOR_NON_TRADE_RECEIVABLES_CHANGE]->(:Amount)\n(:Date)-[:OTHER_ASSETS_CHANGE]->(:Amount)\n(:Date)-[:ACCOUNTS_PAYABLE_CHANGE]->(:Amount)\n(:Date)-[:DEFERRED_REVENUE_CHANGE]->(:Amount)\n(:Date)-[:OTHER_LIABILITIES_CHANGE]->(:Amount)\n(:Date)-[:CASH_GENERATED_OPERATING_ACTIVITIES]->(:Amount)\n(:Date)-[:PURCHASES_MARKETABLE_SECURITIES]->(:Amount)\n(:Date)-[:PROCEEDS_MATURITIES_MARKETABLE_SECURITIES]->(:Amount)\n(:Date)-[:PROCEEDS_SALES_MARKETABLE_SECURITIES]->(:Amount)\n(:Date)-[:PAYMENTS_ACQUISITION_PROPERTY]->(:Amount)\n(:Date)-[:PAYMENTS_BUSINESS_ACQUISITIONS]->(:Amount)\n(:Date)-[:OTHER_INVESTING_ACTIVITIES]->(:Amount)\n(:Date)-[:CASH_USED_INVESTING_ACTIVITIES]->(:Amount)\n(:Date)-[:PAYMENTS_TAXES_EQUITY_AWARDS]->(:Amount)\n(:Date)-[:PAYMENTS_DIVIDENDS]->(:Amount)\n(:Date)-[:REPURCHASES_COMMON_STOCK]->(:Amount)\n(:Date)-[:REPAYMENTS_TERM_DEBT]->(:Amount)\n(:Date)-[:PROCEEDS_COMMERCIAL_PAPER]->(:Amount)\n(:Date)-[:PROCEEDS_ISSUANCE_TERM_DEBT]->(:Amount)\n(:Product)-[:CATEGORY]->(:Financial_metric)\n(:Product)-[:COMPARISON_YEAR]->(:Date)\n(:Product)-[:COMPARISON_YEAR]->(:Time period)\n(:Product)-[:COMPARISON_YEAR]->(:Year)\n(:Product)-[:NET_SALES_INCREASED]->(:Date)\n(:Product)-[:NET_SALES_INCREASED]->(:Year)\n(:Product)-[:NET_SALES_INCREASED]->(:Time period)\n(:Product)-[:HIGHER_PROPORTION_OF_SALES]->(:Location)\n(:Product)-[:HIGHER_PROPORTION_OF_SALES]->(:Region)\n(:Product)-[:CONTRIBUTES_TO]->(:Financial_metric)\n(:Product)-[:NET_SALES_DECREASED]->(:Time period)\n(:Product)-[:OFFSET_BY]->(:Concept)\n(:Product)-[:INCREASED]->(:Financial metric)\n(:Product)-[:INCREASED]->(:Financial_metric)\n(:Product)-[:COMPARED_TO]->(:Time_period)\n(:Product)-[:DUE_TO_HIGHER_NET_SALES_FROM]->(:Concept)\n(:Product)-[:INCREASED_DURING]->(:Time_period)\n(:Product)-[:LOWER_NET_SALES]->(:Product)\n(:Product)-[:POWERED_BY]->(:Product)\n(:Product)-[:HIGHER_NET_SALES]->(:Product)\n(:Product)-[:DECREASED]->(:Financial metric)\n(:Product)-[:DECREASED]->(:Financial_metric)\n(:Percentage)-[:TIME_FRAME]->(:Time_period)\n(:Risk)-[:EXPOSED_TO]->(:Organization)\n(:Financial asset)-[:DATE]->(:Date)\n(:Program)-[:AUTHORIZED_BY]->(:Organization)\n(:Program)-[:AUTHORIZED_BY]->(:Entity)\n(:Financial_instrument)-[:REGISTERED_ON]->(:Organization)\n(:Financial_instrument)-[:ISSUED_BY]->(:Organization)\n(:Financial_instrument)-[:DATE]->(:Date)\n(:Financial_instrument)-[:OWNED_BY]->(:Organization)\n(:Financial_instrument)-[:USED_BY]->(:Organization)\n(:Financial_instrument)-[:VALUE_14,250_MILLION]->(:Date)\n(:Financial_instrument)-[:VALUE_15,954_MILLION]->(:Date)\n(:Financial_instrument)-[:VALUE_(19,281)_MILLION]->(:Date)\n(:Financial_instrument)-[:VALUE_(17,857)_MILLION]->(:Date)\n(:Financial_instrument)-[:HELD_BY]->(:Organization)\n(:Financial_instrument)-[:COMPONENT_OF]->(:Financial_instrument)\n(:Event)-[:IMPACT]->(:Organization)\n(:Concept)-[:INCLUDES]->(:Concept)\n(:Concept)-[:INCLUDES]->(:Financial_metric)\n(:Concept)-[:INCLUDES]->(:Financial_concept)\n(:Concept)-[:INCLUDES]->(:Financial instrument)\n(:Concept)-[:INCLUDES]->(:Financial_instrument)\n(:Concept)-[:COMPONENT_OF]->(:Financial_concept)\n(:Concept)-[:DATE]->(:Date)\n(:Concept)-[:OFFSET_BY]->(:Concept)\n(:Concept)-[:CONTRIBUTES_TO]->(:Financial_metric)\n(:Concept)-[:CATEGORY]->(:Financial_metric)\n(:Concept)-[:INCREASED]->(:Financial metric)\n(:Concept)-[:INCREASED]->(:Financial_metric)\n(:Concept)-[:COMPARED_TO]->(:Time_period)\n(:Concept)-[:COMPARED_TO]->(:Date)\n(:Concept)-[:COMPARED_TO]->(:Time period)\n(:Concept)-[:COMPARED_TO]->(:Year)\n(:Concept)-[:DUE_TO_HIGHER_NET_SALES_FROM]->(:Concept)\n(:Concept)-[:INCREASED_DURING]->(:Time_period)\n(:Concept)-[:INCREASED_DURING]->(:Time period)\n(:Concept)-[:ISSUED_BY]->(:Organization)\n(:Concept)-[:VALUE_126,918_MILLION]->(:Date)\n(:Concept)-[:VALUE_76,234_MILLION]->(:Date)\n(:Concept)-[:VALUE_76,475_MILLION]->(:Date)\n(:Concept)-[:VALUE_84,506_MILLION]->(:Date)\n(:Concept)-[:VALUE_16,875_MILLION]->(:Date)\n(:Concept)-[:VALUE_20,775_MILLION]->(:Date)\n(:Concept)-[:ACTIVITY]->(:Event)\n(:Concept)-[:EXPENSE]->(:Date)\n(:Concept)-[:COST]->(:Date)\n(:Concept)-[:RELATED_TO]->(:Organization)\n(:Concept)-[:AFFECTS]->(:Concept)\n(:Concept)-[:INVOLVES]->(:Entity)\n(:Concept)-[:INVOLVES]->(:Concept)\n(:Concept)-[:RELATIVE_TO]->(:Currency)\n(:Concept)-[:RELATIVE_TO]->(:Concept)\n(:Concept)-[:DECREASED]->(:Date)\n(:Concept)-[:DECREASED]->(:Year)\n(:Region)-[:UNFAVORABLE_IMPACT]->(:Currency)\n(:Region)-[:UNFAVORABLE_IMPACT]->(:Concept)\n(:Region)-[:FAVORABLE_IMPACT]->(:Currency)\n(:Region)-[:HAS_VALUE]->(:Financial_metric)\n(:Region)-[:LOWER_NET_SALES]->(:Product)\n(:Region)-[:HIGHER_NET_SALES]->(:Product)\n(:Region)-[:HIGHER_NET_SALES]->(:Concept)\n(:Region)-[:HIGHER_NET_SALES]->(:Service)\n(:Region)-[:INCREASED_NET_SALES]->(:Product)\n(:Region)-[:INCREASED_NET_SALES]->(:Concept)\n(:Region)-[:INCREASED_NET_SALES]->(:Service)\n(:Person)-[:HAS]->(:Concept)\n(:Person)-[:CERTIFY]->(:Document)\n(:Person)-[:REVIEWED]->(:Document)\n(:Person)-[:HOLDS_POSITION]->(:Role)\n(:Person)-[:SIGNED_ON]->(:Date)\n(:Person)-[:CERTIFYING_OFFICER]->(:Organization)\n(:Person)-[:CERTIFYING_OFFICER]->(:Entity)\n(:Person)-[:AFFILIATION]->(:Organization)\n(:Person)-[:HOLDS]->(:Position)\n(:Person)-[:DISCLOSED_TO]->(:Entity)\n(:Person)-[:DISCLOSED_TO]->(:Organization)\n(:Classification)-[:DEFINED_IN]->(:Regulation)\n(:Financial_metric)-[:INCLUDES]->(:Concept)\n(:Financial_metric)-[:INCLUDES]->(:Financial_metric)\n(:Financial_metric)-[:INCLUDES]->(:Financial_concept)\n(:Financial_metric)-[:COMPONENT_OF]->(:Financial_concept)\n(:Financial_metric)-[:DATE]->(:Date)\n(:Financial_metric)-[:REPORTED_ON]->(:Date)\n(:Financial_metric)-[:PROPORTION_OF]->(:Financial_metric)\n(:Financial_metric)-[:HIGHER_PROPORTION_IN]->(:Location)\n(:Financial_metric)-[:HIGHER_PROPORTION_IN]->(:Region)\n(:Financial_metric)-[:VALUE_AS_OF]->(:Amount)\n(:Financial_metric)-[:REALIZED_IN]->(:Percentage)\n(:Financial_metric)-[:CONTRIBUTES_TO]->(:Financial_metric)\n(:Financial_metric)-[:REDUCES]->(:Financial_metric)\n(:Financial_concept)-[:INCLUDES]->(:Concept)\n(:Financial_concept)-[:INCLUDES]->(:Financial_concept)\n(:Financial_concept)-[:COMPONENT_OF]->(:Financial_concept)\n(:Financial_concept)-[:RELATED_TO]->(:Organization)\n(:Service)-[:OFFSET_BY]->(:Concept)\n(:Service)-[:CONTRIBUTES_TO]->(:Financial_metric)\n(:Service)-[:CATEGORY]->(:Financial_metric)\n(:Service)-[:INCREASED]->(:Financial metric)\n(:Service)-[:INCREASED]->(:Financial_metric)\n(:Service)-[:COMPARED_TO]->(:Time_period)\n(:Service)-[:DUE_TO_HIGHER_NET_SALES_FROM]->(:Concept)\n(:Service)-[:INCREASED_DURING]->(:Time_period)\n(:Financial statement item)-[:AS_OF]->(:Date)\n(:Financial statement item)-[:COMPONENT_OF]->(:Financial statement item)\n(:Financial_term)-[:RECORDED_ON]->(:Date)\n(:Legal_term)-[:ASSOCIATED_WITH]->(:Organization)\n(:Financial obligation)-[:PAYABLE_WITHIN_12_MONTHS]->(:Amount)\n(:Financial obligation)-[:TOTAL_AMOUNT]->(:Amount)\n(:Financial program)-[:AUTHORIZED_BY]->(:Organization)\n(:Financial program)-[:AUTHORIZED_BY]->(:Entity)\n(:Authorization)-[:UTILIZATION]->(:Amount)\n(:Authorization)-[:TOTAL_AUTHORIZATION]->(:Amount)')

# %%
from langchain_neo4j import GraphCypherQAChain
# from langchain_neo4j.chains.graph_qa.prompts import CYPHER_GENERATION_PROMPT, CYPHER_QA_PROMPT

graph_chain = GraphCypherQAChain.from_llm(
    # llm,
    cypher_llm=llm,
    qa_llm=llm,
    # qa_prompt=CYPHER_QA_PROMPT,
    cypher_prompt=CYPHER_GENERATION_PROMPT,
    graph=graph,
    verbose=True,
    max_depth=2,
    max_hops=3,
    # return_intermediate_steps=True,
    # return_direct=True,
    validate_cypher=True,
    allow_dangerous_requests=True,
)

# %%
# Load the CSV file
import pandas as pd
import time
df = pd.read_csv("../../data/sec-10-q/synthetic_qna_data_7_gpt4o.csv")

# Filter for rows where Source Docs contains only AAPL
apple_df = df[df['Source Docs'].str.contains('AAPL', na=False)]

# Take first 10 samples
apple_df = apple_df.head(10)

# Evaluate the model
correct = 0
for i, row in apple_df.iterrows():
    question = row["New Question"]
    answer = row["New Answer"]
    print(f"\nQuestion: {question}")
    print(f"Expected Answer: {answer}")
    response = graph_chain.invoke(input=question)
    print(f"Model Response: {response}")
    if response == answer:
        correct += 1
    time.sleep(1)
        
print(f"\nAccuracy: {correct / 10}")

# %%
graph_chain.invoke(input="Where was Apple Inc. Incorporated?")

# %%
graph_documents[10].relationships

# %%
graph_chain.invoke(input=" On April 1, 2023, what was the Amount of CASH_BEGINNING_BALANCE?")

# %%
graph_chain.invoke(input="What assets does Apple Inc. have?")

# %%
graph_chain.invoke(input="Apple inc. What was the amount for Cash Used In Investing Activities in 2023 Q3?")

# %%
graph_chain.invoke(input="What was Apple Inc's Products gross margin percentage for the third quarter of 2022 as reported in their most recent 10-Q? Provide the percentage rounded to one decimal place.")

================
File: reference_implementations/sec-10-q/kg-rag-entity-v2.py
================
# %% [markdown]
# # SEC 10-Q Knowledge Graph Construction
# 
# This notebook demonstrates constructing a knowledge graph from SEC 10-Q filings using LangChain. The approach uses LLM-based extraction to identify entities and relationships without pre-defining a schema.

# %%
import os
from pathlib import Path
from dotenv import load_dotenv
import neo4j
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
import networkx as nx
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from dotenv import load_dotenv

load_dotenv()

# Initialize OpenAI client
llm = ChatOpenAI(temperature=0, 
                 model_name="gpt-4o", 
                 api_key=os.environ["OPENAI_API_KEY"])
llm_transformer = LLMGraphTransformer(llm=llm)

# %%
import os
import pickle

# Check if "graph_documents.pkl" exists
if os.path.exists("graph_documents.pkl"):
    # Load graph_documents from the file
    with open("graph_documents.pkl", "rb") as f:
        graph_documents = pickle.load(f)
    print("Loaded graph_documents from graph_documents.pkl")
else:
    # Convert documents to graph documents
    graph_documents = llm_transformer.convert_to_graph_documents(tqdm(documents))
    
    # Save graph_documents to the file
    with open("graph_documents.pkl", "wb") as f:
        pickle.dump(graph_documents, f)
    print("Converted documents to graph_documents and saved to graph_documents.pkl")

# %%
from langchain_community.graphs.networkx_graph import NetworkxEntityGraph

graph = NetworkxEntityGraph()

# Add nodes to the graph
for doc in graph_documents:
    for node in doc.nodes:
        graph.add_node(node.id)

for doc in graph_documents:
    for edge in doc.relationships:
        graph._graph.add_edge(
            edge.source.id,
            edge.target.id,
            relation=edge.type,
        )

# %%
import random

# Define the depth to go down
n_hops = 5

# Select a random node from the graph
random_node = "Apple Inc."

# Function to recursively add a random neighbor up to n_hops
def add_neighbors(node, depth, nodes_to_include):
    if depth > 0:
        neighbors = list(graph._graph.neighbors(node))
        if neighbors:
            random_neighbor = random.choice(neighbors)
            nodes_to_include.add(random_neighbor)
            add_neighbors(random_neighbor, depth - 1, nodes_to_include)

# Create 16 subplots
fig, axes = plt.subplots(4, 4, figsize=(20, 20))
axes = axes.flatten()

for i in range(16):
    # Initialize the list of nodes to include in the subgraph
    nodes_to_include = {random_node}
    
    # Add neighbors starting from the random node
    add_neighbors(random_node, n_hops, nodes_to_include)
    
    # Extract the subgraph containing the selected nodes
    subgraph = graph._graph.subgraph(nodes_to_include)
    
    # Draw the subgraph
    pos = nx.spring_layout(subgraph)
    nx.draw(subgraph, pos, with_labels=True, node_size=2000, node_color="skyblue", font_size=10, font_weight="bold", ax=axes[i])
    edge_labels = nx.get_edge_attributes(subgraph, 'relation')
    nx.draw_networkx_edge_labels(subgraph, pos, edge_labels=edge_labels, font_color='red', ax=axes[i])
    axes[i].set_title(f"Subgraph {i+1}")

plt.tight_layout()
plt.show()

# %% [markdown]
# The knowledge graph has been constructed and stored in Neo4j. You can now query it using Cypher or use it for downstream tasks like question answering.

# %%
from langchain_openai import OpenAIEmbeddings
import numpy as np
from scipy.spatial.distance import cosine

class EntityLinker:
    def __init__(self, graph):
        self.node_ids = list(graph._graph.nodes())
        self.embeddings_model = OpenAIEmbeddings()
        self.node_embeddings = self.embeddings_model.embed_documents(self.node_ids)
    
    def link_entities(self, query, top_n=3):
        # Extract entities using LLM
        entity_prompt = f"""Extract key entities from this query: {query}
        Return as comma-separated list:"""
        entities = llm.invoke(entity_prompt).content.split(',')
        
        # Find closest nodes for each entity
        matched_nodes = []
        for entity in entities:
            query_embed = self.embeddings_model.embed_query(entity.strip())
            similarities = [1 - cosine(query_embed, node_embed) for node_embed in self.node_embeddings]
            top_indices = np.argsort(similarities)[-top_n:]
            matched_nodes.extend([self.node_ids[i] for i in top_indices])
        
        return list(set(matched_nodes))

# %%
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
import random

def expand_subgraph(graph, seed_nodes, depth=2):
    """
    Expand subgraph from seed nodes while preserving edge attributes
    """
    subgraph = nx.DiGraph()
    nodes_to_explore = set(seed_nodes)
    explored = set()
    
    for _ in range(depth):
        current_level = nodes_to_explore - explored
        if not current_level:
            break
            
        for node in current_level:
            if node in graph:
                # Add outgoing edges
                for _, target, data in graph.edges(node, data=True):
                    subgraph.add_edge(node, target, **data)
                
                # Add incoming edges
                for source, _, data in graph.in_edges(node, data=True):
                    subgraph.add_edge(source, node, **data)
                
                # Add neighbors to exploration set
                nodes_to_explore.update(graph.successors(node))
                nodes_to_explore.update(graph.predecessors(node))
        
        explored.update(current_level)
    
    return subgraph

class GraphQAChain:
    def __init__(self, llm, nx_graph, entity_linker, verbose=True):
        self.llm = llm
        self.nx_graph = nx_graph
        self.entity_linker = entity_linker
        self.verbose = verbose
        self.qa_prompt = PromptTemplate(
            template="""Based on the following knowledge graph triplets:
            {triplets}
            
            Please answer this question: {question}
            
            If you cannot find the exact information in the triplets, say "I cannot find the specific information in the available data."
            """,
            input_variables=["triplets", "question"]
        )

    def _print_verbose(self, msg, data=None):
        """Helper method for verbose output with optional data"""
        if self.verbose:
            print("\n" + "="*50)
            print(msg)
            if data:
                if isinstance(data, list):
                    for item in data:
                        print(f"   {item}")
                else:
                    print(data)
            print("="*50 + "\n")

    def invoke(self, query):
        # Step 1: Entity linking
        seed_nodes = self.entity_linker.link_entities(query)
        if self.verbose:
            self._print_verbose(" Detected Entities:", seed_nodes)
        
        # Step 2: Subgraph expansion
        subgraph = expand_subgraph(self.nx_graph, seed_nodes)
        
        # Step 3: Triplet extraction and formatting
        try:
            triplets = [
                f"{u}  {data.get('relation', 'RELATED_TO')}  {v}"
                for u, v, data in subgraph.edges(data=True)
            ]
            
            if not triplets:
                self._print_verbose(" No relevant triplets found in knowledge graph")
                return self.llm.invoke("I cannot find any relevant connections in the knowledge graph to answer this question.")
            
            # If there are many triplets, show a sample in verbose mode
            if self.verbose:
                sample_size = min(5, len(triplets))
                sample_triplets = random.sample(triplets, sample_size)
                self._print_verbose(
                    f" Found {len(triplets)} relevant connections. Sample of {sample_size}:", 
                    sample_triplets
                )
            
            # Step 4: LLM reasoning
            response = self.llm.invoke(self.qa_prompt.format(
                triplets="\n".join(triplets),
                question=query
            ))
            
            if self.verbose:
                self._print_verbose(" Final Answer:", response.content)
            
            return response

        except Exception as e:
            error_msg = f"Error processing graph: {str(e)}"
            self._print_verbose(" Error:", error_msg)
            return self.llm.invoke("There was an error processing the knowledge graph structure.")

# %%
# Initialize linker component
entity_linker = EntityLinker(graph)

# %%
enhanced_chain = GraphQAChain(llm, graph._graph, entity_linker)

# Sample usage
response = enhanced_chain.invoke(
    "What was Apple Inc's Products gross margin percentage for the third quarter of 2022? Provide the percentage rounded to one decimal place."
)

# %%
enhanced_chain.invoke(invoke(input="Where was Apple Inc. Incorporated?"))

# %%
graph_chain.invoke(input=" On April 1, 2023, what was the Amount of CASH_BEGINNING_BALANCE?")

# %%
graph_chain.invoke(input="What assets does Apple Inc. have?")

# %%
graph_chain.invoke(input="Apple inc. What was the amount for Cash Used In Investing Activities in 2023 Q3?")

# %%
graph_chain.invoke(input="What was Apple Inc's Products gross margin percentage for the third quarter of 2022? Provide the percentage rounded to one decimal place.") 

# %%
# Load the CSV file
df = pd.read_csv("../../data/sec-10-q/synthetic_qna_data_7_gpt4o.csv")

# Filter for rows where Source Docs contains only AAPL
apple_df = df[df['Source Docs'].str.contains('AAPL', na=False)]

# Take first 10 samples
apple_df = apple_df.head(10)

# Evaluate the model
correct = 0
for i, row in apple_df.iterrows():
    question = row["New Question"]
    answer = row["New Answer"]
    print(f"\nQuestion: {question}")
    print(f"Expected Answer: {answer}")
    response = graph_chain.invoke(input=question)
    print(f"Model Response: {response}")
    if response == answer:
        correct += 1
        
print(f"\nAccuracy: {correct / 10}")

================
File: reference_implementations/sec-10-q/pdf_processor.py
================
import os
import subprocess
import tempfile
from pathlib import Path
from typing import List, Optional, Tuple
import pytesseract
from pdf2image import convert_from_path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PDFProcessor:
    """Process PDFs using poppler and tesseract for text extraction."""
    
    def __init__(self, tesseract_path: Optional[str] = None, poppler_path: Optional[str] = None):
        """Initialize PDF processor with optional paths to tesseract and poppler.
        
        Args:
            tesseract_path: Path to tesseract executable
            poppler_path: Path to poppler binaries
        """
        if tesseract_path:
            pytesseract.pytesseract.tesseract_cmd = tesseract_path
        
        self.poppler_path = poppler_path
        
    def _extract_text_poppler(self, pdf_path: Path) -> str:
        """Extract text from PDF using pdftotext (poppler)."""
        try:
            with tempfile.NamedTemporaryFile(suffix='.txt') as tmp:
                # Use pdftotext command line tool
                cmd = ['pdftotext', '-layout', str(pdf_path), tmp.name]
                subprocess.run(cmd, check=True, capture_output=True)
                return tmp.read().decode('utf-8')
        except subprocess.CalledProcessError as e:
            logger.warning(f"pdftotext failed for {pdf_path}: {e}")
            return ""
        
    def _perform_ocr(self, pdf_path: Path) -> str:
        """Perform OCR on PDF using tesseract."""
        try:
            # Convert PDF to images
            images = convert_from_path(
                pdf_path,
                poppler_path=self.poppler_path,
                fmt='jpeg',
                grayscale=True,
                dpi=300
            )
            
            # Perform OCR on each image
            text_parts = []
            for i, image in enumerate(images):
                logger.info(f"Processing page {i+1}/{len(images)} with OCR")
                text = pytesseract.image_to_string(image, lang='eng')
                text_parts.append(text)
                
            return "\n\n".join(text_parts)
            
        except Exception as e:
            logger.error(f"OCR failed for {pdf_path}: {e}")
            return ""
        
    def process_pdf(self, pdf_path: Path) -> Tuple[str, str]:
        """Process a PDF file using both text extraction and OCR if needed.
        
        Args:
            pdf_path: Path to the PDF file
        
        Returns:
            Tuple of (extracted_text, method_used)
        """
        logger.info(f"Processing {pdf_path}")
        
        # Try poppler first
        text = self._extract_text_poppler(pdf_path)
        
        # If we got meaningful text, return it
        if len(text.strip()) > 100:  # Arbitrary threshold for "meaningful" text
            logger.info(f"Successfully extracted text using poppler from {pdf_path}")
            return text, "poppler"
            
        # If poppler didn't get good results, try OCR
        logger.info(f"Poppler extraction insufficient for {pdf_path}, attempting OCR")
        ocr_text = self._perform_ocr(pdf_path)
        
        if len(ocr_text.strip()) > 100:
            logger.info(f"Successfully extracted text using OCR from {pdf_path}")
            return ocr_text, "ocr"
            
        # If both methods failed
        logger.warning(f"Both text extraction methods failed for {pdf_path}")
        return "", "failed"

def process_pdf_directory(
    input_dir: Path,
    output_dir: Path,
    file_pattern: str = "*.pdf"
) -> List[Tuple[Path, str, str]]:
    """Process all PDFs in a directory and save extracted text.
    
    Args:
        input_dir: Directory containing PDF files
        output_dir: Directory to save extracted text files
        file_pattern: Glob pattern for PDF files
        
    Returns:
        List of tuples (pdf_path, text_path, method_used)
    """
    os.makedirs(output_dir, exist_ok=True)
    processor = PDFProcessor()
    results = []
    
    for pdf_path in input_dir.glob(file_pattern):
        try:
            # Process the PDF
            text, method = processor.process_pdf(pdf_path)
            
            if text:
                # Save the extracted text
                text_path = output_dir / f"{pdf_path.stem}.txt"
                text_path.write_text(text)
                
                results.append((pdf_path, text_path, method))
                logger.info(f"Saved extracted text to {text_path}")
            else:
                logger.error(f"No text extracted from {pdf_path}")
                
        except Exception as e:
            logger.error(f"Error processing {pdf_path}: {e}")
            
    return results

def main():
    """Main function to demonstrate PDF processing."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Process PDF files for text extraction')
    parser.add_argument('--input-dir', type=Path, required=True, help='Directory containing PDF files')
    parser.add_argument('--output-dir', type=Path, required=True, help='Directory to save extracted text')
    parser.add_argument('--pattern', default="*.pdf", help='File pattern for PDFs')
    
    args = parser.parse_args()
    
    logger.info(f"Processing PDFs from {args.input_dir}")
    results = process_pdf_directory(args.input_dir, args.output_dir, args.pattern)
    
    # Print summary
    logger.info("\nProcessing Summary:")
    methods_used = {}
    for _, _, method in results:
        methods_used[method] = methods_used.get(method, 0) + 1
        
    for method, count in methods_used.items():
        logger.info(f"{method}: {count} files")
        
    logger.info(f"Total files processed: {len(results)}")

if __name__ == "__main__":
    main()

================
File: reference_implementations/sec-10-q/README.md
================
# SEC 10-Q Reference Implementation Overview

## Introduction
This folder contains a collection of Jupyter notebooks, Python scripts, and resources designed to demonstrate various approaches to question answering using SEC 10-Q filings. This includes baseline RAG implementations, knowledge graph construction, and document processing pipelines.

## Prerequisites
Before you dive into the materials, ensure you have the following prerequisites:
- Python 3.10 or higher
- Poetry for dependency management
- An OpenAI API key
- A Neo4j database instance (for knowledge graph notebooks)
- Tesseract and Poppler (for PDF processing)

## Notebooks
Here you will find the following Jupyter notebooks:
1.  **`baseline-rag-cot.ipynb`**: This notebook implements a Retrieval-Augmented Generation (RAG) system with chain of thought reasoning for answering questions about SEC 10-Q filings. It includes document loading, chunking, embedding generation, and querying functionalities, as well as evaluation code.
2.  **`baseline-rag.ipynb`**: This notebook implements a basic RAG system for answering questions about SEC 10-Q filings. It includes document loading, chunking, embedding generation, and querying functionalities, as well as evaluation code.
3.  **`kg-rag-cypher.ipynb`**: This notebook demonstrates constructing a knowledge graph from SEC 10-Q filings using LangChain and Neo4j. It extracts entities and relationships using an LLM, stores them in Neo4j, and then uses Cypher queries to answer questions. It also includes evaluation code.
4.  **`kg-rag-entity.ipynb`**: This notebook demonstrates constructing a knowledge graph from SEC 10-Q filings using LangChain and Networkx. It extracts entities and relationships using an LLM and stores them in a Networkx graph. It then uses a GraphQAChain to answer questions. It also includes evaluation code.

## Code
This section includes code files that demonstrate:
-   **`pdf_processor.py`**: This script provides a class `PDFProcessor` that extracts text from PDF files using either `pdftotext` or OCR with `tesseract`. It also includes a function `process_pdf_directory` to process all PDFs in a directory and save the extracted text to text files.
-   **`visualizer.py`**: This script provides a class `GraphVisualizer` that visualizes Neo4j graphs using NetworkX and Matplotlib. It includes methods to get graph statistics and visualize subgraphs.

## Resources
For further reading and additional studies, consider the following resources:
- LangChain documentation: [https://python.langchain.com/docs/](https://python.langchain.com/docs/)
- Neo4j documentation: [https://neo4j.com/docs/](https://neo4j.com/docs/)
- Tesseract documentation: [https://tesseract-ocr.github.io/tessdoc/](https://tesseract-ocr.github.io/tessdoc/)
- Poppler documentation: [https://poppler.freedesktop.org/](https://poppler.freedesktop.org/)

## Getting Started
To get started with the materials in this topic:
1. Ensure you have installed all the required dependencies using `poetry install`.
2. Set up your environment variables, including your OpenAI API key and Neo4j connection details.
3. Explore the notebooks and scripts to understand the different approaches to question answering with SEC 10-Q filings.
4. Run the notebooks and scripts to reproduce the results and experiment with different parameters.

================
File: reference_implementations/sec-10-q/visualizer.py
================
import networkx as nx
import matplotlib.pyplot as plt
from typing import Optional, List, Tuple
from neo4j import GraphDatabase

class GraphVisualizer:
    """Utility class for visualizing Neo4j graphs."""
    
    def __init__(self, uri: str, username: str, password: str, database: str = "neo4j"):
        """Initialize visualizer.
        
        Args:
            uri: Neo4j database URI
            username: Neo4j username
            password: Neo4j password
            database: Neo4j database name
        """
        self.driver = GraphDatabase.driver(uri, auth=(username, password))
        self.database = database
        
    def get_graph_statistics(self) -> Tuple[int, int, List[Tuple[str, int]]]:
        """Get basic statistics about the graph.
        
        Returns:
            Tuple of (node_count, relationship_count, label_counts)
        """
        with self.driver.session(database=self.database) as session:
            # Get total counts
            result = session.run("""
                MATCH (n)
                WITH count(n) as nodes
                MATCH ()-[r]->()
                RETURN nodes, count(r) as rels
            """)
            record = result.single()
            node_count = record["nodes"]
            rel_count = record["rels"]
            
            # Get label counts
            result = session.run("""
                MATCH (n)
                WITH labels(n) as labels
                UNWIND labels as label
                WITH label, count(*) as count
                RETURN label, count
                ORDER BY count DESC
            """)
            label_counts = [(record["label"], record["count"]) for record in result]
            
        return node_count, rel_count, label_counts

    def visualize_subgraph(self, 
                          limit: int = 100,
                          layout: str = "spring",
                          figsize: Tuple[int, int] = (12, 8),
                          node_size: int = 1000,
                          with_labels: bool = True):
        """Visualize a subgraph of the database.
        
        Args:
            limit: Maximum number of nodes to visualize
            layout: NetworkX layout algorithm to use
            figsize: Figure size (width, height)
            node_size: Size of nodes in visualization
            with_labels: Whether to show node labels
        """
        # Get subgraph data
        with self.driver.session(database=self.database) as session:
            result = session.run(f"""
                MATCH (n)-[r]->(m)
                WITH * LIMIT {limit}
                RETURN 
                    id(n) as source_id,
                    labels(n) as source_labels,
                    id(m) as target_id,
                    labels(m) as target_labels,
                    type(r) as relationship_type
            """)
            
            # Create NetworkX graph
            G = nx.DiGraph()
            
            for record in result:
                source = f"{record['source_id']}\n{record['source_labels'][0]}"
                target = f"{record['target_id']}\n{record['target_labels'][0]}"
                rel_type = record['relationship_type']
                
                G.add_edge(source, target, relationship=rel_type)
        
        # Visualize
        plt.figure(figsize=figsize)
        
        if layout == "spring":
            pos = nx.spring_layout(G)
        elif layout == "circular":
            pos = nx.circular_layout(G)
        else:
            pos = nx.random_layout(G)
            
        nx.draw(G, pos,
               with_labels=with_labels,
               node_color='lightblue',
               node_size=node_size,
               arrowsize=20,
               font_size=8)
        
        # Add relationship labels
        edge_labels = nx.get_edge_attributes(G, 'relationship')
        nx.draw_networkx_edge_labels(G, pos, edge_labels)
        
        plt.title(f"Neo4j Graph Visualization (showing {G.number_of_nodes()} nodes)")
        plt.axis('off')
        plt.show()

    def close(self):
        """Close database connection."""
        self.driver.close()

================
File: reference_implementations/synthea-fhir/README.md
================
# [Topic Name] Overview

## Introduction
Welcome to the [Topic Name] module of the [Bootcamp Name]. This folder contains a collection of [choices of Jupyter notebooks, code samples, and resources] designed to enhance your understanding of [specific aspect of the topic]. Here, we will explore [brief description of what will be covered].

## Prerequisites
Before you dive into the materials, ensure you have the following prerequisites:
- [Software/Tool/Library] version x.x or higher
- Basic knowledge of [relevant technology or field]
- [Any other requirement]

## Notebooks
Here you will find the following Jupyter notebooks:
1. **[Subtopic 1]** - This notebook covers [brief description].
2. **[Subtopic 2] Application** - This notebook covers [brief description].

## Code
This section includes code files that demonstrate:
- **[Filename 1].py**: [Description of what the script does]
- **[Filename 2].py**: [Description of what the script does]

## Resources
For further reading and additional studies, consider the following resources:
- [Online course, book, or tutorial 1] - [brief description] ([link])
- [Online course, book, or tutorial 2] - [brief description] ([link])

## Getting Started
To get started with the materials in this topic:
1. Ensure you have followed the [reference to the installation guide, environment creation, etc.] in `\docs`.
2. Complete any other steps that are required before moving to the notebooks or code.
3. Move to notebook [notebook name] to [a brief objective of the notebook].
4. Run code [code name] with the command [command] to [a brief objective of the code].
5. Add any extra steps if needed.

================
File: scripts/generate_synthetic_data.py
================
import pandas as pd
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
import os

def generate_synthetic_data(input_csv_path, output_csv_path):
    """
    Generates a synthetic dataset with numerical/objective answers based on the input CSV.

    Args:
        input_csv_path (str): Path to the input CSV file.
        output_csv_path (str): Path to the output CSV file.
    """
    df = pd.read_csv(input_csv_path)

    llm = ChatOpenAI(temperature=0, model="gpt-4o")

    prompt_template = """
    Given the following question and answer, generate a new question that requires a numerical or objective answer, and provide the numerical or objective answer.
    The new question should be at one of three levels:
    - level 1: single chunk.
    - level 2: multi-doc/multi-chunk
    - level 3: multi-step reasoning

    Here is a good example of the specificity we are looking for when describing the format of the answers:

    Level 1
    Question: What was the actual enrollment count of the clinical trial on H. pylori in acne vulgaris
    patients from Jan-May 2018 as listed on the NIH website?
    Ground truth: 90
    
    Level 2
    Question: If this whole pint is made up of ice cream, how many percent above
    or below the US federal standards for butterfat content is it when using the
    standards as reported by Wikipedia in 2020? Answer as + or - a number rounded
    to one decimal place.
    Ground truth: +4.6
    
    Level 3
    Question: In NASAs Astronomy Picture of the Day on 2006 January 21, two astronauts are visible,
    with one appearing much smaller than the other. As of August 2023, out of the astronauts in the
    NASA Astronaut Group that the smaller astronaut was a member of, which one spent the least time
    in space, and how many minutes did he spend in space, rounded to the nearest minute? Exclude any
    astronauts who did not spend any time in space. Give the last name of the astronaut, separated from
    the number of minutes by a semicolon. Use commas as thousands separators in the number of minutes.
    Ground truth: White; 5876

    Remember to specify the format of the answer in the new question. Make sure to specify units if necessary. The answer should only be a numerical value that can be easily verified and aligned with the units specified in the question.   
    
    Also specify the level as an integer in a separate column.

    Question: {question}
    Answer: {answer}

    New Question:
    New Answer:
    Level:
    """

    prompt = ChatPromptTemplate.from_template(prompt_template)

    chain = LLMChain(llm=llm, prompt=prompt)

    new_questions = []
    new_answers = []
    levels = []

    for index, row in df.iterrows():
        question = row["Question"]
        answer = row["Answer"]
        output = chain.run(question=question, answer=answer)
        try:
            new_question = output.split("New Question:")[1].split("New Answer:")[0].strip()
            new_answer = output.split("New Answer:")[1].split("Level:")[0].strip()
            level = output.split("Level:")[1].strip()
            new_questions.append(new_question)
            new_answers.append(new_answer)
            levels.append(level)
        except IndexError:
            new_questions.append("")
            new_answers.append("")
            levels.append("")

    df["New Question"] = new_questions
    df["New Answer"] = new_answers
    df["Level"] = levels
    df.to_csv(output_csv_path, index=False)
    print(f"Synthetic data generated and saved to {output_csv_path}")

if __name__ == "__main__":
    input_csv_path = "data/sec-10-q/qna_data.csv"
    output_csv_path = "data/sec-10-q/synthetic_qna_data.csv"
    generate_synthetic_data(input_csv_path, output_csv_path)

================
File: .gitignore
================
.venv/*
# *.pkl
reference_implementations/sec-10-q/chroma_db/*
.DS_Store
reference_implementations/sec-10-q/evaluation_results/*
reference_implementations/sec-10-q/evaluation_results_graphrag/*

================
File: .pre-commit-config.yaml
================
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v2.3.0
    hooks:
      - id: check-yaml
      - id: end-of-file-fixer
      - id: trailing-whitespace

  - repo: https://github.com/psf/black
    rev: 22.10.0
    hooks:
      - id: black

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.4.3
    hooks:
      - id: ruff
        types_or: [python, pyi, jupyter]
        args: [--fix]
      - id: ruff-format
        types_or: [python, pyi, jupyter]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.10.0
    hooks:
    - id: mypy
      entry: python3 -m mypy --config-file pyproject.toml
      language: system
      types: [python]
      exclude: "tests"

================
File: CONTRIBUTING.md
================
# Contributing to bootcamp_template

Thanks for your interest in contributing to the bootcamp_template!

To submit PRs, please fill out the PR template along with the PR. If the PR fixes an issue, don't forget to link the PR to the issue!

## Pre-commit hooks

Once the python virtual environment is setup, you can run pre-commit hooks using:

```bash
pre-commit run --all-files
```

## Coding guidelines

For docstrings we use [numpy format](https://numpydoc.readthedocs.io/en/latest/format.html).

For code style, we recommend the [PEP 8 style guide](https://peps.python.org/pep-0008/).
Pre-commit hooks apply [black](https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html) code formatting.

We use [ruff](https://docs.astral.sh/ruff/) for code formatting and static code
analysis. Ruff checks various rules including [flake8](https://docs.astral.sh/ruff/faq/#how-does-ruff-compare-to-flake8). The pre-commit hooks show errors which you need to fix before submitting a PR.

Last but not the least, we use type hints in our code which is then checked using
[mypy](https://mypy.readthedocs.io/en/stable/).

================
File: LICENSE.md
================
# License

## Overview
For an example of the license, please refer to the [LICENSE.md](https://github.com/VectorInstitute/aieng-template/blob/main/LICENSE.md) file in our aieng-template repository.

================
File: literature_review.md
================
# Literature Survey of RAG pipelines, datasets and evaluation.

## Papers

1. [Evaluation of Retrieval-Augmented Generation: A Survey](https://arxiv.org/pdf/2405.07437)
2. [MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation](https://arxiv.org/abs/2501.06713)

## Datasets

## Question-Answering Based
**Natural Questions (NQ)**
   - Size: 307,373 examples with 1-way annotations, 7,830 examples with 5-way annotations for dev data and 7,842 with 5-way annotations for test data
   - Ground-truth: Human-verified 
   - Tags: wikipedia, factual QA
   - Reference: [Kwiatkowski et al., 2019](https://doi.org/10.1162/tacl_a_00276)

**HotpotQA**
   - Size: 113,000 examples
   - Ground-truth: Human-verified
   - Tags: multi-hop reasoning, wikipedia
   - Reference: [Yang et al., 2018](https://arxiv.org/abs/1809.09600)

**FEVER**
   - Size: 185,445 examples
   - Ground-truth: Human-verified
   - Tags: fact verification, wikipedia
   - Reference: [Thorne et al., 2018](https://aclanthology.org/N18-1074/)

## Domain-Specific
**MIRAGE (MedRAG)**
   - Size: 7,663 questions across 5 datasets
   - Ground-truth: Human-verified
   - Tags: biomedical, healthcare, zero-shot evaluation, question-only retrieval
   - Reference: [Xiong et al., 2024](https://arxiv.org/abs/2402.13178)

**DomainRAG**
   - Size: 36,166 examples, focused on 6 specific domain capabilities:  
     - Extractive, Conversational, Structural, Time-sensitive, Multi-document, Faithfulness
   - Ground-truth: Machine-generated
   - Tags: college admissions, temporal data
   - Reference: [Wang et al., 2024](https://arxiv.org/abs/2406.05654)

## Dynamic/Time-Sensitive
**RealTimeQA**
   - Size: Continuously updated
   - Ground-truth: Human-verified
   - Tags: current events, temporal
   - Reference: [Kasai et al., 2022](https://arxiv.org/abs/2207.13332)

## Synthetic/Generated
**RGB Dataset**
    - Size: 300 base questions, 200 for information integration and 200 for counterfactual robustness.
   - Source: News articles, tests four fundamental capabilities:
     - noise robustness, negative rejection, information integration, and counterfactual robust-ness.
   - Ground-truth: Machine-generated
   - Tags: news, robustness testing
   - Reference: [Chen et al., 2023](https://arxiv.org/abs/2309.01431)

**CRUD-RAG**
   - Size: Total 36,166 examples, 10,728 text continuation, 9,580 question answering, 5,130 hallucination modification, 10,728 multi-doc summarization
   - Source: News articles
   - Knowledge base: 86,834 documents
   - Ground-truth: Machine-generated
   - Tags: create, read, update, delete operations
   - Reference: [Lyu et al., 2024](https://arxiv.org/abs/2401.17043)

**MultiHop-RAG**
   - Size: 2,556 queries, 31.92% inference queries, 33.49% comparison queries, 22.81% temporal queries, 11.78% null queries
   - Knowledge base: 609 news articles
   - Source: News articles
   - Ground-truth: Machine-generated
   - Tags: multi-hop reasoning
   - Reference: [Tang & Yang, 2024](https://arxiv.org/abs/2401.15391)

================
File: pyproject.toml
================
[tool.poetry]
name = "bootcamp-template"
version = "0.1.0"
description = ""
authors = ["Your Name <you@example.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = ">=3.10,<3.13"
pre-commit = "^3.7.0"
chromadb = "^0.5.23"
openai = "^1.58.1"
pandas = "^2.2.3"
pytesseract = "^0.3.13"
pdf2image = "^1.17.0"
tiktoken = "^0.8.0"
neo4j = "^5.27.0"
langchain = "^0.3.13"
langchain-neo4j = "^0.2.0"
langchain-openai = "^0.2.13"
langgraph = "^0.2.60"
networkx = "^3.4.2"
matplotlib = "^3.10.0"
langchain-experimental = "^0.3.3"
pypdf = "^5.1.0"
ipywidgets = "^8.1.5"
python-dotenv = "^1.0.1"
langchain-google-genai = "^2.0.9"
google-generativeai = "^0.8.4"
langchain-chroma = "^0.2.1"
scipy = "1.12.0"
langchain-graphrag = "^0.0.9"
ragas = "^0.2.12"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

================
File: README.md
================
# Exploring the use of Knowledge Graphs to enhance the performance of RAG systems


## Goals

================
File: requirements.txt
================
pandas
scikit-learn
ollama
tqdm
codecarbon==2.6

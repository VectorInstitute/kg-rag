<!doctype html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!./style.css") %></style>
</head>
<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Enhancing RAG with Knowledge Graphs: A Comparative Analysis",
  "description": "A deep dive into knowledge graph-based approaches for improving retrieval augmented generation",
  "authors": [
    {
      "author": "Ali Kore",
      "authorURL": "https://github.com/a-kore",
      "affiliation": "Vector Institute",
      "affiliationURL": "https://vectorinstitute.ai"
    },
    {
      "author": "Amrit Krishnan",
      "authorURL": "https://github.com/amrit110",
      "affiliation": "Vector Institute",
      "affiliationURL": "https://vectorinstitute.ai"
    }
  ],
  "katex": {
    "delimiters": [
      {"left": "$", "right": "$", "display": false},
      {"left": "$$", "right": "$$", "display": true}
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>Enhancing RAG with Knowledge Graphs</h1>
  <p>
    Leveraging structured knowledge to improve accuracy and relevance in retrieval augmented generation systems
  </p>
</d-title>

<d-byline></d-byline>

<div id="contents" class="base-grid" style="border-top: 1px solid #eee; border-radius: 6px; padding: 1.8rem; background-color: #f8f9fa; margin: 1.5rem 0;">
    <nav class="l-text toc figcaption">
        <h3 style="margin-top: 0; margin-bottom: 1rem; color: #333; font-size: 1.2rem;">Contents</h3>
        <details open>
            <summary>Introduction</summary>
            <a href="#introduction">Navigate to Introduction section</a>
        </details>
        <details open>
            <summary>Baseline RAG Architecture</summary>
            <a href="#baseline-rag">Navigate to Baseline RAG Architecture section</a>
            <ul>
                <li><a href="#baseline-components">Key Components</a></li>
                <li><a href="#baseline-limitations">Limitations</a></li>
            </ul>
        </details>
        <details open>
            <summary>Knowledge Graph-Based RAG</summary>
            <a href="#kg-rag">Navigate to Knowledge Graph-Based RAG section</a>
            <ul>
                <li><a href="#entity-based-approach">Entity-Based Approach</a></li>
                <li><a href="#interactive-visualization">Interactive Visualization</a></li>
                <li><a href="#kg-rag-advantages">Advantages</a></li>
            </ul>
        </details>
        <details open>
            <summary>SEC 10-Q Dataset & Evaluation</summary>
            <a href="#dataset-evaluation">Navigate to Dataset & Evaluation section</a>
            <ul>
                <li><a href="#dataset-overview">Dataset Overview</a></li>
                <li><a href="#evaluation-methodology">Evaluation Methodology</a></li>
                <li><a href="#performance-results">Performance Results</a></li>
            </ul>
        </details>
        <details open>
            <summary>Other KG-RAG Approaches</summary>
            <a href="#other-approaches">Navigate to Other Approaches section</a>
            <ul>
                <li><a href="#cypher-based">Cypher-Based KG-RAG</a></li>
                <li><a href="#graphrag">GraphRAG</a></li>
            </ul>
        </details>
        <div><a href="#conclusion">Conclusion and Future Directions</a></div>
    </nav>
</div>


<d-article>

  <h2 id="introduction">Introduction</h2>

  <p>Retrieval Augmented Generation (RAG) systems have revolutionized how large language models (LLMs) access and utilize external knowledge. By retrieving relevant information from a knowledge base before generating responses, RAG enables LLMs to provide more accurate, up-to-date, and verifiable answers. However, traditional RAG systems face significant challenges when dealing with complex information structures.</p>

  <h3>The Challenges of Traditional RAG</h3>

  <p>Standard RAG implementations rely primarily on vector similarity search, treating documents as collections of independent chunks with limited contextual relationships. While effective for simple question answering, this approach struggles with:</p>

  <ol>
    <li><strong>Connecting Related Information:</strong> When relevant information is spread across multiple documents or sections</li>
    <li><strong>Understanding Complex Relationships:</strong> Between entities mentioned in different contexts</li>
    <li><strong>Multi-hop Reasoning:</strong> Questions that require synthesizing facts from multiple sources</li>
    <li><strong>Preserving Structural Information:</strong> Important relationships that exist in the original documents</li>
  </ol>

  <p>For example, when answering questions about financial data, a traditional RAG system might retrieve document chunks containing relevant numbers but may miss crucial context about which fiscal periods, products, or business segments they relate to.  This is particularly relevant for standardized financial documentation with highly correlated chunks.</p>

  <h3>Knowledge Graphs: A Structural Solution</h3>

  <p>Knowledge graphs provide a natural solution to these challenges by explicitly modeling entities and their relationships. By representing documents as interconnected nodes and edges rather than isolated chunks, knowledge graph-based RAG systems can:</p>

  <ol>
    <li><strong>Capture Meaningful Relationships:</strong> Between entities mentioned across documents</li>
    <li><strong>Enable Traversal-Based Retrieval:</strong> Following connection paths between related concepts</li>
    <li><strong>Combine Structural and Semantic Information:</strong> Leveraging both relationships and textual content</li>
    <li><strong>Support Explainable Retrieval:</strong> Making it clear why certain information was selected</li>
  </ol>

  <p>This blog post explores how knowledge graphs can enhance RAG systems through a comparative analysis of traditional vector-based retrieval and our novel knowledge graph-based approaches. We focus particularly on quantitative question answering in the complex domain of financial documents, where relationships between entities and figures are crucial for accurate responses.</p>

  <h2 id="baseline-rag">Baseline RAG Architecture</h2>

  <p>The standard RAG approach relies on vector similarity to retrieve relevant context. The process follows a straightforward pipeline as illustrated below:</p>

  <figure>
    <img src="assets/baseline.png" alt="Baseline RAG Architecture" style="width: 70%; max-width: 600px; margin: 0 auto 1rem auto; display: block; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 4px;">
    <figcaption>Figure 1: Standard RAG architecture using vector similarity for retrieval</figcaption>
  </figure>

  <p>The workflow consists of five main stages:</p>

  <ol>
    <li><strong>Query Embedding:</strong> Convert the user query into a vector embedding</li>
    <li><strong>Chunk Similarity Matching:</strong> Find document chunks with embeddings similar to the query</li>
    <li><strong>Chunk Selection:</strong> Select the top-k most similar chunks</li>
    <li><strong>Context Assembly:</strong> Combine the selected chunks into a unified context</li>
    <li><strong>LLM Generation:</strong> Generate an answer based on the query and assembled context</li>
  </ol>

  <h3 id="baseline-components">Key Components</h3>

  <p>The baseline RAG system consists of three main components:</p>

  <ul>
    <li><strong>Document Processor:</strong> Handles document loading, chunking, and metadata extraction</li>
    <li><strong>Embedding Handler:</strong> Manages vector embeddings creation and similarity calculations</li>
    <li><strong>Vector Store:</strong> Stores and retrieves document chunks and their embeddings</li>
  </ul>

  <p>This approach works well for many question-answering tasks but has certain limitations when dealing with complex, relationship-heavy domains.</p>

  <h3 id="baseline-limitations">Limitations</h3>

  <p>While effective for many use cases, traditional RAG systems face several challenges:</p>

  <ul>
    <li><strong>Semantic Gap:</strong> Vector similarity may not capture complex semantic relationships between entities</li>
    <li><strong>Lack of Structure:</strong> The flat representation misses hierarchical and relational information</li>
    <li><strong>Context Fragmentation:</strong> Related information may be spread across different chunks that don't individually match the query well</li>
    <li><strong>Multi-hop Reasoning:</strong> Difficulty in answering questions that require connecting information across multiple documents</li>
  </ul>

  <p>These limitations become particularly apparent when dealing with domains that have rich relational structures, such as financial data, scientific literature, or complex technical documentation.</p>

  <h2 id="kg-rag">Knowledge Graph-Based RAG</h2>

  <p>To address the limitations of vector-based RAG, we introduce knowledge graph-based approaches that incorporate structured relationships into the retrieval process. These methods build and leverage a knowledge graph representing entities and relationships extracted from the document collection.</p>

  <h3 id="entity-based-approach">Entity-Based Approach</h3>

  <p>The Entity-Based KG-RAG approach enhances the standard RAG pipeline by incorporating graph-based retrieval. The process follows these steps:</p>

  <figure>
    <img src="assets/entity.png" alt="Entity-Based KG-RAG Architecture" style="width: 70%; max-width: 600px; margin: 0 auto 1rem auto; display: block; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 4px;">
    <figcaption>Figure 2: Entity-Based KG-RAG architecture leveraging knowledge graph relationships</figcaption>
  </figure>

  <p>The workflow consists of six main stages:</p>

  <ol>
    <li><strong>Query Embedding:</strong> Convert the user query to a vector embedding</li>
    <li><strong>Entity Similarity Matching:</strong> Find entities in the knowledge graph most similar to the query</li>
    <li><strong>Subgraph Exploration:</strong> Explore the local neighborhood around similar entities to provide valuable context to document chunks</li>
    <li><strong>Chunk Selection:</strong> Select document chunks based on both entity presence and relevance</li>
    <li><strong>Context Assembly:</strong> Combine knowledge graph paths and document chunks into a rich context</li>
    <li><strong>LLM Generation:</strong> Generate a comprehensive answer using both structural and textual information</li>
  </ol>

  <p>This approach leverages both semantic similarity (through embeddings) and structural relationships (through the knowledge graph) to provide more accurate and comprehensive answers.</p>

  <h3 id="interactive-visualization">Interactive Visualization</h3>

  <p>To better understand how the Entity-Based KG-RAG method works in practice, let's look at an interactive visualization of the process for a query where the baseline system answers incorrectly but the entity-based system gets right:</p>

  <figure>
    <iframe src="assets/diagram.html" width="100%" height="896px" frameborder="0" style="max-width: 100%; margin: 0 auto 1rem auto; display: block; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 6px;"></iframe>
    <figcaption>Figure 3: Interactive visualization of the Entity-Based KG-RAG approach showing query processing flow for a question about Apple's gross margin percentage. The visualization demonstrates how entities are identified, the subgraph is explored, and relevant document chunks are selected to provide comprehensive answers.</figcaption>
  </figure>

  <p>The visualization above shows how a query about Apple's gross margin percentage flows through the Entity-Based KG-RAG system:</p>

  <ol>
    <li>The system first identifies relevant entities in the knowledge graph based on similarity to the query</li>
    <li>It then explores the subgraph around these entities to discover related information</li>
    <li>Based on the explored subgraph, it selects the most relevant document chunks</li>
    <li>Finally, it assembles a comprehensive context that combines both structural knowledge and textual information</li>
  </ol>

  <h3 id="kg-rag-advantages">Advantages</h3>

  <p>The Entity-Based KG-RAG approach offers several advantages over traditional RAG systems:</p>

  <ol>
    <li><strong>Relational Context Preservation:</strong> Knowledge graphs explicitly maintain the relationships between entities, preserving crucial contextual information that might be lost in vector-based approaches.</li>
    <li><strong>Multi-hop Reasoning Support:</strong> By exploring the neighborhood around high-similarity entities, the system can discover relevant contextual information in the knowledge graph.</li>
    <li><strong>Entity-Grounded Context Selection:</strong> The scoring of document chunks incorporates both entity presence and relevance, ensuring that the retrieved context is firmly grounded in entities relevant to the query.</li>
    <li><strong>Structural Patterns in Financial Data:</strong> Financial documents follow predictable structures, with information organized around key entities like companies, time periods, and financial metrics. Knowledge graphs naturally capture these patterns, making them particularly effective for this domain.</li>
    <li><strong>Explanation and Transparency:</strong> The paths in the knowledge graph provide a clear explanation of how different pieces of information are related, enhancing the transparency of the retrieval process.</li>
  </ol>

  <h2 id="dataset-evaluation">SEC 10-Q Dataset & Evaluation</h2>

  <h3 id="dataset-overview">Dataset Overview</h3>

  <p>To evaluate the performance of different RAG approaches, we leverage a <a href="https://github.com/docugami/KG-RAG-datasets/tree/main" target="_blank">specialized dataset from Docugami</a> based on SEC 10-Q quarterly financial reports from major technology companies. The dataset includes:</p>

  <ul>
    <li>Financial reports from Apple, Amazon, Intel, Microsoft, and NVIDIA</li>
    <li>Multiple quarters per company (2022-2023)</li>
    <li>PDF files with extractable text content</li>
    <li>Structured financial data including revenue, profit margins, and other metrics</li>
  </ul>

  <!-- add sample screenshot of sec-10-q document from apple 2023 q3 report  -->
  <figure>
    <img src="assets/sec10q_sample.png" alt="Sample SEC 10-Q Document" style="width: 80%; max-width: 700px; margin: 0 auto 1rem auto; display: block; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 4px;">
    <figcaption>Figure 4: Sample SEC 10-Q document from Apple's Q3 2023 report. The document contains structured financial data and textual information.</figcaption>
  </figure>


  <p>This dataset was chosen because financial documents represent an ideal use case for knowledge graph approaches - they contain numerous entities with complex relationships between them, and answering questions often requires connecting information across different sections.</p>

  <h3 id="evaluation-methodology">Evaluation Methodology</h3>

  <p>While the original dataset included human-reviewed LLM-generated question-answer pairs, these tended to be qualitative in nature, making precise evaluation challenging. To address this limitation, we developed a set of 100 synthetic question-answer pairs with the following characteristics:</p>

  <ul>
    <li>Derived from original Q&A pairs but focused on quantitative answers</li>
    <li>Designed to have objective numerical answers that can be precisely evaluated</li>
    <li>Questions require understanding relationships between entities (e.g., companies, time periods, financial metrics)</li>
  </ul>

  <p>For example, a qualitative question like:
    <br><br>
    <em>"Can any trends be identified in Apple's Services segment revenue over the reported periods?"</em>
    <br><br>
    was transformed into a quantitative question such as:
    <br><br>
    <em>"What was the increase in Apple's Services segment net sales from the quarter ended June 25, 2022, to the quarter ended July 1, 2023, as reported in their 2022 Q3 and 2023 Q3 10-Q filings? Provide the answer in millions of dollars as a whole number without commas."</em>
  </p>

  <p>We evaluated each RAG system using the following methodology:</p>

  <ol>
    <li><strong>Accuracy:</strong> An answer is considered correct only if the numerical value exactly matches the ground truth.</li>
    <li><strong>Controlled Environment:</strong> All systems used the same LLMs (GPT-4o/GPT-4o-mini) for generation, ensuring that performance differences were attributable to the retrieval components.</li>
    <li><strong>Hyperparameter Consistency:</strong> Where applicable, we used consistent hyperparameters (e.g., top-k = 5 chunks) across systems for fair comparison.</li>
    <li><strong>Error Analysis:</strong> Beyond simple accuracy, we analyzed the confusion matrix between systems to understand where and why different approaches succeeded or failed.</li>
  </ol>

  <h3 id="performance-results">Performance Results</h3>

  <p>Our evaluation revealed significant performance differences between the baseline RAG and Entity-Based KG-RAG approaches. The following visualization shows the overall accuracy comparison:</p>

  <figure>
    <img src="assets/accuracy.png" alt="Performance Comparison" style="width: 80%; max-width: 700px; margin: 0 auto 1rem auto; display: block; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 4px;">
    <figcaption>Figure 4: Performance comparison between Entity-based KG-RAG and Baseline RAG across different LLM models. Entity-based KG-RAG consistently outperforms the baseline approach.</figcaption>
  </figure>

  <p>
    The Entity-Based KG-RAG approach showed a substantial improvement over the baseline, with accuracy increasing from 40% to 55% when using GPT-4o, and from 36.36% to 56% when using GPT-4o-mini.
    This represents a relative improvement of approximately 37.5% and 54% respectively.
  </p>

  <p>
    Surprisingly, the performance of the entity-based approach was even more pronounced with the smaller GPT-4o-mini model, which typically performs worse than the larger GPT-4o model.
    This suggests that the structural knowledge provided by the knowledge graph compensates for the limitations of the smaller model, allowing it to leverage relationships more effectively than the baseline approach.
  </p>

  <p>Here, we conduct a detailed error analysis using a confusion matrix to understand the patterns of success and failure between the two approaches:</p>

  <figure>
    <img src="assets/confusion_matrix.png" alt="Confusion Matrix" style="width: 80%; max-width: 700px; margin: 0 auto 1rem auto; display: block; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 4px; ">
    <figcaption>Figure 5: Confusion matrix comparing Entity-based KG-RAG and Baseline RAG performance. The matrix shows that Entity-based KG-RAG correctly answers many questions that the Baseline approach misses, while rarely missing questions that the Baseline gets right.</figcaption>
  </figure>

  <p>The confusion matrix reveals that:</p>

  <ul>
    <li>Both systems correctly answered 38 questions (38% of the dataset)</li>
    <li>Entity-based KG-RAG correctly answered 17 questions that the Baseline RAG missed</li>
    <li>Baseline RAG correctly answered only 2 questions that Entity-based KG-RAG missed</li>
    <li>Both systems incorrectly answered 43 questions (43% of the dataset)</li>
  </ul>

  <p>This asymmetric pattern suggests that the Entity-based KG-RAG approach maintains most of the strengths of the baseline approach while addressing many of its weaknesses through improved structural understanding.</p>

  <p>We also investigated how the performance of the Entity-based KG-RAG approach varies with different configuration parameters, particularly the number of top nodes considered in the similarity matching stage:</p>

  <figure>
    <img src="assets/entity_topn.png" alt="Entity-based KG-RAG Performance by Top-N Nodes" style="width: 80%; max-width: 700px; margin: 0 auto 1rem auto; display: block; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 4px;">
    <figcaption>Figure 6: Entity-based KG-RAG performance with varying numbers of top similarity nodes considered. The peak performance occurs around 30-40 nodes, with diminishing returns when considering too many or too few nodes.</figcaption>
  </figure>

  <p>This analysis reveals that performance peaks when considering between 30-40 top similar nodes (56% accuracy), with a noticeable decline when considering either too few (&lt; 10 nodes) or too many (&gt; 50 nodes) similar entities. This suggests an optimal balance where the system has enough similar entities to explore related connections, but not so many that it introduces noise or dilutes the relevance of the retrieved context.</p>

  <h2 id="other-approaches">Other Knowledge Graph RAG Approaches</h2>

  <p>While this article has focused on the entity-based KG-RAG approach, we also implemented several other knowledge graph-based methods that show promise for different use cases.</p>

  <h3 id="cypher-based">Cypher-Based KG-RAG</h3>

  <p>The Cypher-based KG-RAG method leverages a Neo4j graph database and uses structured query language (Cypher) instead of vector embeddings as the primary retrieval mechanism:</p>

  <ul>
    <li><strong>Cypher Query Generation:</strong> A specialized LLM prompt template helps generate valid Cypher queries based on natural language questions</li>
    <li><strong>Schema-Aware Design:</strong> The system maintains awareness of the underlying graph schema to ensure generated queries use the correct entity types and relationships</li>
    <li><strong>Declarative Retrieval:</strong> Rather than exploring a subgraph based on similarity, this approach directly queries for specific patterns of relationships</li>
    <li><strong>Error Handling:</strong> Includes mechanisms to detect and correct malformed queries through an iterative process</li>
  </ul>

  <p>This approach excels when questions map clearly to specific relationship patterns in the knowledge graph, but requires more specialized knowledge of the underlying graph structure.  We use the <a href="https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/"" target="_blank">Langchain neo4j cookbooks</a> as a reference for our implementation.</p>

  <h3 id="graphrag">GraphRAG</h3>

  <p>GraphRAG combines knowledge graphs with embedding-based retrieval and community detection algorithms:</p>

  <ul>
    <li><strong>Document-to-Graph Transformation:</strong> Transforms documents into graph structures with nodes, edges, and community clusters</li>
    <li><strong>Hybrid Search Strategies:</strong> Implements both local (node-centered) and global (community-based) search strategies</li>
    <li><strong>Community Detection:</strong> Uses graph algorithms to identify clusters of related information</li>
    <li><strong>LangChain Integration:</strong> Built on the LangChain framework for seamless integration with other components</li>
  </ul>

  <p>This approach is particularly effective for documents with natural community structures, such as research papers with distinct sections or reports covering various business segments.  We leverage a <a href="https://github.com/ksachdeva/langchain-graphrag/tree/main" target="_blank">lanchain implementation of GraphRAG</a> for ease-of-use.</p>

  <h2 id="conclusion">Conclusion and Future Directions</h2>

  <p>Knowledge graph-based RAG approaches represent a significant advancement over traditional vector-based methods, especially for domains with complex relational structures. By incorporating structured relationships into the retrieval process, these methods can provide more accurate, comprehensive, and explainable answers.</p>

  <p>Our experiments with the Entity-Based KG-RAG method show promising results, particularly for questions that require understanding relationships between multiple entities and documents. The ability to explore subgraphs and combine structural knowledge with textual information enables more nuanced and accurate responses.</p>

  <p>The comparative analysis clearly demonstrates that incorporating structural knowledge through knowledge graphs significantly improves the ability of RAG systems to handle complex information needs, particularly in domains with rich relational structures like financial documentation.</p>

  <p>Future directions for this research include:</p>

  <ul>
    <li><strong>Improved graph construction techniques:</strong> Developing better methods for automatically extracting entities and relationships from documents</li>
    <li><strong>Dynamic graph updates:</strong> Creating systems that can continuously update the knowledge graph as new information becomes available</li>
    <li><strong>Reasoning-enhanced retrieval:</strong> Incorporating logical reasoning capabilities into the graph exploration process</li>
    <li><strong>Hybrid approaches:</strong> Integrating the strengths of different KG-RAG methods for optimal performance across diverse question types</li>
  </ul>

</d-article>

<d-appendix>
  <h3 id="acknowledgements">Acknowledgments</h3>
  <p>
    We would like to thank the Vector Institute for supporting this research, and the open-source community for providing valuable tools and frameworks that made this work possible.
  </p>

  <d-footnote-list></d-footnote-list>
  <d-bibliography src="references.bib"></d-bibliography>
  <d-citation-list></d-citation-list>
</d-appendix>


</body>

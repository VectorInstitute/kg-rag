{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abd70a7",
   "metadata": {},
   "source": [
    "\n",
    "# Optimizing LLMs Notebook\n",
    "\n",
    "This notebook dynamically initializes and evaluates LLaMA 3.21B and other large language models (LLMs).\n",
    "The performance and emissions of the models are tracked using the CodeCarbon library.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01dba2f-4bb3-4130-9423-ddc7ab19f663",
   "metadata": {},
   "source": [
    "### Instructions to Access a Model from Hugging Face, Set Up a Login, and Use It in Your Notebook\n",
    "\n",
    "#### 1. Access a Model on Hugging Face\n",
    "- Visit the [Hugging Face Model Hub](https://huggingface.co/models). \n",
    "- Search for your desired model, e.g., `meta-llama/Llama-3.2-1B-Instruct`,or SmolLM2-360M-Instruct for faster loading .\n",
    "  [https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)\n",
    "  [https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct] (https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct)\n",
    "- Check if the model requires special permissions:\n",
    "  - If restricted, look for a **\"Request Access\"** or **\"Fill Form\"** button.\n",
    "  - Follow the instructions to request access.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Filling Out Forms for Restricted Models\n",
    "- If the model is restricted:\n",
    "  1. Click the **\"Request Access\"** button.\n",
    "  2. Fill out the form with:\n",
    "     - Your **email address**.\n",
    "     - A brief **reason for accessing the model** (e.g., research or benchmarking).\n",
    "     - Your **organization** or **affiliation** (if applicable).\n",
    "  3. Submit the form and wait for approval (typically within a few hours to days).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Login to Hugging Face from Your Notebook\n",
    "1. Install the required libraries:\n",
    "   ```bash\n",
    "   pip install  huggingface_hub\n",
    "\n",
    "#### 4 Obtain your access token:\n",
    "\n",
    "Go to your [Hugging Face Settings](https://huggingface.co/settings/tokens). \n",
    "Create \"New Token\" and generate the token.\n",
    "Copy the token and use it in the login function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f1510f-a373-4ec5-b965-39dcab0ed056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate using your Hugging Face token\n",
    "#uncomment it below line and enter your token\n",
    "# login(token=\"your-access-token\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28677256-3aca-4f67-bc84-b367570544be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CPU Information ===\n",
      "Processor: x86_64\n",
      "CPU Count: 32\n",
      "Logical CPUs: 32\n",
      "Physical CPUs: 16\n",
      "CPU Frequency: 2401.5331875000006 MHz\n",
      "\n",
      "=== RAM Information ===\n",
      "Total RAM: 188.59 GB\n",
      "Available RAM: 179.44 GB\n",
      "\n",
      "=== GPU Information ===\n",
      "GPU Count: 1\n",
      "GPU 0: NVIDIA A40\n",
      "Memory Allocated: 0.00 MB\n",
      "Memory Cached: 0.00 MB\n",
      "\n",
      "=== System Information ===\n",
      "System: Linux\n",
      "Machine: x86_64\n",
      "Node: gpu001\n",
      "Version: #147~18.04.1-Ubuntu SMP Sat Oct 15 13:10:18 UTC 2022\n"
     ]
    }
   ],
   "source": [
    "#check hardware\n",
    "\n",
    "#checking hardware related information\n",
    "\n",
    "import os\n",
    "import platform\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "def check_hardware():\n",
    "    print(\"=== CPU Information ===\")\n",
    "    print(f\"Processor: {platform.processor()}\")\n",
    "    print(f\"CPU Count: {os.cpu_count()}\")\n",
    "    if psutil:\n",
    "        print(f\"Logical CPUs: {psutil.cpu_count(logical=True)}\")\n",
    "        print(f\"Physical CPUs: {psutil.cpu_count(logical=False)}\")\n",
    "        print(f\"CPU Frequency: {psutil.cpu_freq().current} MHz\")\n",
    "    \n",
    "    print(\"\\n=== RAM Information ===\")\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {ram.total / 1024**3:.2f} GB\")\n",
    "    print(f\"Available RAM: {ram.available / 1024**3:.2f} GB\")\n",
    "    \n",
    "    print(\"\\n=== GPU Information ===\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n",
    "            print(f\"Memory Cached: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "    \n",
    "    print(\"\\n=== System Information ===\")\n",
    "    print(f\"System: {platform.system()}\")\n",
    "    print(f\"Machine: {platform.machine()}\")\n",
    "    print(f\"Node: {platform.node()}\")\n",
    "    print(f\"Version: {platform.version()}\")\n",
    "\n",
    "check_hardware()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d491978-4456-435b-b513-878e81dd2622",
   "metadata": {},
   "source": [
    "## Measuring Performance and Emissions of Language Models\n",
    "This script evaluates the performance and carbon emissions of language models using the Hugging Face transformers library and the codecarbon package. It provides a measure_performance function to compute runtime and emissions for a given pipeline and input prompt.\n",
    "\n",
    "**Hugging Face Transformers: Load and evaluate causal language models.**\n",
    "\n",
    "**CodeCarbon: Monitor and log CO2 emissions.**\n",
    "\n",
    "**BitsAndBytes: Optimize model execution with low-precision quantization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48756c06-4ac1-4ad9-835f-b4b5559b12ac",
   "metadata": {},
   "source": [
    "### Standard text generation pipeline\n",
    "\n",
    "**Initialize a basic text generation pipeline without any optimization, using a pre-trained causal language model and tokenizer from Hugging Face.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb2450-08db-4c39-82f4-2dc87d22f702",
   "metadata": {},
   "source": [
    "### **Optimizations**\n",
    "\n",
    "This pipeline initialization uses multiple techniques for efficient memory usage, faster inference, and seamless operation on limited hardware:\n",
    "\n",
    "1. **8-bit Quantization (`load_in_8bit=True`)**  \n",
    "   - Compresses weights to 8 bits to reduce memory usage.  \n",
    "   - Works with `bitsandbytes`.\n",
    "\n",
    "2. **Mixed Precision (`torch_dtype=torch.bfloat16`)**  \n",
    "   - Uses `bfloat16` for faster computation and reduced memory.  \n",
    "   - Requires GPUs with mixed precision support (e.g., NVIDIA Ampere).\n",
    "\n",
    "3. **Gradient Checkpointing (`model.gradient_checkpointing_enable()`)**  \n",
    "   - Saves memory by recomputing activations during the backward pass.  \n",
    "   - Useful for large model training or memory-efficient inference.\n",
    "\n",
    "4. **Device Mapping (`device_map=\"auto\"`)**  \n",
    "   - Automatically distributes model layers across available devices.  \n",
    "\n",
    "5. **Tokenization and Preprocessing**  \n",
    "   - Efficient tokenizer loading to handle text inputs seamlessly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee1ce00-b91f-4282-9258-97b7efb8d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29ca5d-5ce2-4a1d-b2cb-8b184075ea19",
   "metadata": {},
   "source": [
    "### `load_model` Function\n",
    "\n",
    "The `load_model` function is responsible for loading a language model with optional optimizations for enhanced performance or reduced memory usage. This function dynamically adjusts its behavior based on the specified optimization parameters.\n",
    "\n",
    "#### Function Parameters\n",
    "- `model_name` (str): The Hugging Face model ID for the language model to be loaded.\n",
    "- `optimized` (bool, default: `False`): If `True`, applies optimizations such as quantization (8-bit or INT4) or mixed precision.\n",
    "- `bf16` (bool, default: `False`): If `True`, uses bfloat16 (BF16) mixed precision for computation.\n",
    "- `int4` (bool, default: `False`): If `True`, uses INT4 quantization for efficient memory usage and computation.\n",
    "\n",
    "#### Optimization Modes\n",
    "1. **Standard Loading**:\n",
    "   - If no optimizations are specified, the model is loaded with standard precision (`float32`) and mapped to the available devices (`device_map=\"auto\"`).\n",
    "   - Example:\n",
    "     ```python\n",
    "     model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "     ```\n",
    "\n",
    "2. **8-Bit Quantization (QLoRA)**:\n",
    "   - If `optimized=True` and `bf16=False`, the model is loaded with 8-bit quantization using the `bitsandbytes` library.\n",
    "   - Example:\n",
    "     ```python\n",
    "     model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map=\"auto\")\n",
    "     ```\n",
    "\n",
    "3. **BF16 Precision**:\n",
    "   - If `optimized=True` and `bf16=True`, the model is loaded with bfloat16 precision for reduced memory usage and faster computation on supported GPUs.\n",
    "   - Example:\n",
    "     ```python\n",
    "     model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "     ```\n",
    "\n",
    "4. **INT4 Quantization**:\n",
    "   - If `int4=True`, the model is loaded with 4-bit quantization using `BitsAndBytesConfig` for memory-efficient operations.\n",
    "   - Parameters in `BitsAndBytesConfig`:\n",
    "     - `load_in_4bit=True`: Enables INT4 quantization.\n",
    "     - `bnb_4bit_use_double_quant=True`: Uses double quantization for higher precision.\n",
    "     - `bnb_4bit_quant_type=\"nf4\"`: NormalFloat4 quantization type for improved performance.\n",
    "     - `bnb_4bit_compute_dtype=torch.bfloat16`: Sets compute precision to bfloat16.\n",
    "   - Example:\n",
    "     ```python\n",
    "     bnb_config = BitsAndBytesConfig(\n",
    "         load_in_4bit=True,\n",
    "         bnb_4bit_use_double_quant=True,\n",
    "         bnb_4bit_quant_type=\"nf4\",\n",
    "         bnb_4bit_compute_dtype=torch.bfloat16\n",
    "     )\n",
    "     model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=bnb_config)\n",
    "     ```\n",
    "\n",
    "#### Tokenizer\n",
    "The tokenizer is loaded alongside the model to process input text into tensors compatible with the model. The `use_fast=False` parameter ensures compatibility with Llama-based models.\n",
    "\n",
    "#### Return Value\n",
    "The function returns a tuple:\n",
    "- `model`: The loaded and configured model.\n",
    "- `tokenizer`: The corresponding tokenizer for the model.\n",
    "\n",
    "#### Usage Example\n",
    "```python\n",
    "model, tokenizer = load_model(\"meta-llama/Llama-3.2-1B-Instruct\", optimized=True, bf16=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e49f917-5f47-4c41-8d18-f23ddfb1caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, optimized=False, bf16=False, int4=False):\n",
    "    \"\"\"\n",
    "    Load the specified language model with optional optimizations.\n",
    "    \"\"\"\n",
    "    if int4:\n",
    "        print(\"Loading model with INT4 optimization...\")\n",
    "        # BitsAndBytesConfig for int-4 optimization\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16  # You can adjust to torch.float16 if needed\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=bnb_config\n",
    "        )\n",
    "    elif optimized and bf16:\n",
    "        print(\"Loading model with BF16 optimization...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    elif optimized:\n",
    "        print(\"Loading model with 8-bit quantization (QLoRA)...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            load_in_8bit=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Loading standard model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2757d729-7560-4011-bd3f-a9ffa44ef411",
   "metadata": {},
   "source": [
    "# Function: `run_inference`\n",
    "\n",
    "## Description\n",
    "Generates text based on a given prompt using a pre-trained language model and tokenizer.\n",
    "\n",
    "## Parameters\n",
    "- **`model`**: Pre-trained language model (e.g., GPT, LLaMA).\n",
    "- **`tokenizer`**: Tokenizer associated with the model.\n",
    "- **`prompt`** (str): The input text for inference.\n",
    "- **`max_length`** (int, optional): Maximum length of the generated text. Default is `50`.\n",
    "\n",
    "## Returns\n",
    "- **str**: The generated text from the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd004dc5-2dda-476d-a215-e8e553481fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_inference(model, tokenizer, prompt, max_length=50):\n",
    "    \"\"\"\n",
    "    Run inference on the given prompt and return the generated text.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579d8e3-1dc2-4365-bd9c-6baefcda8ba3",
   "metadata": {},
   "source": [
    "# Function: `calculate_emissions`\n",
    "\n",
    "## Description\n",
    "Calculates the carbon footprint of running inference with a specific model and configuration.\n",
    "\n",
    "## Parameters\n",
    "- **`model_name`** (str): Name of the pre-trained model.\n",
    "- **`prompt`** (str): Input text for inference.\n",
    "- **`max_length`** (int, optional): Maximum length of the generated text. Default is `50`.\n",
    "- **`optimized`** (bool, optional): Indicates if the model is optimized (e.g., pruning, distillation). Default is `False`.\n",
    "- **`bf16`** (bool, optional): Enables bfloat16 precision for inference. Default is `False`.\n",
    "- **`int4`** (bool, optional): Enables 4-bit quantization. Default is `False`.\n",
    "\n",
    "## Returns\n",
    "- **float**: Carbon emissions in kilograms of CO2 equivalent (`kgCO2eq`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9ef5df-06eb-49f8-a001-ada7c2b481af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_emissions(model_name, prompt, max_length=50, optimized=False, bf16=False, int4=False):\n",
    "    \"\"\"\n",
    "    Calculate carbon emissions for running inference with a specific model.\n",
    "    \"\"\"\n",
    "    tracker = EmissionsTracker(project_name=f\"{model_name} {'INT4' if int4 else 'Optimized' if optimized else 'Standard'}\")\n",
    "    tracker.start()\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model(model_name, optimized=optimized, bf16=bf16, int4=int4)\n",
    "    \n",
    "    # Run inference\n",
    "    print(\"Running inference...\")\n",
    "    output = run_inference(model, tokenizer, prompt, max_length)\n",
    "    print(f\"Generated Text: {output}\")\n",
    "    \n",
    "    # Stop tracking and get emissions\n",
    "    emissions = tracker.stop()\n",
    "    print(f\"Carbon emissions (kgCO2eq): {emissions}\")\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0f9e9-bba8-4267-917b-c03ffade01b3",
   "metadata": {},
   "source": [
    "# Compare Standard and Optimized Models for Carbon Emissions\n",
    "\n",
    "## Description\n",
    "This script calculates and compares the carbon emissions (`kgCO2eq`) for running inference on a pre-trained model under different configurations: standard, QLoRA optimization, BF16 precision, and INT4 quantization.\n",
    "\n",
    "## Workflow\n",
    "1. **Model**: `meta-llama/Llama-2-7b-hf`\n",
    "2. **Prompt**: `\"Once upon a time, in a world filled with AI magic,\"`\n",
    "3. **Max Length**: `50` tokens\n",
    "\n",
    "## Configurations\n",
    "- **Standard Model**: Default setup with no optimizations.\n",
    "- **QLoRA**: Optimized model with quantized low-rank adapters.\n",
    "- **BF16**: Optimized model with bfloat16 precision.\n",
    "- **INT4**: Optimized model with 4-bit quantization.\n",
    "\n",
    "## Results\n",
    "The script prints the carbon emissions for each configuration.\n",
    "\n",
    "## Example Output\n",
    "```plaintext\n",
    "Comparison of Emissions:\n",
    "Standard Model: 0.123456 kgCO2eq\n",
    "Optimized Model with QLoRA: 0.098765 kgCO2eq\n",
    "Optimized Model with BF16: 0.087654 kgCO2eq\n",
    "Optimized Model with INT4: 0.056789 kgCO2eq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c328e8f-3acd-4acb-9a58-799ba07cd4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:33:47] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:33:47] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:33:47] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 10:33:47] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:33:47] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 10:33:48] CPU Model on constant consumption mode: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:33:48] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:33:48]   Platform system: Linux-5.4.0-131-generic-x86_64-with-glibc2.27\n",
      "[codecarbon INFO @ 10:33:48]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:33:48]   CodeCarbon version: 2.6.0\n",
      "[codecarbon INFO @ 10:33:48]   Available RAM : 1.000 GB\n",
      "[codecarbon INFO @ 10:33:48]   CPU count: 2\n",
      "[codecarbon INFO @ 10:33:48]   CPU model: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:33:48]   GPU count: 1\n",
      "[codecarbon INFO @ 10:33:48]   GPU model: 1 x NVIDIA A40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: /fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/codecarbon/data/hardware/cpu_power.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:33:51] Saving emissions data to file /fs01/projects/green-ai/Shaina/emissions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading standard model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:34:08] Energy consumed for RAM : 0.000002 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:34:15] Energy consumed for all GPUs : 0.000534 kWh. Total GPU Power : 83.81667327086336 W\n",
      "[codecarbon INFO @ 10:34:15] Energy consumed for all CPUs : 0.000282 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:34:15] 0.000818 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:34:22] Energy consumed for RAM : 0.000002 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:34:22] Energy consumed for all GPUs : 0.000724 kWh. Total GPU Power : 96.04053662668517 W\n",
      "[codecarbon INFO @ 10:34:22] Energy consumed for all CPUs : 0.000367 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:34:22] 0.001093 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:34:38] Energy consumed for RAM : 0.000004 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:34:38] Energy consumed for all GPUs : 0.001131 kWh. Total GPU Power : 96.00924542827806 W\n",
      "[codecarbon INFO @ 10:34:38] Energy consumed for all CPUs : 0.000547 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:34:38] 0.001682 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:34:53] Energy consumed for RAM : 0.000006 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:34:54] Energy consumed for all GPUs : 0.001567 kWh. Total GPU Power : 97.5715460783072 W\n",
      "[codecarbon INFO @ 10:34:54] Energy consumed for all CPUs : 0.000740 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:34:54] 0.002312 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:35:22] Energy consumed for RAM : 0.000008 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:35:22] Energy consumed for all GPUs : 0.002332 kWh. Total GPU Power : 97.64980255848266 W\n",
      "[codecarbon INFO @ 10:35:22] Energy consumed for all CPUs : 0.001074 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:35:22] 0.003414 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:35:37] Energy consumed for RAM : 0.000010 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:35:37] Energy consumed for all GPUs : 0.002742 kWh. Total GPU Power : 98.65310528855213 W\n",
      "[codecarbon INFO @ 10:35:37] Energy consumed for all CPUs : 0.001250 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:35:37] 0.004002 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:35:49] Energy consumed for RAM : 0.000011 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:35:49] Energy consumed for all GPUs : 0.003061 kWh. Total GPU Power : 97.91986939807556 W\n",
      "[codecarbon INFO @ 10:35:49] Energy consumed for all CPUs : 0.001389 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:35:49] 0.004461 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Once upon a time, in a world filled with AI magic, there was a small, mysterious shop called \"The Enchanted Emporium.\" The shop was said to be run by a wise and enigmatic wizard named Zorvath,\n",
      "ref: /fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n",
      "Carbon emissions (kgCO2eq): 0.00017620818071783338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:35:50] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:35:50] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:35:50] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 10:35:50] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:35:50] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 10:35:51] CPU Model on constant consumption mode: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:51] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:35:51]   Platform system: Linux-5.4.0-131-generic-x86_64-with-glibc2.27\n",
      "[codecarbon INFO @ 10:35:51]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:35:51]   CodeCarbon version: 2.6.0\n",
      "[codecarbon INFO @ 10:35:51]   Available RAM : 1.000 GB\n",
      "[codecarbon INFO @ 10:35:51]   CPU count: 2\n",
      "[codecarbon INFO @ 10:35:51]   CPU model: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:51]   GPU count: 1\n",
      "[codecarbon INFO @ 10:35:51]   GPU model: 1 x NVIDIA A40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: /fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/codecarbon/data/hardware/cpu_power.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:35:55] Saving emissions data to file /fs01/projects/green-ai/Shaina/emissions.csv\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with 8-bit quantization (QLoRA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:36:24] Energy consumed for RAM : 0.000003 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:36:24] Energy consumed for all GPUs : 0.000798 kWh. Total GPU Power : 96.92475782183483 W\n",
      "[codecarbon INFO @ 10:36:24] Energy consumed for all CPUs : 0.000350 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:36:24] 0.001152 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:36:36] Energy consumed for RAM : 0.000004 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:36:36] Energy consumed for all GPUs : 0.001109 kWh. Total GPU Power : 97.7918234161375 W\n",
      "[codecarbon INFO @ 10:36:36] Energy consumed for all CPUs : 0.000485 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:36:36] 0.001598 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:36:36] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:36:36] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:36:36] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 10:36:36] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:36:36] No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Once upon a time, in a world filled with AI magic, there was a powerful enchantress named Lyra. She had the ability to weave spells of wonder and magic, but she was also a bit of a perfectionist. She spent hours\n",
      "ref: /fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n",
      "Carbon emissions (kgCO2eq): 6.3123055137539e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:36:37] CPU Model on constant consumption mode: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:36:37] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:36:37]   Platform system: Linux-5.4.0-131-generic-x86_64-with-glibc2.27\n",
      "[codecarbon INFO @ 10:36:37]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:36:37]   CodeCarbon version: 2.6.0\n",
      "[codecarbon INFO @ 10:36:37]   Available RAM : 1.000 GB\n",
      "[codecarbon INFO @ 10:36:37]   CPU count: 2\n",
      "[codecarbon INFO @ 10:36:37]   CPU model: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:36:37]   GPU count: 1\n",
      "[codecarbon INFO @ 10:36:37]   GPU model: 1 x NVIDIA A40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: /fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/codecarbon/data/hardware/cpu_power.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:36:40] Saving emissions data to file /fs01/projects/green-ai/Shaina/emissions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with BF16 optimization...\n",
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:36:52] Energy consumed for RAM : 0.000001 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:36:52] Energy consumed for all GPUs : 0.000336 kWh. Total GPU Power : 98.58707014041666 W\n",
      "[codecarbon INFO @ 10:36:52] Energy consumed for all CPUs : 0.000145 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:36:52] 0.000482 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:36:53] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:36:53] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:36:53] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 10:36:53] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:36:53] No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Once upon a time, in a world filled with AI magic, there was a small, mysterious shop called \"The Enchanted Emporium.\" The shop was said to be run by a wise and powerful wizard named Zorvath,\n",
      "ref: /fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n",
      "Carbon emissions (kgCO2eq): 1.9039901167641488e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:36:54] CPU Model on constant consumption mode: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:36:54] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:36:54]   Platform system: Linux-5.4.0-131-generic-x86_64-with-glibc2.27\n",
      "[codecarbon INFO @ 10:36:54]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:36:54]   CodeCarbon version: 2.6.0\n",
      "[codecarbon INFO @ 10:36:54]   Available RAM : 1.000 GB\n",
      "[codecarbon INFO @ 10:36:54]   CPU count: 2\n",
      "[codecarbon INFO @ 10:36:54]   CPU model: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:36:54]   GPU count: 1\n",
      "[codecarbon INFO @ 10:36:54]   GPU model: 1 x NVIDIA A40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: /fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/codecarbon/data/hardware/cpu_power.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:36:57] Saving emissions data to file /fs01/projects/green-ai/Shaina/emissions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with INT4 optimization...\n",
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:37:11] Energy consumed for RAM : 0.000001 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:37:11] Energy consumed for all GPUs : 0.000381 kWh. Total GPU Power : 98.19856961906925 W\n",
      "[codecarbon INFO @ 10:37:11] Energy consumed for all CPUs : 0.000165 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:37:11] 0.000547 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Once upon a time, in a world filled with AI magic, there was a small, independent company called \"EchoPlex.\" EchoPlex was known for its innovative AI-powered tools and services, which were designed to help people achieve their\n",
      "ref: /fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n",
      "Carbon emissions (kgCO2eq): 2.1624181808329322e-05\n",
      "\n",
      "Comparison of Emissions:\n",
      "Standard Model: 0.000176 kgCO2eq\n",
      "Optimized Model with QLoRA: 0.000063 kgCO2eq\n",
      "Optimized Model with BF16: 0.000019 kgCO2eq\n",
      "Optimized Model with INT4: 0.000022 kgCO2eq\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Main function to compare standard and optimized models\n",
    "if __name__ == \"__main__\":\n",
    "    # MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # Updated to a valid model ID\n",
    "    MODEL_NAME = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "    PROMPT = \"Once upon a time, in a world filled with AI magic,\"\n",
    "    MAX_LENGTH = 50\n",
    "\n",
    "    # Calculate emissions for standard model\n",
    "    standard_emissions = calculate_emissions(MODEL_NAME, PROMPT, MAX_LENGTH, optimized=False)\n",
    "\n",
    "    # Calculate emissions for optimized model with QLoRA\n",
    "    qlora_emissions = calculate_emissions(MODEL_NAME, PROMPT, MAX_LENGTH, optimized=True, bf16=False)\n",
    "\n",
    "    # Calculate emissions for optimized model with BF16\n",
    "    bf16_emissions = calculate_emissions(MODEL_NAME, PROMPT, MAX_LENGTH, optimized=True, bf16=True)\n",
    "\n",
    "    # Calculate emissions for optimized model with INT4\n",
    "    int4_emissions = calculate_emissions(MODEL_NAME, PROMPT, MAX_LENGTH, int4=True)\n",
    "\n",
    "    print(\"\\nComparison of Emissions:\")\n",
    "    print(f\"Standard Model: {standard_emissions:.6f} kgCO2eq\")\n",
    "    print(f\"Optimized Model with QLoRA: {qlora_emissions:.6f} kgCO2eq\")\n",
    "    print(f\"Optimized Model with BF16: {bf16_emissions:.6f} kgCO2eq\")\n",
    "    print(f\"Optimized Model with INT4: {int4_emissions:.6f} kgCO2eq\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef722136-a5cc-4b51-bc25-fb540fed7780",
   "metadata": {},
   "source": [
    "## Prepared By\n",
    "\n",
    "- **Name**: **Shaina Raza, PhD** [shaina.raza@vectorinstitute.ai](mailto:shaina.raza@vectorinstitute.ai)\n",
    "- **Affiliation**: Vector Institute for Artificial Intelligence\n",
    "\n",
    "This notebook was prepared as part of a practical guide for efficient evaluation and optimization of large language models (LLMs), with an emphasis on reducing carbon emissions and computational costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce462f3-a82f-4299-90f4-bad35463fc38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "green-ai",
   "language": "python",
   "name": "green-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmqMToqgtCU0"
   },
   "source": [
    "# Introduction to Language Models\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Background\n",
    "2. Architecture\n",
    "3. Hugging Face Transformers Library\n",
    "4. Introduction to BERT\n",
    "5. Introduction to GPT-2\n",
    "6. Introduction to LLAMA\n",
    "7. Introduction to FairSense-AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCgIJFWN2y4W"
   },
   "source": [
    "## Background\n",
    "\n",
    "Language models are a type of artificial intelligence model that can understand, interpret, and generate human language. They are a crucial component in various Natural Language Processing (NLP) tasks such as translation, summarization, and question answering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zj1o0l5U29Zi"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "There are various architectures used to build language models, with some of the popular ones being:\n",
    "\n",
    "- **Recurrent Neural Networks (RNNs) & Long Short-Term Memory (LSTM) :** RNNs process sequences of data by passing the hidden state from one step in the sequence to the next. Whereas LSTMs, are a special kind of RNNs capable of learning long-term dependencies.\n",
    "\n",
    "- **Transformer:** Introduced in the paper \"Attention is All You Need,\" (Vaswani et al. in 2017) this architecture uses attention mechanisms to process input sequences in parallel rather than sequentially, which significantly improves efficiency.\n",
    "\n",
    "**What advantages do transformers have over RNNS/LSTMs**\n",
    "\n",
    "1. **Parallel Processing**: Transformers process all tokens in a sequence simultaneously, enabling faster training and inference, while RNNs and LSTMs process tokens sequentially which can be time-consuming.\n",
    "2. **Long-Term Dependencies**: With their attention mechanisms, transformers can handle long-term dependencies in sequences better than standard RNNs and often LSTMs as well, by directly attending over all positions in the input sequence in constant time.\n",
    "3. **Positional Embeddings**: Unlike RNNs and LSTMs, transformers incorporate positional embeddings in their input representations to retain the order information of sequences, which is crucial for many NLP tasks.\n",
    "4. **Global Context Awareness**: Transformers' attention mechanisms allow each position to focus on different parts of the input sequence, providing a more global context awareness compared to the local receptive fields of RNNs and LSTMs.\n",
    "5. **Easier to Train**: Transformers tend to be easier to train and require less tinkering with hyperparameters compared to RNNs and LSTMs, which often suffer from vanishing or exploding gradient problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iS80H5ECP6C"
   },
   "source": [
    "### Recurrent Neural Networks (RNNs):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7hIOTVTHB7C"
   },
   "source": [
    "\n",
    "1. **Input**:\n",
    "   - RNNs take in a sequence of data, one item at a time (e.g., a word in a sentence).\n",
    "   - x(t)​ is taken as the input to the network at time step t.\n",
    "\n",
    "2. **Hidden State**:\n",
    "   - RNNs have a \"memory\" (hidden state) that captures information about what has been seen so far.\n",
    "   - h(t)​ represents a hidden state at time t.\n",
    "\n",
    "3. **Recurrent Loop**:\n",
    "   - The same operations (a set of weights) are performed at each step of the sequence, looping the output back into the model at the next step.\n",
    "   - Input to hidden connections parameterized by a weight matrix U\n",
    "   - Hidden-to-hidden recurrent connections parameterized by a weight matrix W.\n",
    "   - Hidden-to-output connections parameterized by a weight matrix V\n",
    "\n",
    "4. **Output**:\n",
    "   - At each step, RNNs can produce an output based on the current input and memory.\n",
    "   - o(t)​ illustrates the output of the network.\n",
    "\n",
    "5. **Backpropagation**:\n",
    "   - During training, errors are back-propagated through the network to adjust the weights, improving the model's predictions.\n",
    "\n",
    "\n",
    "\n",
    "<div> <img src=https://miro.medium.com/v2/resize:fit:640/format:webp/1*JOkrQoJ3J3-451GzRcayRg.png width=\"400\"/> </div>\n",
    "\n",
    "*The left side of the diagram shows a notation of an RNN and on the right side is the full network representing an RNN.\n",
    "\n",
    "*Image Source: https://www.deeplearningbook.org/contents/rnn.html/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuofgKj0D6U0"
   },
   "source": [
    "### Long Short-Term Memory Networks (LSTMs):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ8yA6juHHtU"
   },
   "source": [
    "1. **Input, Forget, and Output Gates**:\n",
    "   - LSTMs have a more complex structure than simple RNNs, with three \"gates\" controlling the flow of information.\n",
    "   - The **Input Gate** decides how much of the new input to keep.\n",
    "   - The **Forget Gate** decides how much of the current memory to forget.\n",
    "   - The **Output Gate** decides what part of the current state will be outputted.\n",
    "\n",
    "2. **Cell State**:\n",
    "   - The \"long-term memory\" where information can be stored, retrieved, or forgotten over time.\n",
    "\n",
    "3. **Hidden State**:\n",
    "   - Similar to RNNs, it's the \"short-term memory\" capturing information from recent steps.\n",
    "\n",
    "4. **Recurrent Loop**:\n",
    "   - Like RNNs, LSTMs process one part of the sequence at a time, but with more complex operations to avoid problems like vanishing gradients, making them capable of learning long-term dependencies.\n",
    "\n",
    "5. **Output**:\n",
    "   - LSTMs also produce an output at each step based on the current input, memory, and state.\n",
    "\n",
    "6. **Backpropagation**:\n",
    "   - Similar to RNNs, errors are back-propagated through the network during training to adjust the weights.\n",
    "\n",
    "<div> <img src=https://dwbi1.files.wordpress.com/2021/08/fig-4-lstm.jpg  width =\"400\"/> </div>\n",
    "\n",
    "*Image Source: https://dwbi1.wordpress.com/2021/08/07/recurrent-neural-network-rnn-and-lstm/*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MKMMQKW7wRk"
   },
   "source": [
    "### Transformer Architecture\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LV7-5k1BCM8w"
   },
   "source": [
    "\n",
    "1. **Input Embedding**:\n",
    "    - Your words (text) are turned into numbers using a process called embedding, so the model can understand and process them.\n",
    "\n",
    "2. **Positional Encoding**:\n",
    "    - Since the model doesn't read words in order like humans do, it needs to know the position of each word in a sentence. Positional encoding is added to the embeddings to give that information.\n",
    "\n",
    "3. **Multi-Head Attention**:\n",
    "    - This is the part where the model pays \"attention\" to different parts of the input text. It helps the model to focus on different words when trying to understand the meaning of a particular word.\n",
    "\n",
    "4. **Normalization and Residual Connection**:\n",
    "    - After attention, the outputs are normalized (made more uniform) and added to the original input embeddings to ensure smooth training and to help with learning long-term dependencies.\n",
    "\n",
    "5. **Feed Forward Neural Network**:\n",
    "    - The model then applies a simple neural network independently to each position, processing the representations further.\n",
    "\n",
    "6. **Output of Encoder**:\n",
    "    - The processed representations are then passed to the next layer of the model (or to the decoder if this is the final layer).\n",
    "\n",
    "7. **Decoder**:\n",
    "    - The decoder has similar components as the encoder but with an additional multi-head attention layer that looks at the encoder's output. This helps the model to generate coherent responses or translations.\n",
    "\n",
    "8. **Final Linear and Softmax Layer**:\n",
    "    - In the final step, the model makes predictions, like predicting the next word in a sequence or translating the input text into another language.\n",
    "\n",
    "9. **Output**:\n",
    "    - The model gives out the predicted words or translated text as the output.\n",
    "    \n",
    "<div> <img src=https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png  width =\"400\"/> </div>\n",
    "\n",
    "*Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 6000–6010.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ow3Z_wr61DB"
   },
   "source": [
    "### Hugging Face Transformers Library\n",
    "\n",
    "The **Hugging Face Transformers** library is a popular library that provides pre-trained models and tokenizers for various NLP tasks. It supports a wide range of models including GPT-2, BERT, T5, and many others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ahRaMWDft9W"
   },
   "source": [
    "# Introduction to BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNvx16mRfxIY"
   },
   "source": [
    "## 1. Background\n",
    "\n",
    "Traditional language models operated in a unidirectional manner, either from left-to-right (like GPT) or from right-to-left. However, BERT brought a paradigm shift by being designed to read text bidirectionally. This bidirectional understanding means that when processing a particular word, BERT considers context from both its left and its right, capturing richer semantic meanings, especially for polysemous words (words with multiple meanings).\n",
    "\n",
    "## 2. BERT's Architecture\n",
    "\n",
    "BERT utilizes the Transformer architecture, first introduced by Vaswani et al. in 2017. Let's dive deeper into the architecture:\n",
    "\n",
    "### Embedding Layer:\n",
    "The input tokens are converted into vectors through embeddings. BERT uniquely combines token, segment, and position embeddings for each token's representation.\n",
    "\n",
    "### Transformer Blocks:\n",
    "The embedded tokens undergo a series of Transformer blocks. Each block has:\n",
    "\n",
    "- **Multi-head Self Attention Mechanism**: Allows the model to focus on different parts of the input when interpreting a word.\n",
    "- **Feed-forward Neural Networks**: Post attention, the tokens are channeled through this network.\n",
    "- **Residual Connections**: These connections surround every sub-layer and help avoid vanishing gradient issues in deep networks.\n",
    "- **Layer Normalization**: Applied before each sub-layer to stabilize neuron activations.\n",
    "\n",
    "### Output:\n",
    "Post all Transformer blocks, the final embeddings are used for tasks like classification or token-level predictions.\n",
    "\n",
    "## 3. Pre-training and Fine-tuning\n",
    "\n",
    "BERT's prowess comes from its two-stage training:\n",
    "\n",
    "- **Pre-training**: BERT learns word representations from vast text data through two tasks: masked language modeling (predicting masked words from their context) and next sentence prediction (understanding sentence relationships).\n",
    "\n",
    "- **Fine-tuning**: Post pre-training, BERT is tailored on specific tasks using smaller, task-specific datasets.\n",
    "\n",
    "Let's explore a hands-on example of fine-tuning BERT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nxS2CNAf7xH"
   },
   "source": [
    "## 4. Hands-on: Using BERT for a Simple Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTad_Qx1APpW"
   },
   "source": [
    "###  Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qJCzhsu1n4ok"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A47kF2sx_vg9"
   },
   "source": [
    "### Step 1: Install All the Required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-O7WQTuEgAlo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFnQI9tRAmEI"
   },
   "source": [
    "### Step 2: Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJPNtBRcM2IM"
   },
   "source": [
    "The Microsoft Research Paraphrase Corpus (MRPC) is a part of the General Language Understanding Evaluation (GLUE) benchmark, which is a collection of natural language understanding tasks for evaluating the performance of language models. MRPC specifically focuses on the task of paraphrase identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LXQXvDm7BCOu"
   },
   "outputs": [],
   "source": [
    "# 1. Load the MRPC part of the GLUE dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux_jeglAAsWk"
   },
   "source": [
    "### Step 3: Load the Tokenizer and Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IYDHTKEHBE_r"
   },
   "outputs": [],
   "source": [
    "# 2. Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 3. Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdn43CtrA1W1"
   },
   "source": [
    "### Step 4: Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JgWSPXZ7BHdY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 4. Load the model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # MRPC is binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lh-zcrDyAvdN"
   },
   "source": [
    "### Step 5: Model Training & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m5AVvDxEgJBG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[codecarbon INFO @ 10:47:11] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:47:11] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:47:11] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 10:47:11] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:47:11] No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: /fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/codecarbon/data/hardware/cpu_power.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:47:14] CPU Model on constant consumption mode: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:47:14] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:47:14]   Platform system: Linux-5.4.0-131-generic-x86_64-with-glibc2.27\n",
      "[codecarbon INFO @ 10:47:14]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:47:14]   CodeCarbon version: 2.6.0\n",
      "[codecarbon INFO @ 10:47:14]   Available RAM : 1.000 GB\n",
      "[codecarbon INFO @ 10:47:14]   CPU count: 2\n",
      "[codecarbon INFO @ 10:47:14]   CPU model: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:47:14]   GPU count: 1\n",
      "[codecarbon INFO @ 10:47:14]   GPU model: 1 x NVIDIA A40\n",
      "[codecarbon INFO @ 10:47:18] Saving emissions data to file /fs01/projects/green-ai/Shaina/results/emissions.csv\n",
      "[codecarbon INFO @ 10:47:47] Energy consumed for RAM : 0.000002 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:47:48] Energy consumed for all GPUs : 0.000383 kWh. Total GPU Power : 89.30155833035045 W\n",
      "[codecarbon INFO @ 10:47:48] Energy consumed for all CPUs : 0.000183 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:47:48] 0.000568 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 02:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>0.569732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.567600</td>\n",
       "      <td>0.535866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.534100</td>\n",
       "      <td>0.522772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.511300</td>\n",
       "      <td>0.485494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.440300</td>\n",
       "      <td>0.534196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.615421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.301600</td>\n",
       "      <td>0.531237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.622070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.355900</td>\n",
       "      <td>0.561880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.745218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.770624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.801923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.782590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:48:02] Energy consumed for RAM : 0.000003 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:48:02] Energy consumed for all GPUs : 0.001092 kWh. Total GPU Power : 174.96301846037366 W\n",
      "[codecarbon INFO @ 10:48:02] Energy consumed for all CPUs : 0.000355 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:48:02] 0.001450 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:48:17] Energy consumed for RAM : 0.000005 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:48:17] Energy consumed for all GPUs : 0.002049 kWh. Total GPU Power : 229.9959771531692 W\n",
      "[codecarbon INFO @ 10:48:17] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:48:17] 0.002586 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:48:32] Energy consumed for RAM : 0.000006 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:48:32] Energy consumed for all GPUs : 0.003041 kWh. Total GPU Power : 238.42567606421713 W\n",
      "[codecarbon INFO @ 10:48:32] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:48:32] 0.003756 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:48:47] Energy consumed for RAM : 0.000008 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:48:47] Energy consumed for all GPUs : 0.003709 kWh. Total GPU Power : 159.05335929872757 W\n",
      "[codecarbon INFO @ 10:48:47] Energy consumed for all CPUs : 0.000889 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:48:48] 0.004606 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:49:02] Energy consumed for RAM : 0.000009 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:49:02] Energy consumed for all GPUs : 0.004609 kWh. Total GPU Power : 223.86779355461567 W\n",
      "[codecarbon INFO @ 10:49:02] Energy consumed for all CPUs : 0.001060 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:49:02] 0.005678 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:49:17] Energy consumed for RAM : 0.000011 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:49:17] Energy consumed for all GPUs : 0.005641 kWh. Total GPU Power : 247.85899348955337 W\n",
      "[codecarbon INFO @ 10:49:17] Energy consumed for all CPUs : 0.001237 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:49:17] 0.006889 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:49:32] Energy consumed for RAM : 0.000012 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:49:32] Energy consumed for all GPUs : 0.006681 kWh. Total GPU Power : 249.36426401683678 W\n",
      "[codecarbon INFO @ 10:49:32] Energy consumed for all CPUs : 0.001414 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:49:32] 0.008107 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:49:33] 0.002664 g.CO2eq/s mean an estimation of 84.00348601150708 kg.CO2eq/year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: /fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:49:47] Energy consumed for RAM : 0.000014 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:49:47] Energy consumed for all GPUs : 0.007606 kWh. Total GPU Power : 222.512740694916 W\n",
      "[codecarbon INFO @ 10:49:47] Energy consumed for all CPUs : 0.001591 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:49:47] 0.009211 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:50:02] Energy consumed for RAM : 0.000016 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:50:02] Energy consumed for all GPUs : 0.008639 kWh. Total GPU Power : 247.79183059121914 W\n",
      "[codecarbon INFO @ 10:50:02] Energy consumed for all CPUs : 0.001768 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:50:02] 0.010422 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:50:13] Energy consumed for RAM : 0.000017 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 10:50:13] Energy consumed for all GPUs : 0.009273 kWh. Total GPU Power : 207.95569709472076 W\n",
      "[codecarbon INFO @ 10:50:13] Energy consumed for all CPUs : 0.001898 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:50:13] 0.011188 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: /fs01/projects/green-ai/greenai/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# 6. Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].remove_columns([\"sentence1\", \"sentence2\", \"idx\"]),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].remove_columns([\"sentence1\", \"sentence2\", \"idx\"]),\n",
    "    compute_metrics=None  # You can define a metric function here if needed\n",
    ")\n",
    "\n",
    "# 7. Train the model\n",
    "trainer.train()\n",
    "\n",
    "# 8. Evaluate the model\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncnbRdEPBat8"
   },
   "source": [
    "### Step 6: Print the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wQrgq_rWBZW0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5341963171958923, 'eval_runtime': 1.2485, 'eval_samples_per_second': 326.797, 'eval_steps_per_second': 40.85, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGH2bXYhBiRf"
   },
   "source": [
    "### Step 7: Save Pre-Trained Model to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjY4OAi9oCfi",
    "outputId": "e6e23094-f4bb-48ca-eda0-a0c08aefa36a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert_sentiment_model/tokenizer_config.json',\n",
       " 'bert_sentiment_model/special_tokens_map.json',\n",
       " 'bert_sentiment_model/vocab.txt',\n",
       " 'bert_sentiment_model/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save_directory = \"/content/drive/MyDrive/bert_sentiment_model\" #use this command when saving to google drive\n",
    "save_directory = \"bert_sentiment_model\"\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgS34cfnBz-J"
   },
   "source": [
    "### Step 8: Load Pre-Trained Model from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jUJ72JfoHK1",
    "outputId": "17bf9a0e-b900-4d36-988c-4a77861f4f80"
   },
   "outputs": [],
   "source": [
    "#load_directory = \"/content/drive/MyDrive/bert_sentiment_model\" #use this command when saving to google drive\n",
    "load_directory = \"bert_sentiment_model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(load_directory)\n",
    "model = BertForSequenceClassification.from_pretrained(load_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2FP5cUtB7O-"
   },
   "source": [
    "### Step 9: Test Pre-Trained Model on Different Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZqW0FISqgoY",
    "outputId": "1ff518a2-ac12-4008-d58b-5be81d8f79c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: \n",
      "'The movie was fantastic! Absolutely loved it.' \n",
      "and \n",
      "'I really enjoyed the movie, it was wonderful.' \n",
      "are equivalent\n",
      "\n",
      "\n",
      "Sentences: \n",
      "'The movie was terrible. I regret watching it.' \n",
      "and \n",
      "'This film was a masterpiece!' \n",
      "are not equivalent\n"
     ]
    }
   ],
   "source": [
    "# Test on diverse examples\n",
    "def predict_equivalence(sentence1, sentence2):\n",
    "    inputs = tokenizer(sentence1, sentence2, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    is_equivalent = 'equivalent' if torch.argmax(outputs.logits, dim=1).item() == 1 else 'not equivalent'\n",
    "    return is_equivalent\n",
    "\n",
    "sentence_pair1 = (\"The movie was fantastic! Absolutely loved it.\", \"I really enjoyed the movie, it was wonderful.\")\n",
    "sentence_pair2 = (\"The movie was terrible. I regret watching it.\", \"This film was a masterpiece!\")\n",
    "\n",
    "print(f\"Sentences: \\n'{sentence_pair1[0]}' \\nand \\n'{sentence_pair1[1]}' \\nare {predict_equivalence(*sentence_pair1)}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Sentences: \\n'{sentence_pair2[0]}' \\nand \\n'{sentence_pair2[1]}' \\nare {predict_equivalence(*sentence_pair2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEz3zFsAgpr-"
   },
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "BERT's bidirectional attention and its Transformer-based architecture allow it to achieve state-of-the-art performance on many NLP tasks. This, combined with its adaptability through fine-tuning, showcases its versatility and significance in the modern NLP landscape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVzYM7eA8gxV"
   },
   "source": [
    "# Introduction to GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh0GK8-i72pe"
   },
   "source": [
    "## 1. Background\n",
    "GPT-2, which stands for Generative Pre-trained Transformer 2, is a language model. Developed by OpenAI, GPT-2 was released in February 2019. It's a large model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 follows a Transformer architecture and was notable for its size on release. It was pre-trained on a WebText dataset, which includes text from 45 million website linksThe primary objective of GPT-2 is to predict the next word given all previous words within a text. It is designed to generate human-like text by predicting the next word in a sequence given the words that have come before it. With 1.5 billion parameters, GPT-2 is capable of understanding and generating text across a variety of tasks without task-specific training data, making it a powerful and versatile language model.\n",
    "\n",
    "## 2. Architecture\n",
    "\n",
    "GPT-2 is based on the Transformer architecture, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. Unlike recurrent or convolutional architectures, Transformer models process input sequences in parallel, enabling faster training and inference times. GPT-2, in particular, utilizes the decoder-only variant of the Transformer, and it is trained to minimize the negative log likelihood of the predicted word, given the preceding words.\n",
    "\n",
    "## 3. Pre-training and Fine-tuning\n",
    "\n",
    "### Pre-training\n",
    "GPT-2 is pre-trained on a large corpus of text data. During this phase, the model learns to predict the next word in a sequence given the previous words. It captures a wide range of language patterns and structures.\n",
    "\n",
    "### Fine-tuning\n",
    "Although GPT-2 can be used out-of-the-box for a variety of tasks, fine-tuning it on a smaller, task-specific dataset can further improve performance. Fine-tuning adjusts the pre-trained parameters to better suit the specific task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZD0Dcy0d8JO1"
   },
   "source": [
    "\n",
    "## 4. Hands-on: Using GPT-2 for a Simple Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3aI_ovp-kP_"
   },
   "source": [
    "### Step 1: Install All the Required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KbySERG7339M"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc-qfE-M-m6H"
   },
   "source": [
    "### Step 2: Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rGh_jNOc-U5l"
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqWQorcf-3KB"
   },
   "source": [
    "### Step 3: Function for Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AkMMp46w-VOm"
   },
   "outputs": [],
   "source": [
    "# Define a function to generate text\n",
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTc9H78A_Hdi"
   },
   "source": [
    "### Step 4: Test the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qvA5nwpV-VZq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a village where.\n",
      "\n",
      "\"I'm not sure if it's a village or not, but it's a village. It's a village. It's a village. It's a village. It's a village. It's a village. It's a village. It's a village. It's a village. It's a village. It's a village. It's a village. It's a village. It's a village. It's a\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "prompt = \"Once upon a time, there was a village where.\"\n",
    "generated_story = generate_text(prompt)\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XtHcvCb8MMo"
   },
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "GPT-2 is a powerful and versatile language model that can be used for a wide range of natural language processing tasks. Its Transformer-based architecture allows it to generate coherent and contextually relevant text across a variety of domains. By pre-training on a large corpus and fine-tuning on a task-specific dataset, GPT-2 can be adapted to many different NLP tasks, showcasing the potential and versatility of modern language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylSwKEEGpf7Y"
   },
   "source": [
    "# Introduction to LLAMA \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZ8AQl2x4b6N"
   },
   "source": [
    "### 1. Background\n",
    "The Llama (Large Language Model Meta AI) series, developed by Meta, represents state-of-the-art advancements in large language models. Building on the foundational Transformers architecture, which has been pivotal in natural language processing since 2016, Llama models are designed to provide robust performance across diverse language tasks. These models are optimized through techniques like supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to enhance alignment with human preferences for safety and helpfulness. They are released in multiple sizes and configurations to cater to both commercial and research purposes.\n",
    "\n",
    "#### Key Features of Llama Models:\n",
    "Model Sizes: Llama models are available in various parameter sizes, designed to balance performance and resource efficiency. Newer versions often expand these options to cater to evolving needs.\n",
    "Performance: Each iteration builds on its predecessor, offering improved natural language understanding and generation capabilities.\n",
    "Accessibility: While earlier versions may have been limited to research use, newer iterations typically expand access for commercial applications, often under permissive licenses.\n",
    "Training and Context Length: Successive versions often benefit from larger and more diverse training datasets, extended context lengths, and advanced fine-tuning methods.\n",
    "Alignment: Focused on user-centric design, the models undergo extensive alignment processes to ensure safe and effective interactions.\n",
    "### 2. Architecture\n",
    "Llama models are based on the Generative Pretrained Transformer (GPT) architecture. They are auto-regressive transformers that predict text sequentially, optimizing for performance and usability. Iterative improvements in architecture and training methodologies distinguish each version of the Llama series, ensuring better alignment with user needs and advancing the state of the art in NLP.\n",
    "\n",
    "### 3. Pre-training and Fine-tuning\n",
    "Pre-training\n",
    "Llama models are pre-trained on massive text corpora encompassing diverse domains, enabling them to build a strong foundational understanding of language. During pre-training, the models learn to predict the next word in a sequence, mastering linguistic patterns and structures across a broad spectrum of topics.\n",
    "\n",
    "Fine-tuning\n",
    "Fine-tuning tailors a pre-trained Llama model to specific tasks, enhancing its utility in targeted applications. This process adjusts the model's parameters using smaller, task-specific datasets, significantly improving its performance for specialized use cases.\n",
    "\n",
    "### 4. Quantized Models and Community Contributions\n",
    "The Hugging Face community and similar platforms offer quantized versions of Llama models, which optimize them for resource-constrained environments like GPUs on platforms such as Google Colab. Quantization reduces model size and computational requirements without significant loss of accuracy, making Llama models accessible for broader use cases.\n",
    "\n",
    " Currently, in this work we use **Llama3.2 1B instruct* model, however you can switch to other versions in the model_id field.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hrteOEu3mgI"
   },
   "source": [
    "## Hands-on: Using Llama-3.2-1B for a Simple Task\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkY9zSx88_yh"
   },
   "source": [
    "### Step 1: Install All the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rz_-QQqlpsew",
    "outputId": "500788d9-d6e1-4fc1-b630-9bed537a23b8"
   },
   "outputs": [],
   "source": [
    "# GPU llama-cpp-python\n",
    "# The llama-cpp-python bindings provide simple access to using llama.cpp from within Python.\n",
    "#!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LxswtYo6pyss"
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"  # if it does not run due to restrictions, please fill in form here https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct an use the below\n",
    "\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4dVwi3qY9dg"
   },
   "source": [
    "### Step 2: Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8AQnG6DEXLwR",
    "outputId": "bc5162f6-6309-496c-9bdb-46ee31c1c4f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"I'd be happy to suggest some healthy diets for you. Here are a few options:\\n\\n**1. Mediterranean Diet:**\\n\\nThis diet emphasizes whole grains, fruits, vegetables, lean proteins, and healthy fats, such as those found in olive oil. It's been shown to reduce the risk of heart disease, type 2 diabetes, and certain cancers.\\n\\n**2. Plant-Based Diet:**\\n\\nThis diet focuses on plant-based foods, such as fruits, vegetables, whole grains, and legumes. It's been shown to reduce the risk of heart disease, type 2 diabetes, and certain cancers.\\n\\n**3. Flexitarian Diet:**\\n\\nThis diet is primarily vegetarian but allows for occasional consumption of lean meats. It's been shown to reduce the risk of heart disease and type 2 diabetes.\\n\\n**4. DASH Diet (Dietary Approaches to Stop Hypertension):**\\n\\nThis diet emphasizes whole grains, fruits, vegetables, lean proteins, and low-fat dairy. It's been shown to reduce blood pressure and improve overall health.\\n\\n**5. Keto Diet (Ketogenic Diet):**\\n\\nThis diet involves a very low-carb, high-fat diet. It's been shown to promote weight loss, improve blood sugar control, and reduce inflammation.\\n\\n**6.\"}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = model_id\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a great medical advisor.!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you suggest me some healthy diet?\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEje_19n7jRL"
   },
   "source": [
    "\n",
    "## 5. Conclusion\n",
    "Llama 2 excels in various natural language processing tasks due to its extensive training and fine-tuning. With improved accessibility for commercial use, it demonstrates the evolving potential and adaptability of modern language models in addressing real-world challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEsPvITUIMAR"
   },
   "source": [
    "# Introduction to Fair-Sense-AI\n",
    "\n",
    "## A bias detection multi modal tool to find biases in texts and images. It is optimized on green ai methods for ease of use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uAZ_1nEA0E4"
   },
   "source": [
    "\n",
    "### **FairSense-AI Resources**\n",
    "\n",
    "#### 1. **Documentation and Overview**\n",
    "   - **Website**: [FairSense-AI Documentation](https://vectorinstitute.github.io/FairSense-AI/)\n",
    "   - **Description**: A comprehensive guide to understanding and implementing fairness in AI systems. This site provides insights into the methodology, applications, and best practices for using FairSense-AI.\n",
    "\n",
    "#### 2. **Python Package**\n",
    "   - **PyPI Page**: [Fair-Sense-AI Python Package](https://pypi.org/project/fair-sense-ai/)\n",
    "   - **Description**: The official Python package for FairSense-AI, designed for seamless integration into AI pipelines. It provides tools for fairness evaluation, bias mitigation, and reporting.\n",
    "\n",
    "#### **Key Features**\n",
    "   - **User-Friendly Interface**: Simplifies the fairness analysis process.\n",
    "   - **Comprehensive Metrics**: Includes multiple fairness metrics tailored for various AI models.\n",
    "   - **Customizable Pipelines**: Enables adaptation to diverse datasets and models.\n",
    "\n",
    "#### **Get Started**\n",
    "   - **Install the Package**:\n",
    "     ```bash\n",
    "     pip install Fair-Sense-AI\n",
    "     ```\n",
    "   - **Explore the Documentation**: Visit the [documentation site](https://vectorinstitute.github.io/FairSense-AI/) to learn how to use the package effectively.\n",
    "\n",
    "#### **Applications**\n",
    "   - Evaluating and improving fairness in machine learning models.\n",
    "   - Supporting ethical AI development across industries such as healthcare, finance, and education.\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install fair-sense-ai\n",
    "!pip show fair-sense-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Run This Colab Notebook\n",
    "\n",
    "### Steps to Run the Notebook\n",
    "1. **Open the Notebook:**\n",
    "   - Click on this [link](https://colab.research.google.com/drive/1en8JtZTAIa5MuV5OZWYNteYl95Ql9xy7?usp=sharing) to open the notebook in Google Colab.\n",
    "\n",
    "2. **Set Up Your Environment:**\n",
    "   - Ensure you're signed in to your Google account.\n",
    "   - Once opened, Colab will automatically provision a runtime environment.\n",
    "\n",
    "3. **Connect to the Runtime:**\n",
    "   - Click the \"Connect\" button in the top-right corner of the Colab interface to initialize the environment.\n",
    "\n",
    "4. **Install Required Dependencies:**\n",
    "   - The notebook might include cells with `!pip install` commands to set up required libraries. Run these cells first to install dependencies.\n",
    "\n",
    "5. **Run All Cells:**\n",
    "   - To execute all cells in the notebook:\n",
    "     - Go to **Runtime > Run all** in the menu bar.\n",
    "   - Alternatively, run individual cells by selecting a cell and pressing `Shift + Enter`.\n",
    "\n",
    "6. **Customize Execution (if needed):**\n",
    "   - Modify input cells or parameters as required before running the notebook.\n",
    "\n",
    "7. **Save Your Work:**\n",
    "   - To save a copy of the notebook:\n",
    "     - Click **File > Save a \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LuofgKj0D6U0",
    "7MKMMQKW7wRk",
    "SVzYM7eA8gxV"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "green-ai",
   "language": "python",
   "name": "green-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "curious-lexington",
   "metadata": {},
   "source": [
    "# Tutorial on Optimizing Language Models\n",
    "\n",
    "This tutorial explores advanced optimization techniques for language models like GPT-2. The goal is to \n",
    "reduce computational costs and environmental impact while retaining high performance.\n",
    "\n",
    "### Objectives:\n",
    "1. Details on optimization methods like pruning, quantization, and distillation, including their mathematical foundations.\n",
    "2. Measure the impact of optimizations on performance and carbon emissions.\n",
    "3. Visualize and compare results to identify the most effective techniques.\n",
    "\n",
    "### Why Optimize?\n",
    "LLMs require significant computational resources. By optimizing models:\n",
    "- **Efficiency**: Reduce inference time and memory usage.\n",
    "- **Sustainability**: Lower carbon emissions.\n",
    "- **Deployability**: Enable models to run on resource-constrained devices.\n",
    "\n",
    "### References:\n",
    "- **Pruning**: Han et al., [\"Learning both Weights and Connections for Efficient Neural Networks\"](https://arxiv.org/abs/1506.02626)\n",
    "- **Quantization**: Jacob et al., [\"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\"](https://arxiv.org/abs/1712.05877)\n",
    "- **Knowledge Distillation**: Hinton et al., [\"Distilling the Knowledge in a Neural Network\"](https://arxiv.org/abs/1503.02531)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "capable-appliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CPU Information ===\n",
      "Processor: x86_64\n",
      "CPU Count: 32\n",
      "Logical CPUs: 32\n",
      "Physical CPUs: 16\n",
      "CPU Frequency: 2387.6037187499996 MHz\n",
      "\n",
      "=== RAM Information ===\n",
      "Total RAM: 188.59 GB\n",
      "Available RAM: 170.31 GB\n",
      "\n",
      "=== GPU Information ===\n",
      "GPU Count: 1\n",
      "GPU 0: NVIDIA A40\n",
      "Memory Allocated: 0.00 MB\n",
      "Memory Cached: 0.00 MB\n",
      "\n",
      "=== System Information ===\n",
      "System: Linux\n",
      "Machine: x86_64\n",
      "Node: gpu028\n",
      "Version: #147~18.04.1-Ubuntu SMP Sat Oct 15 13:10:18 UTC 2022\n"
     ]
    }
   ],
   "source": [
    "#checking hardware related information\n",
    "\n",
    "import os\n",
    "import platform\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "def check_hardware():\n",
    "    print(\"=== CPU Information ===\")\n",
    "    print(f\"Processor: {platform.processor()}\")\n",
    "    print(f\"CPU Count: {os.cpu_count()}\")\n",
    "    if psutil:\n",
    "        print(f\"Logical CPUs: {psutil.cpu_count(logical=True)}\")\n",
    "        print(f\"Physical CPUs: {psutil.cpu_count(logical=False)}\")\n",
    "        print(f\"CPU Frequency: {psutil.cpu_freq().current} MHz\")\n",
    "    \n",
    "    print(\"\\n=== RAM Information ===\")\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {ram.total / 1024**3:.2f} GB\")\n",
    "    print(f\"Available RAM: {ram.available / 1024**3:.2f} GB\")\n",
    "    \n",
    "    print(\"\\n=== GPU Information ===\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n",
    "            print(f\"Memory Cached: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "    \n",
    "    print(\"\\n=== System Information ===\")\n",
    "    print(f\"System: {platform.system()}\")\n",
    "    print(f\"Machine: {platform.machine()}\")\n",
    "    print(f\"Node: {platform.node()}\")\n",
    "    print(f\"Version: {platform.version()}\")\n",
    "\n",
    "check_hardware()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "flying-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppress warnings\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings or errors\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Disable SLURM-related queries in the environment\n",
    "os.environ[\"CODECARBON_SCONTROL_WARNING\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-budapest",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Do the necassary imports\n",
    "1. **`time`**: For measuring execution time of code.\n",
    "2. **`torch`**: PyTorch library for tensor computations and deep learning.\n",
    "3. **`transformers`**: Hugging Face tools for pre-trained transformer models.\n",
    "4. **`copy`**: For creating deep or shallow copies of objects.\n",
    "5. **`os`**: To interact with the operating system (e.g., environment variables).\n",
    "6. **`logging`**: For debugging and monitoring with log messages.\n",
    "7. **`matplotlib.pyplot`**: For creating visualizations and charts.\n",
    "8. **`codecarbon`**: To estimate carbon emissions from computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "introductory-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Config\n",
    "import copy\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-wright",
   "metadata": {},
   "source": [
    "### Disabling Tokenizer Parallelism\n",
    "\n",
    "Setting `TOKENIZERS_PARALLELISM` to `False` avoids multi-threading conflicts during tokenization. \n",
    "This ensures smoother execution of the Hugging Face pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "trying-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"False\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-calvin",
   "metadata": {},
   "source": [
    "\n",
    "## Initialize Model and Tokenizer\n",
    "\n",
    "#### Explanation\n",
    "1. **Tokenization**: Converts text into a sequence of tokens (integers) that the model can process.\n",
    "2. **Model Loading**: GPT-2, a pre-trained generative model, is loaded.\n",
    "3. **Device Setup**: Ensures computations run on a GPU (if available) for efficiency.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continued-superintendent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-thomson",
   "metadata": {},
   "source": [
    "\n",
    "## Measure Baseline Performance\n",
    "\n",
    "We measure the **inference time** and **carbon emissions** of the unoptimized model using `codecarbon`. \n",
    "This baseline serves as a reference to evaluate optimizations.\n",
    "\n",
    "#### Key Metrics:\n",
    "- **Duration**: Time taken for the model to generate outputs.\n",
    "- **Emissions**: Estimated carbon footprint in kg CO2eq.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "similar-module",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:27:11] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 13:27:11] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 13:27:12] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 13:27:12] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 13:27:12] No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: /fs01/projects/green-ai/envs/green-ai/lib/python3.10/site-packages/codecarbon/data/hardware/cpu_power.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:27:16] CPU Model on constant consumption mode: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 13:27:16] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 13:27:16]   Platform system: Linux-5.4.0-131-generic-x86_64-with-glibc2.27\n",
      "[codecarbon INFO @ 13:27:16]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 13:27:16]   CodeCarbon version: 2.6.0\n",
      "[codecarbon INFO @ 13:27:16]   Available RAM : 1.000 GB\n",
      "[codecarbon INFO @ 13:27:16]   CPU count: 2\n",
      "[codecarbon INFO @ 13:27:16]   CPU model: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 13:27:16]   GPU count: 1\n",
      "[codecarbon INFO @ 13:27:16]   GPU model: 1 x NVIDIA A40\n",
      "[codecarbon INFO @ 13:27:20] Saving emissions data to file /fs01/projects/green-ai/Shaina/emissions.csv\n",
      "[codecarbon INFO @ 13:27:36] Energy consumed for RAM : 0.000002 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 13:27:36] Energy consumed for all GPUs : 0.000366 kWh. Total GPU Power : 87.05686032317743 W\n",
      "[codecarbon INFO @ 13:27:36] Energy consumed for all CPUs : 0.000179 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:27:36] 0.000547 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:27:40] Energy consumed for RAM : 0.000002 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 13:27:40] Energy consumed for all GPUs : 0.000477 kWh. Total GPU Power : 95.60816449712976 W\n",
      "[codecarbon INFO @ 13:27:40] Energy consumed for all CPUs : 0.000228 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:27:40] 0.000707 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total outputs generated: 5\n",
      "ref: /fs01/projects/green-ai/envs/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 13:27:41] Tracker already stopped !\n",
      "[codecarbon INFO @ 13:27:41] Energy consumed for RAM : 0.000002 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 13:27:41] Energy consumed for all GPUs : 0.000501 kWh. Total GPU Power : 86.89445059900272 W\n",
      "[codecarbon INFO @ 13:27:41] Energy consumed for all CPUs : 0.000240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:27:41] 0.000743 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline:\n",
      "Duration: 19.25 seconds\n",
      "Estimated Emissions: 0.000028 kg CO2eq\n",
      "Generated Texts: ['Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a', 'Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a', 'Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a', 'Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a', 'Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a']\n",
      "ref: /fs01/projects/green-ai/envs/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n"
     ]
    }
   ],
   "source": [
    "def run_inference(model, prompt, model_device, num_iterations=5):\n",
    "    model.eval()\n",
    "    results = []  # Initialize as a list to collect decoded outputs\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_device)\n",
    "            outputs = model.generate(**inputs, max_length=50, pad_token_id=model.config.eos_token_id)\n",
    "            decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            results.append(decoded_output)  # Append decoded text to the list\n",
    "    print(f\"Total outputs generated: {len(results)}\")  # Debug statement\n",
    "    return results  # Return all collected outputs\n",
    "\n",
    "# Modify measure_performance to handle returned results\n",
    "def measure_performance(model, prompt, name, cc_verbose=True):\n",
    "    model_device = next(model.parameters()).device\n",
    "    log_level = \"info\" if cc_verbose else \"warning\"\n",
    "    tracker = EmissionsTracker(log_level=log_level)\n",
    "    try:\n",
    "        tracker.start()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = run_inference(model, prompt, model_device)  # Get all outputs\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        emissions = tracker.stop()\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"Duration: {duration:.2f} seconds\")\n",
    "        print(f\"Estimated Emissions: {emissions:.6f} kg CO2eq\")\n",
    "        print(f\"Generated Texts: {results}\")  # Print all collected outputs\n",
    "        return duration, emissions\n",
    "    except Exception as e:\n",
    "        print(f\"Error measuring {name}: {str(e)}\")\n",
    "        return None, None\n",
    "    finally:\n",
    "        tracker.stop()\n",
    "\n",
    "\n",
    "baseline_duration, baseline_emissions = measure_performance(model, \"Once upon a time\", \"Baseline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-puzzle",
   "metadata": {},
   "source": [
    "\n",
    "## **Method 1 : Model Pruning**\n",
    "\n",
    "Model pruning is a widely-used optimization technique that eliminates unimportant or redundant weights from a neural network. By reducing the number of active parameters, it minimizes computational requirements and leads to a smaller, more efficient model. The resulting weight matrix is typically sparse:\n",
    "\n",
    "```\n",
    "W_pruned = W * M\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `W` is the original weight matrix.\n",
    "- `M` is a binary mask where:\n",
    "  - `1` indicates weights retained.\n",
    "  - `0` indicates weights pruned.\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Pruning**\n",
    "1. **Global Pruning**: Removes weights across the entire model based on importance.\n",
    "2. **Layer-Wise Pruning**: Targets specific layers for pruning, maintaining balance in model architecture.\n",
    "3. **Structured Pruning**: Eliminates entire neurons, channels, or filters, simplifying computations further.\n",
    "----\n",
    "\n",
    "### **Advantages**\n",
    "1. **Accelerated Inference**: Smaller models require fewer computations, enabling faster predictions—especially critical for real-time applications like speech recognition or online recommendations.\n",
    "2. **Memory Efficiency**: Reduces the memory footprint, making models deployable on resource-constrained devices such as mobile phones, IoT devices, or edge servers.\n",
    "3. **Energy Savings**: Fewer computations lead to lower energy consumption, contributing to sustainable AI practices.\n",
    "\n",
    "---\n",
    "\n",
    "### **Trade-offs and Considerations**\n",
    "1. **Potential Accuracy Loss**: Removing too many weights or critical ones can degrade model accuracy. Careful pruning is essential.\n",
    "2. **Increased Complexity**: Requires iterative fine-tuning or retraining to recover lost accuracy, especially in aggressive pruning scenarios.\n",
    "3. **Hardware Support**: Sparse matrices may not achieve peak performance on some hardware architectures unless optimized libraries are used.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key References**\n",
    "- Han et al., [\"Learning both Weights and Connections for Efficient Neural Networks\"](https://arxiv.org/abs/1506.02626)  \n",
    "- Hugging Face Documentation on [Pruning](https://huggingface.co/docs/optimum/v1.2.1/en/intel/pruning)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "figured-farmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Model Pruning\n",
      "Model pruning involves removing less important weights from the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:27:41] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 13:27:41] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 13:27:41] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 13:27:41] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 13:27:41] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 13:27:43] CPU Model on constant consumption mode: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 13:27:43] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 13:27:43]   Platform system: Linux-5.4.0-131-generic-x86_64-with-glibc2.27\n",
      "[codecarbon INFO @ 13:27:43]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 13:27:43]   CodeCarbon version: 2.6.0\n",
      "[codecarbon INFO @ 13:27:43]   Available RAM : 1.000 GB\n",
      "[codecarbon INFO @ 13:27:43]   CPU count: 2\n",
      "[codecarbon INFO @ 13:27:43]   CPU model: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 13:27:43]   GPU count: 1\n",
      "[codecarbon INFO @ 13:27:43]   GPU model: 1 x NVIDIA A40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: /fs01/projects/green-ai/envs/green-ai/lib/python3.10/site-packages/codecarbon/data/hardware/cpu_power.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:27:46] Saving emissions data to file /fs01/projects/green-ai/Shaina/emissions.csv\n",
      "[codecarbon INFO @ 13:27:50] Energy consumed for RAM : 0.000000 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 13:27:50] Energy consumed for all GPUs : 0.000104 kWh. Total GPU Power : 97.11724794784514 W\n",
      "[codecarbon INFO @ 13:27:50] Energy consumed for all CPUs : 0.000045 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:27:50] 0.000149 kWh of electricity used since the beginning.\n",
      "[codecarbon WARNING @ 13:27:50] Tracker already stopped !\n",
      "[codecarbon INFO @ 13:27:50] Energy consumed for RAM : 0.000000 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 13:27:50] Energy consumed for all GPUs : 0.000104 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 13:27:50] Energy consumed for all CPUs : 0.000046 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:27:50] 0.000150 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total outputs generated: 5\n",
      "ref: /fs01/projects/green-ai/envs/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n",
      "\n",
      "Pruned Model:\n",
      "Duration: 3.84 seconds\n",
      "Estimated Emissions: 0.000006 kg CO2eq\n",
      "Generated Texts: ['Once upon a time comm one II II X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X', 'Once upon a time comm one II II X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X', 'Once upon a time comm one II II X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X', 'Once upon a time comm one II II X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X', 'Once upon a time comm one II II X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X']\n",
      "ref: /fs01/projects/green-ai/envs/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n"
     ]
    }
   ],
   "source": [
    "# 1. Model Pruning\n",
    "print(\"\\n1. Model Pruning\")\n",
    "print(\"Model pruning involves removing less important weights from the model.\")\n",
    "def prune_model(model, pruning_amount=0.3):\n",
    "    for param in model.parameters():\n",
    "        mask = torch.rand(param.shape, device=param.device) > pruning_amount\n",
    "        param.data *= mask\n",
    "    return model\n",
    "\n",
    "pruned_model = prune_model(copy.deepcopy(model))\n",
    "pruned_duration, pruned_emissions = measure_performance(pruned_model, \"Once upon a time\", \"Pruned Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-occurrence",
   "metadata": {},
   "source": [
    "\n",
    "## **Method 2 : Quantization**\n",
    "\n",
    "Quantization is an optimization technique that reduces the precision of model parameters, typically converting 32-bit floating-point weights to 8-bit integers. This decreases the memory footprint and computational overhead, making the model more efficient. It is defined as:\n",
    "\n",
    "```\n",
    "w_quantized = round(w / s)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `w` is the original weight value.\n",
    "- `s` is a scaling factor used to map the floating-point values into the integer range.\n",
    "\n",
    "The quantized model uses these integer values during inference, significantly improving performance, especially on hardware optimized for lower precision arithmetic.\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Quantization**:\n",
    "\n",
    "Quantization is a process to optimize machine learning models by reducing the precision of the weights and activations. There are three main types: **Dynamic Quantization**, which applies quantization during runtime for weights while leaving activations in floating-point precision; **Static Quantization**, which quantizes both weights and activations by calibrating on a dataset beforehand; and **Quantization-Aware Training**, which incorporates quantization directly into the training process, often yielding better performance than post-training quantization.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "\n",
    "Quantization offers several benefits. It reduces the memory footprint as storing weights as 8-bit integers requires four times less memory compared to 32-bit floating-point weights. It also accelerates inference since integer arithmetic is computationally cheaper, especially on CPUs or hardware with dedicated quantization support. Additionally, quantization improves energy efficiency as reduced computations translate to lower energy consumption.\n",
    "\n",
    "---\n",
    "### **Trade-offs and Considerations**\n",
    "\n",
    "Despite its advantages, quantization has trade-offs. It may lead to a slight accuracy loss depending on the model and task. Moreover, the performance gains are hardware-dependent, relying on the underlying system's ability to efficiently handle integer operations.\n",
    "\n",
    "---\n",
    "### Key References:\n",
    "   - Jacob et al., [\"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\"](https://arxiv.org/abs/1712.05877)\n",
    "   - [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)\n",
    "   - Hugging Face Blog: [\"Quantization\"](https://huggingface.co/docs/transformers/en/main_classes/quantization)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecological-darkness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Quantization\n",
      "Quantization reduces the precision of the model's weights, typically from 32-bit to 8-bit.\n",
      "Note: Dynamic quantization is currently only supported on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:28:17] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 13:28:18] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 13:28:18] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 13:28:18] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 13:28:18] No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: /fs01/projects/green-ai/envs/green-ai/lib/python3.10/site-packages/codecarbon/data/hardware/cpu_power.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:28:21] CPU Model on constant consumption mode: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 13:28:21] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 13:28:21]   Platform system: Linux-5.4.0-131-generic-x86_64-with-glibc2.27\n",
      "[codecarbon INFO @ 13:28:21]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 13:28:21]   CodeCarbon version: 2.6.0\n",
      "[codecarbon INFO @ 13:28:21]   Available RAM : 1.000 GB\n",
      "[codecarbon INFO @ 13:28:21]   CPU count: 2\n",
      "[codecarbon INFO @ 13:28:21]   CPU model: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 13:28:21]   GPU count: 1\n",
      "[codecarbon INFO @ 13:28:21]   GPU model: 1 x NVIDIA A40\n",
      "[codecarbon INFO @ 13:28:26] Saving emissions data to file /fs01/projects/green-ai/Shaina/emissions.csv\n",
      "[codecarbon INFO @ 13:28:41] Energy consumed for RAM : 0.000002 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 13:28:41] Energy consumed for all GPUs : 0.000373 kWh. Total GPU Power : 88.37316561476844 W\n",
      "[codecarbon INFO @ 13:28:41] Energy consumed for all CPUs : 0.000180 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:28:41] 0.000554 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:28:53] Energy consumed for RAM : 0.000003 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 13:28:53] Energy consumed for all GPUs : 0.000675 kWh. Total GPU Power : 87.68852158552271 W\n",
      "[codecarbon INFO @ 13:28:53] Energy consumed for all CPUs : 0.000326 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:28:53] 0.001004 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total outputs generated: 5\n",
      "ref: /fs01/projects/green-ai/envs/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 13:28:54] Tracker already stopped !\n",
      "[codecarbon INFO @ 13:28:54] Energy consumed for RAM : 0.000003 kWh. RAM Power : 0.375 W\n",
      "[codecarbon INFO @ 13:28:54] Energy consumed for all GPUs : 0.000687 kWh. Total GPU Power : 84.9694006112296 W\n",
      "[codecarbon INFO @ 13:28:54] Energy consumed for all CPUs : 0.000332 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:28:54] 0.001023 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantized Model (CPU):\n",
      "Duration: 27.52 seconds\n",
      "Estimated Emissions: 0.000040 kg CO2eq\n",
      "Generated Texts: ['Once upon a time the \"a\" and \"b\" in the \"-\" and \"-\" in the \"-\" and \"-\" in the \"-\" and, except in the, the, and, the, the, the', 'Once upon a time the \"a\" and \"b\" in the \"-\" and \"-\" in the \"-\" and \"-\" in the \"-\" and, except in the, the, and, the, the, the', 'Once upon a time the \"a\" and \"b\" in the \"-\" and \"-\" in the \"-\" and \"-\" in the \"-\" and, except in the, the, and, the, the, the', 'Once upon a time the \"a\" and \"b\" in the \"-\" and \"-\" in the \"-\" and \"-\" in the \"-\" and, except in the, the, and, the, the, the', 'Once upon a time the \"a\" and \"b\" in the \"-\" and \"-\" in the \"-\" and \"-\" in the \"-\" and, except in the, the, and, the, the, the']\n",
      "ref: /fs01/projects/green-ai/envs/green-ai/lib/python3.10/site-packages/codecarbon/data/private_infra/2016/canada_energy_mix.json\n"
     ]
    }
   ],
   "source": [
    "# 2. Quantization\n",
    "print(\"\\n2. Quantization\")\n",
    "print(\"Quantization reduces the precision of the model's weights, typically from 32-bit to 8-bit.\")\n",
    "print(\"Note: Dynamic quantization is currently only supported on CPU.\")\n",
    "model_cpu = model.cpu()\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model_cpu, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "quantized_duration, quantized_emissions = measure_performance(quantized_model, \"Once upon a time\", \"Quantized Model (CPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-stand",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Method 3 : Knowledge Distillation**\n",
    "\n",
    "Knowledge Distillation is a model compression technique where a smaller, simpler model (the **student model**) learns to mimic the behavior of a larger, more complex model (the **teacher model**). This process transfers knowledge from the teacher to the student, allowing the smaller model to approximate the larger model's performance with significantly fewer parameters.\n",
    "\n",
    "The student model is trained using the teacher model's predictions instead of the actual labels. This involves minimizing a distillation loss:\n",
    "\n",
    "```\n",
    "L = (1 - α) * L_hard + α * L_soft\n",
    "```\n",
    "\n",
    "Where:\n",
    "- \\( L_{hard} \\): Loss based on the actual labels (e.g., cross-entropy).\n",
    "- \\( L_{soft} \\): Loss based on the teacher's predictions (softened using temperature \\( T \\)).\n",
    "- \\( \\alpha \\): Balances the contribution of hard and soft losses.\n",
    "- \\( T \\): A temperature parameter used to soften the output probabilities of the teacher.\n",
    "\n",
    "The softened probabilities allow the student to capture finer-grained information about the teacher's decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Knowledge Distillation**\n",
    "\n",
    "Knowledge distillation offers several benefits, including **model compression**, where the smaller student model is faster, consumes less memory, and is ideal for deployment. It facilitates **knowledge transfer**, enabling the student model to inherit nuanced understanding from the teacher, such as handling ambiguous inputs. Additionally, it improves **efficiency**, making distilled models suitable for edge devices with limited computational power.\n",
    "\n",
    "---\n",
    "\n",
    "### **Trade-offs and Considerations**\n",
    "\n",
    "Despite its advantages, knowledge distillation comes with trade-offs. There may be an **accuracy gap**, as the student model might not fully replicate the teacher's performance. Furthermore, the process involves **training overhead**, requiring additional computational resources during the initial training phase of the student model.\n",
    "\n",
    "------\n",
    "\n",
    "In this tutorial, we use **DistilGPT2**, a pre-trained distilled version of GPT-2, as our student model. DistilGPT2 retains about 97% of GPT-2's performance while being 60% faster and smaller.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Key References**:\n",
    "- Hinton et al., [\"Distilling the Knowledge in a Neural Network\"](https://arxiv.org/abs/1503.02531)\n",
    "- Hugging Face documentation on [DistilGPT2](https://huggingface.co/distilgpt2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Knowledge Distillation\n",
      "Using a pre-trained smaller model (DistilGPT2) as an example of knowledge distillation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:29:01] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 13:29:02] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 13:29:02] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 13:29:02] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 13:29:02] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 13:29:03] CPU Model on constant consumption mode: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 13:29:03] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 13:29:03]   Platform system: Linux-5.4.0-131-generic-x86_64-with-glibc2.27\n",
      "[codecarbon INFO @ 13:29:03]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 13:29:03]   CodeCarbon version: 2.6.0\n",
      "[codecarbon INFO @ 13:29:03]   Available RAM : 1.000 GB\n",
      "[codecarbon INFO @ 13:29:03]   CPU count: 2\n",
      "[codecarbon INFO @ 13:29:03]   CPU model: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 13:29:03]   GPU count: 1\n",
      "[codecarbon INFO @ 13:29:03]   GPU model: 1 x NVIDIA A40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: /fs01/projects/green-ai/envs/green-ai/lib/python3.10/site-packages/codecarbon/data/hardware/cpu_power.csv\n"
     ]
    }
   ],
   "source": [
    "# 3. Knowledge Distillation\n",
    "print(\"\\n3. Knowledge Distillation\")\n",
    "print(\"Using a pre-trained smaller model (DistilGPT2) as an example of knowledge distillation.\")\n",
    "distil_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
    "distil_duration, distil_emissions = measure_performance(distil_model, \"Once upon a time\", \"Distilled Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-settlement",
   "metadata": {},
   "source": [
    "\n",
    "## **Method 4: Efficient Attention**\n",
    "\n",
    "Efficient attention refers to optimizations applied to the attention mechanism in transformer models to reduce memory and computational overhead. The standard self-attention mechanism in transformers has a computational complexity of \\(O(n^2)\\), where \\(n\\) is the sequence length. For large inputs, this quadratic complexity can become a bottleneck.\n",
    "\n",
    "Efficient attention mechanisms aim to:\n",
    "1. Reduce the memory footprint required for attention computations.\n",
    "2. Speed up the attention process, especially for long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Does it Work?**\n",
    "In this example, we use a simplified approach where the attention mechanism is wrapped in a custom module that disables gradient computation during the forward pass:\n",
    "\n",
    "1. **Disabling Gradients**:\n",
    "   - By applying `torch.no_grad()`, the memory and computation associated with storing gradients for backpropagation are avoided during inference.\n",
    "   - This is useful for scenarios where the model is used solely for inference, not training.\n",
    "\n",
    "2. **Class Modification**:\n",
    "   - The `EfficientAttention` class replaces the default `torch.nn.MultiheadAttention` module with a more memory-efficient implementation that skips unnecessary gradient computations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages**\n",
    "1. **Reduced Memory Usage**:\n",
    "   - Memory-intensive gradients are skipped, making the model more lightweight during inference.\n",
    "2. **Improved Speed**:\n",
    "   - Eliminating gradient computations accelerates the forward pass, especially for large inputs or batch sizes.\n",
    "3. **Simplicity**:\n",
    "   - The modification is minimally invasive, as it only wraps the existing attention module.\n",
    "\n",
    "---\n",
    "\n",
    "### **Trade-offs and considerations**\n",
    "1. **No Training Support**:\n",
    "   - The efficient attention mechanism is designed for inference and cannot be used for training without enabling gradients.\n",
    "2. **Applicability**:\n",
    "   - This implementation does not change the quadratic complexity of standard attention; instead, it optimizes the memory and computation for existing operations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key References**\n",
    "- **Transformers**: Vaswani et al., [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762)\n",
    "- **Efficient Attention**: Tay et al., [\"Efficient Transformers: A Survey\"](https://arxiv.org/abs/2009.06732)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4. Efficient Attention\")\n",
    "print(\"This is a simplified example of implementing more efficient attention mechanisms.\")\n",
    "class EfficientAttention(torch.nn.Module):\n",
    "    def __init__(self, attention):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            output = self.attention(*args, **kwargs)\n",
    "        return output\n",
    "\n",
    "def apply_efficient_attention(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.MultiheadAttention):\n",
    "            setattr(model, name, EfficientAttention(module))\n",
    "    return model\n",
    "\n",
    "efficient_model = apply_efficient_attention(copy.deepcopy(model))\n",
    "efficient_duration, efficient_emissions = measure_performance(efficient_model, \"Once upon a time\", \"Efficient Attention Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-bankruptcy",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Method 5 : Smaller Model**\n",
    "\n",
    "Creating a smaller model involves customizing the architecture of a neural network to reduce its size while maintaining acceptable performance. For transformers like GPT-2, this means adjusting key parameters such as:\n",
    "- **Number of Layers** (\\(n_{layer}\\)): Determines the depth of the model.\n",
    "- **Number of Attention Heads** (\\(n_{head}\\)): Affects how the model attends to different parts of the input.\n",
    "- **Embedding Dimensions** (\\(n_{embd}\\)): Controls the size of token representations.\n",
    "\n",
    "In this example, we create a scaled-down GPT-2 model with:\n",
    "- 6 layers (\\(n_{layer} = 6\\)).\n",
    "- 8 attention heads (\\(n_{head} = 8\\)).\n",
    "- Embedding size of 512 (\\(n_{embd} = 512\\)).\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages**\n",
    "1. **Faster Inference**:\n",
    "   - Smaller models require fewer computations, leading to faster predictions.\n",
    "2. **Memory Efficiency**:\n",
    "   - Reduced size decreases the memory footprint, enabling deployment on devices with limited resources.\n",
    "3. **Energy Savings**:\n",
    "   - Fewer computations also lower energy consumption, making the model more eco-friendly.\n",
    "\n",
    "---\n",
    "\n",
    "### **Trade-offs**\n",
    "1. **Accuracy Loss**:\n",
    "   - Smaller models often have reduced capacity to capture complex patterns, which can result in slight accuracy degradation.\n",
    "2. **Task-Specific Performance**:\n",
    "   - Customizing the architecture may require fine-tuning to ensure it meets the demands of specific tasks.\n",
    "---\n",
    "\n",
    "### **Advantages of Smaller Models**\n",
    "- **Deployment Flexibility**:\n",
    "   - Ideal for edge devices, mobile phones, or low-power environments.\n",
    "- **Cost Efficiency**:\n",
    "   - Reduces the infrastructure cost of serving models, especially at scale.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key References**\n",
    "- Hugging Face Documentation: [Model Configuration](https://huggingface.co/docs/transformers/main_classes/configuration)\n",
    "- Vaswani et al., [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-birmingham",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n5. Smaller Model\")\n",
    "print(\"This creates a new, smaller GPT-2 model with fewer layers, heads, and embedding dimensions.\")\n",
    "\n",
    "# Create a smaller GPT-2 model\n",
    "config = GPT2Config(n_layer=6, n_head=8, n_embd=512)\n",
    "smaller_model = AutoModelForCausalLM.from_config(config).to(\"cpu\")  # Adjust device as needed\n",
    "\n",
    "# Measure performance\n",
    "smaller_duration, smaller_emissions = measure_performance(smaller_model, \"Once upon a time\", \"Smaller Model\")\n",
    "\n",
    "if smaller_duration and smaller_emissions:\n",
    "    print(\"\\nSmaller Model Results:\")\n",
    "    print(f\"Duration: {smaller_duration:.2f} seconds\")\n",
    "    print(f\"Estimated Emissions: {smaller_emissions:.6f} kg CO2eq\")\n",
    "else:\n",
    "    print(\"Failed to measure Smaller Model performance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-lodge",
   "metadata": {},
   "source": [
    "\n",
    "## **Calculate Improvements**\n",
    "\n",
    "This step quantifies the effectiveness of each optimization technique by comparing their performance to the baseline model. The key metrics analyzed are:\n",
    "\n",
    "- **Inference Time**: Measures how much faster the optimized model generates predictions.\n",
    "- **Carbon Emissions**: Estimates how optimization reduces the environmental impact of model computations.\n",
    "\n",
    "The percentage improvement for each metric is calculated using the formula:\n",
    "   ```\n",
    "   Improvement (%) = ((Baseline Metric - Optimized Metric) / Baseline Metric) * 100\n",
    "   ```\n",
    " This formula is applied to both **inference time** and **carbon emissions**.\n",
    " Results are reported as a percentage, indicating the relative improvement of each optimized model.\n",
    "\n",
    " The optimized models include Pruned, Quantized, Distilled, Efficient Attention, Compressed, and Smaller models.Each model’s performance is compared to the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-missile",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate  #pip install tabulate\n",
    "\n",
    "# Function to calculate improvements\n",
    "def calculate_improvement(baseline, optimized):\n",
    "    if baseline and optimized:\n",
    "        return (baseline - optimized) / baseline * 100\n",
    "    return None\n",
    "\n",
    "# List of models with durations and emissions\n",
    "models = [\n",
    "    (\"Pruned\", pruned_duration, pruned_emissions),\n",
    "    (\"Quantized\", quantized_duration, quantized_emissions),\n",
    "    (\"Distilled\", distil_duration, distil_emissions),\n",
    "    (\"Efficient Attention\", efficient_duration, efficient_emissions),\n",
    "    (\"Smaller\", smaller_duration, smaller_emissions)\n",
    "]\n",
    "\n",
    "# Prepare table data with baseline and optimized values\n",
    "table_data = []\n",
    "for name, duration, emissions in models:\n",
    "    time_improvement = calculate_improvement(baseline_duration, duration)\n",
    "    emissions_improvement = calculate_improvement(baseline_emissions, emissions)\n",
    "    if time_improvement is not None and emissions_improvement is not None:\n",
    "        table_data.append([\n",
    "            name, \n",
    "            baseline_duration, duration, f\"{time_improvement:.2f}%\",\n",
    "            baseline_emissions, emissions, f\"{emissions_improvement:.2f}%\"\n",
    "        ])\n",
    "    else:\n",
    "        table_data.append([name, \"N/A\", \"N/A\", \"Measurement failed\", \"N/A\", \"N/A\", \"Measurement failed\"])\n",
    "\n",
    "# Print extended table\n",
    "print(\"\\nDetailed Improvements:\")\n",
    "print(tabulate(table_data, headers=[\"Model\", \"Baseline Time\", \"Optimized Time\", \"Time Improvement\", \n",
    "                                    \"Baseline Emissions\", \"Optimized Emissions\", \"Emissions Improvement\"], \n",
    "               tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "print(\"\\nCreating visualization...\")\n",
    "names = [model[0] for model in models if model[1] is not None and model[2] is not None]\n",
    "durations = [model[1] for model in models if model[1] is not None and model[2] is not None]\n",
    "emissions = [model[2] for model in models if model[1] is not None and model[2] is not None]\n",
    "\n",
    "if names and durations and emissions:\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    ax1.bar(names, durations, color='skyblue')\n",
    "    ax1.set_ylabel('Duration (seconds)')\n",
    "    ax1.set_title('Inference Duration Comparison')\n",
    "    ax1.axhline(y=baseline_duration, color='r', linestyle='--', label='Baseline')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.bar(names, emissions, color='lightgreen')\n",
    "    ax2.set_ylabel('Estimated Emissions (kg CO2eq)')\n",
    "    ax2.set_title('Estimated Carbon Emissions Comparison')\n",
    "    ax2.axhline(y=baseline_emissions, color='r', linestyle='--', label='Baseline')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig('llm_optimization_comparison.png')\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data to create visualization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConclusion:\")\n",
    "print(\"This script demonstrated various optimization techniques for LLMs and their impact on performance and estimated carbon emissions.\")\n",
    "print(\"The results show that different techniques can lead to significant improvements in both speed and environmental impact.\")\n",
    "print(\"However, it's important to note that these optimizations may affect model accuracy, which should be evaluated separately.\")\n",
    "print(\"The emissions estimates provided by CodeCarbon are more accurate than the previous rough estimates, but should still be considered approximations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109f62d-3b86-4801-91c4-2edcc33b6aaf",
   "metadata": {},
   "source": [
    "## On Optimization Methods and Efficiency Gains\n",
    "\n",
    "While the optimization methods implemented in this notebook aim to reduce carbon emissions and computational time, the following factors might limit their effectiveness:\n",
    "\n",
    "1. **Hardware Dependency**: \n",
    "   - Some methods, like mixed-precision training or quantization, require specific hardware (e.g., GPUs with Tensor Cores).\n",
    "   - Without compatible hardware, these optimizations might not yield significant gains.\n",
    "\n",
    "2. **Model Characteristics**: \n",
    "   - The architecture of certain models may inherently limit the effectiveness of optimizations.\n",
    "   - Larger models may still consume significant energy, even with optimizations, due to their complexity.\n",
    "\n",
    "3. **Overhead Costs**: \n",
    "   - Implementing methods like gradient checkpointing can introduce additional computational overhead during training, potentially negating runtime gains.\n",
    "\n",
    "4. **Use Case Variability**: \n",
    "   - Optimizations might perform differently across tasks (e.g., fine-tuning, inference), and their benefits may vary depending on the dataset and prompt complexity.\n",
    "\n",
    "Therefore, while these methods improve efficiency in many cases, their impact depends on the specific context and hardware configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0762ed6-eaa5-4b94-a8f6-1d50f42f902a",
   "metadata": {},
   "source": [
    "## Prepared By\n",
    "\n",
    "- **Name**: **Shaina Raza, PhD** [shaina.raza@vectorinstitute.ai](mailto:shaina.raza@vectorinstitute.ai)\n",
    "- **Affiliation**: Vector Institute for Artificial Intelligence\n",
    "\n",
    "This notebook was prepared as part of a practical guide for efficient evaluation and optimization of large language models (LLMs), with an emphasis on reducing carbon emissions and computational costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6b322-4bab-4fdc-a285-a48670c5347d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "green-ai",
   "language": "python",
   "name": "green-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
